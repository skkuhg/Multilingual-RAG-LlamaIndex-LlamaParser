{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Language-Learning Notebook 🌏 (RAG-powered with LlamaIndex + LlamaParser)\n",
    "\n",
    "This notebook implements an end-to-end Retrieval-Augmented-Generation (RAG) workflow for organizing and practicing multilingual vocabulary and example sentences. Using LlamaIndex and LlamaParser, it ingests various text sources (articles, subtitles, chat logs, wordlists), builds a vector index, and provides intelligent query capabilities for language learning.\n",
    "\n",
    "**Key Features:**\n",
    "- Multi-format data ingestion (TXT, HTML, SRT, VTT, JSON, CSV)\n",
    "- RAG-powered vocabulary drills with CEFR level filtering\n",
    "- Spaced repetition scheduling\n",
    "- Cost tracking and token usage monitoring\n",
    "- Export capabilities for Anki integration\n",
    "\n",
    "**How to run:** \n",
    "1. Create a `.env` file with your API keys\n",
    "2. Add your multilingual content to the `data/` folder\n",
    "3. Run all cells sequentially\n",
    "4. Use the demo queries to explore your language learning corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the required dependencies and set up our environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet llama-index llama-parser tavily-python openai python-dotenv pandas tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables Setup\n",
    "\n",
    "Create a `.env` file in the same directory as this notebook with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "LLAMACLOUD_API_KEY=your_llamacloud_api_key_here\n",
    "TAVILY_API_KEY=your_tavily_api_key_here  # Optional for web search\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI API Key: Set\n",
      "✅ LlamaCloud API Key: Set\n",
      "📡 Tavily API Key: Set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are loaded - with Tavily key directly set if needed\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "llama_key = os.getenv('LLAMACLOUD_API_KEY')\n",
    "tavily_key = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "# If Tavily key is not in .env, set it directly\n",
    "if not tavily_key:\n",
    "    tavily_key = \"your_tavily_api_key_here\"\n",
    "    os.environ['TAVILY_API_KEY'] = tavily_key\n",
    "\n",
    "print(f\"✅ OpenAI API Key: {'Set' if openai_key else '❌ Missing'}\")\n",
    "print(f\"✅ LlamaCloud API Key: {'Set' if llama_key else '❌ Missing'}\")\n",
    "print(f\"📡 Tavily API Key: {'Set' if tavily_key else 'Optional - Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI library imported successfully\n",
      "⚠️ LlamaParse not available - will use basic text parsing\n",
      "✅ Tavily imported and configured successfully\n",
      "📦 All imports successful!\n",
      "🚀 Simple RAG system ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Simple OpenAI imports - avoiding LlamaIndex complexity\n",
    "try:\n",
    "    import openai\n",
    "    openai_available = True\n",
    "    print(\"✅ OpenAI library imported successfully\")\n",
    "except ImportError:\n",
    "    openai_available = False\n",
    "    print(\"❌ OpenAI library not available\")\n",
    "\n",
    "# LlamaParser - keep this separate\n",
    "try:\n",
    "    from llama_parse import LlamaParse\n",
    "    llamaparse_available = True\n",
    "    print(\"✅ LlamaParse imported successfully\")\n",
    "except ImportError:\n",
    "    llamaparse_available = False\n",
    "    print(\"⚠️ LlamaParse not available - will use basic text parsing\")\n",
    "\n",
    "# Optional: Tavily for web search\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    # Test Tavily availability with the key\n",
    "    if tavily_key:\n",
    "        try:\n",
    "            test_client = TavilyClient(api_key=tavily_key)\n",
    "            tavily_available = True\n",
    "            print(\"✅ Tavily imported and configured successfully\")\n",
    "        except Exception as e:\n",
    "            tavily_available = False\n",
    "            print(f\"⚠️ Tavily import successful but configuration failed: {e}\")\n",
    "    else:\n",
    "        tavily_available = False\n",
    "        print(\"⚠️ Tavily imported but no API key available\")\n",
    "except ImportError as e:\n",
    "    tavily_available = False\n",
    "    print(f\"❌ Tavily import failed: {e}\")\n",
    "except Exception as e:\n",
    "    tavily_available = False\n",
    "    print(f\"❌ Tavily setup failed: {e}\")\n",
    "\n",
    "# Create simple replacement classes for RAG functionality\n",
    "class Document:\n",
    "    def __init__(self, text: str, metadata: Dict = None):\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.document_metadata = []\n",
    "        \n",
    "        if openai_available:\n",
    "            self.client = openai.OpenAI(api_key=api_key)\n",
    "        else:\n",
    "            self.client = None\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the RAG system with enhanced metadata tracking\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # Track metadata separately for better querying\n",
    "        for doc in documents:\n",
    "            self.document_metadata.append(doc.metadata)\n",
    "        \n",
    "        print(f\"Added {len(documents)} documents. Total: {len(self.documents)}\")\n",
    "        \n",
    "        # Show metadata summary\n",
    "        if self.document_metadata:\n",
    "            languages = [meta.get('language', 'Unknown') for meta in self.document_metadata]\n",
    "            from collections import Counter\n",
    "            lang_counter = Counter(languages)\n",
    "            print(f\"📊 Language distribution in index: {dict(lang_counter)}\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding for text using OpenAI\"\"\"\n",
    "        if not self.client:\n",
    "            return [0.0] * 1536  # Dummy embedding\n",
    "        \n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embedding: {e}\")\n",
    "            return [0.0] * 1536\n",
    "    \n",
    "    def query(self, query_text: str, top_k: int = 5, language_filter: str = None, \n",
    "              level_filter: str = None, file_filter: str = None) -> str:\n",
    "        \"\"\"Enhanced query with filtering capabilities\"\"\"\n",
    "        if not self.documents:\n",
    "            return \"No documents available for querying.\"\n",
    "        \n",
    "        if not self.client:\n",
    "            return \"OpenAI client not available. Using sample response.\"\n",
    "        \n",
    "        # Filter documents based on criteria\n",
    "        filtered_docs = []\n",
    "        filtered_metadata = []\n",
    "        \n",
    "        for doc, meta in zip(self.documents, self.document_metadata):\n",
    "            # Apply filters\n",
    "            if language_filter and meta.get('language', '').lower() != language_filter.lower():\n",
    "                continue\n",
    "            if level_filter and meta.get('cefr_level', '') != level_filter:\n",
    "                continue  \n",
    "            if file_filter and file_filter.lower() not in meta.get('source_file', '').lower():\n",
    "                continue\n",
    "                \n",
    "            filtered_docs.append(doc)\n",
    "            filtered_metadata.append(meta)\n",
    "        \n",
    "        if not filtered_docs:\n",
    "            return f\"No documents found matching filters (language={language_filter}, level={level_filter}, file={file_filter})\"\n",
    "        \n",
    "        # Enhanced keyword-based retrieval\n",
    "        relevant_docs = []\n",
    "        query_lower = query_text.lower()\n",
    "        query_words = query_lower.split()\n",
    "        \n",
    "        # Score documents based on keyword matches and metadata relevance\n",
    "        doc_scores = []\n",
    "        for doc, meta in zip(filtered_docs, filtered_metadata):\n",
    "            score = 0\n",
    "            doc_text_lower = doc.text.lower()\n",
    "            \n",
    "            # Keyword matching\n",
    "            for word in query_words:\n",
    "                if word in doc_text_lower:\n",
    "                    score += 2  # Direct word match\n",
    "                    \n",
    "            # Boost score for exact phrase matches\n",
    "            if query_lower in doc_text_lower:\n",
    "                score += 5\n",
    "                \n",
    "            # Metadata-based scoring\n",
    "            if 'context' in meta and query_lower in meta['context'].lower():\n",
    "                score += 1\n",
    "                \n",
    "            doc_scores.append((score, doc, meta))\n",
    "        \n",
    "        # Sort by score and select top documents\n",
    "        doc_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Take top scoring docs, or fallback to first few if no scores\n",
    "        if any(score > 0 for score, _, _ in doc_scores):\n",
    "            relevant_docs = [(doc, meta) for score, doc, meta in doc_scores[:top_k] if score > 0]\n",
    "        else:\n",
    "            relevant_docs = [(doc, meta) for _, doc, meta in doc_scores[:top_k]]\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            relevant_docs = [(filtered_docs[0], filtered_metadata[0])]  # At least one doc\n",
    "        \n",
    "        # Create enhanced context with metadata\n",
    "        context_parts = []\n",
    "        for doc, meta in relevant_docs:\n",
    "            context_part = f\"Text: {doc.text}\\n\"\n",
    "            if meta.get('language'):\n",
    "                context_part += f\"Language: {meta['language']}\\n\"\n",
    "            if meta.get('cefr_level'):\n",
    "                context_part += f\"Level: {meta['cefr_level']}\\n\"\n",
    "            if meta.get('file_name'):\n",
    "                context_part += f\"Source: {meta['file_name']}\\n\"\n",
    "            if meta.get('context'):\n",
    "                context_part += f\"Context: {meta['context'][:100]}...\\n\"\n",
    "            context_part += \"---\\n\"\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate response using OpenAI with enhanced prompt\n",
    "        try:\n",
    "            system_prompt = \"\"\"You are a helpful multilingual language learning assistant. \n",
    "            Use the provided context to answer questions about language learning content.\n",
    "            Pay attention to the metadata (language, level, source) when providing answers.\n",
    "            If asking about specific patterns or grammar, provide examples from the context.\n",
    "            Always be encouraging and educational in your responses.\"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query_text}\"}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "    \n",
    "    def get_file_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of all files in the RAG system\"\"\"\n",
    "        if not self.document_metadata:\n",
    "            return {}\n",
    "        \n",
    "        from collections import defaultdict, Counter\n",
    "        \n",
    "        summary = {\n",
    "            'total_documents': len(self.documents),\n",
    "            'files': Counter(),\n",
    "            'languages': Counter(),\n",
    "            'levels': Counter(),\n",
    "            'file_types': Counter()\n",
    "        }\n",
    "        \n",
    "        for meta in self.document_metadata:\n",
    "            if 'file_name' in meta:\n",
    "                summary['files'][meta['file_name']] += 1\n",
    "            if 'language' in meta:\n",
    "                summary['languages'][meta['language']] += 1\n",
    "            if 'cefr_level' in meta:\n",
    "                summary['levels'][meta['cefr_level']] += 1\n",
    "            if 'file_type' in meta:\n",
    "                summary['file_types'][meta['file_type']] += 1\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Global variables that will be used throughout the notebook\n",
    "rag_system = None\n",
    "index = None  # For compatibility with existing code\n",
    "query_engine = None  # For compatibility with existing code\n",
    "\n",
    "print(\"📦 All imports successful!\")\n",
    "print(\"🚀 Simple RAG system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Simple RAG system configured with OpenAI!\n",
      "⚙️ Configuration complete!\n"
     ]
    }
   ],
   "source": [
    "# Configure Simple RAG System\n",
    "if openai_available and openai_key:\n",
    "    rag_system = SimpleRAGSystem(api_key=openai_key)\n",
    "    print(\"🤖 Simple RAG system configured with OpenAI!\")\n",
    "else:\n",
    "    rag_system = None\n",
    "    print(\"❌ OpenAI not available - RAG functionality will be limited\")\n",
    "\n",
    "# Create compatibility objects for existing code\n",
    "class Settings:\n",
    "    llm = None\n",
    "    embed_model = None\n",
    "\n",
    "print(\"⚙️ Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 Token tracking system initialized!\n"
     ]
    }
   ],
   "source": [
    "# Token and cost tracking\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.embedding_tokens = 0\n",
    "        self.llm_tokens = 0\n",
    "        \n",
    "        # Pricing (per 1K tokens)\n",
    "        self.embedding_cost_per_1k = 0.00002  # text-embedding-3-small\n",
    "        self.llm_cost_per_1k = 0.00015  # gpt-4o-mini input\n",
    "        \n",
    "    def add_embedding_tokens(self, count: int):\n",
    "        self.embedding_tokens += count\n",
    "        self.total_tokens += count\n",
    "        \n",
    "    def add_llm_tokens(self, count: int):\n",
    "        self.llm_tokens += count\n",
    "        self.total_tokens += count\n",
    "        \n",
    "    def get_cost_summary(self) -> Dict[str, Any]:\n",
    "        embedding_cost = (self.embedding_tokens / 1000) * self.embedding_cost_per_1k\n",
    "        llm_cost = (self.llm_tokens / 1000) * self.llm_cost_per_1k\n",
    "        total_cost = embedding_cost + llm_cost\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'embedding_tokens': self.embedding_tokens,\n",
    "            'llm_tokens': self.llm_tokens,\n",
    "            'embedding_cost': embedding_cost,\n",
    "            'llm_cost': llm_cost,\n",
    "            'total_cost': total_cost\n",
    "        }\n",
    "\n",
    "# Initialize token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Approximate token count for text\"\"\"\n",
    "    return len(text.split()) * 1.3  # Rough approximation\n",
    "\n",
    "print(\"💰 Token tracking system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion\n",
    "\n",
    "This section handles ingesting various file types and parsing them with LlamaParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Data directories created!\n",
      "\n",
      "📋 Supported file types:\n",
      "  • Articles: .txt, .html, .md\n",
      "  • Subtitles: .srt, .vtt\n",
      "  • Chat logs: .json, .txt\n",
      "  • Word lists: .csv\n"
     ]
    }
   ],
   "source": [
    "# Create data directories if they don't exist\n",
    "data_dirs = ['data/articles', 'data/subtitles', 'data/chat_logs', 'data/wordlists', 'storage']\n",
    "for dir_path in data_dirs:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"📁 Data directories created!\")\n",
    "print(\"\\n📋 Supported file types:\")\n",
    "print(\"  • Articles: .txt, .html, .md\")\n",
    "print(\"  • Subtitles: .srt, .vtt\")\n",
    "print(\"  • Chat logs: .json, .txt\")\n",
    "print(\"  • Word lists: .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ LlamaParse not available - will use basic text parsing\n",
      "🔍 Enhanced data analysis function ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LlamaParser if available\n",
    "if llamaparse_available and llama_key:\n",
    "    parser = LlamaParse(\n",
    "        api_key=llama_key,\n",
    "        result_type=\"text\",\n",
    "        verbose=True,\n",
    "        language=\"mixed\"  # Support for multilingual content\n",
    "    )\n",
    "    print(\"✅ LlamaParse initialized successfully\")\n",
    "else:\n",
    "    parser = None\n",
    "    print(\"⚠️ LlamaParse not available - will use basic text parsing\")\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Simple language detection based on character patterns\"\"\"\n",
    "    # Korean characters\n",
    "    if re.search(r'[가-힣]', text):\n",
    "        return 'Korean'\n",
    "    # Japanese characters\n",
    "    elif re.search(r'[ひらがなカタカナ漢字]', text):\n",
    "        return 'Japanese'\n",
    "    # Chinese characters\n",
    "    elif re.search(r'[\\u4e00-\\u9fff]', text):\n",
    "        return 'Chinese'\n",
    "    # Spanish indicators\n",
    "    elif re.search(r'[ñáéíóúü]', text.lower()):\n",
    "        return 'Spanish'\n",
    "    # French indicators\n",
    "    elif re.search(r'[àâäçéèêëïîôùûüÿ]', text.lower()):\n",
    "        return 'French'\n",
    "    # German indicators\n",
    "    elif re.search(r'[äöüß]', text.lower()):\n",
    "        return 'German'\n",
    "    else:\n",
    "        return 'English'  # Default\n",
    "\n",
    "def estimate_cefr_level(text: str, language: str) -> str:\n",
    "    \"\"\"Rough CEFR level estimation based on text complexity\"\"\"\n",
    "    words = text.split()\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    sentence_length = len(words)\n",
    "    \n",
    "    # Simple heuristic\n",
    "    if avg_word_length < 4 and sentence_length < 8:\n",
    "        return 'A1'\n",
    "    elif avg_word_length < 5 and sentence_length < 12:\n",
    "        return 'A2'\n",
    "    elif avg_word_length < 6 and sentence_length < 16:\n",
    "        return 'B1'\n",
    "    elif avg_word_length < 7 and sentence_length < 20:\n",
    "        return 'B2'\n",
    "    else:\n",
    "        return 'C1'\n",
    "\n",
    "def simple_text_parser(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"Enhanced text parser that preserves document structure and context\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Clean the content\n",
    "        content = content.strip()\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        # Split into paragraphs first to preserve structure\n",
    "        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "        if not paragraphs:\n",
    "            # Fallback to line-by-line if no paragraph breaks\n",
    "            paragraphs = [p.strip() for p in content.split('\\n') if p.strip()]\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for para_idx, paragraph in enumerate(paragraphs):\n",
    "            # Enhanced sentence splitting that handles multiple languages\n",
    "            # Korean: ., !, ?, 。\n",
    "            # Spanish/French: ., !, ?, ¡, ¿\n",
    "            # General punctuation\n",
    "            sentence_endings = r'[.!?。¡¿]+(?:\\s+|$)'\n",
    "            sentences = re.split(sentence_endings, paragraph)\n",
    "            \n",
    "            for sent_idx, sentence in enumerate(sentences):\n",
    "                sentence = sentence.strip()\n",
    "                if len(sentence) < 5:  # More lenient minimum length\n",
    "                    continue\n",
    "                \n",
    "                # Create document with enhanced metadata\n",
    "                doc_data = {\n",
    "                    'text': sentence,\n",
    "                    'paragraph_index': para_idx,\n",
    "                    'sentence_index': sent_idx,\n",
    "                    'paragraph_context': paragraph[:200] + '...' if len(paragraph) > 200 else paragraph,\n",
    "                    'file_size': len(content),\n",
    "                    'total_paragraphs': len(paragraphs)\n",
    "                }\n",
    "                documents.append(doc_data)\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def enhanced_csv_parser(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"Enhanced CSV parser that handles various CSV formats\"\"\"\n",
    "    try:\n",
    "        # Try different encodings\n",
    "        encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']\n",
    "        df = None\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if df is None:\n",
    "            print(f\"Could not read CSV file {file_path} with any encoding\")\n",
    "            return []\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        # Handle different CSV structures\n",
    "        if 'word' in df.columns and 'translation' in df.columns:\n",
    "            # Vocabulary list format\n",
    "            for idx, row in df.iterrows():\n",
    "                word = str(row.get('word', ''))\n",
    "                translation = str(row.get('translation', ''))\n",
    "                \n",
    "                if word and word != 'nan':\n",
    "                    doc_data = {\n",
    "                        'text': f\"{word} - {translation}\",\n",
    "                        'word': word,\n",
    "                        'translation': translation,\n",
    "                        'row_index': idx,\n",
    "                        'csv_type': 'vocabulary'\n",
    "                    }\n",
    "                    documents.append(doc_data)\n",
    "                    \n",
    "        elif 'text' in df.columns or 'sentence' in df.columns:\n",
    "            # Sentence list format\n",
    "            text_col = 'text' if 'text' in df.columns else 'sentence'\n",
    "            for idx, row in df.iterrows():\n",
    "                text = str(row.get(text_col, ''))\n",
    "                if text and text != 'nan' and len(text) > 5:\n",
    "                    doc_data = {\n",
    "                        'text': text,\n",
    "                        'row_index': idx,\n",
    "                        'csv_type': 'sentences'\n",
    "                    }\n",
    "                    # Add any additional columns as metadata\n",
    "                    for col in df.columns:\n",
    "                        if col != text_col:\n",
    "                            doc_data[col] = row.get(col)\n",
    "                    documents.append(doc_data)\n",
    "        else:\n",
    "            # Generic CSV - use first text-like column\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':  # Text column\n",
    "                    for idx, row in df.iterrows():\n",
    "                        text = str(row.get(col, ''))\n",
    "                        if text and text != 'nan' and len(text) > 5:\n",
    "                            doc_data = {\n",
    "                                'text': text,\n",
    "                                'row_index': idx,\n",
    "                                'csv_type': 'generic',\n",
    "                                'source_column': col\n",
    "                            }\n",
    "                            documents.append(doc_data)\n",
    "                    break\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing CSV file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def enhanced_json_parser(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"Enhanced JSON parser that handles various JSON structures\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            # List of objects/messages\n",
    "            for idx, item in enumerate(data):\n",
    "                if isinstance(item, dict):\n",
    "                    # Extract text from various possible fields\n",
    "                    text_fields = ['content', 'text', 'message', 'sentence', 'body']\n",
    "                    text = None\n",
    "                    \n",
    "                    for field in text_fields:\n",
    "                        if field in item and item[field]:\n",
    "                            text = str(item[field])\n",
    "                            break\n",
    "                    \n",
    "                    if text and len(text) > 5:\n",
    "                        doc_data = {\n",
    "                            'text': text,\n",
    "                            'json_index': idx,\n",
    "                            'json_type': 'list_item'\n",
    "                        }\n",
    "                        # Add other fields as metadata\n",
    "                        for key, value in item.items():\n",
    "                            if key not in text_fields:\n",
    "                                doc_data[key] = value\n",
    "                        documents.append(doc_data)\n",
    "                        \n",
    "                elif isinstance(item, str) and len(item) > 5:\n",
    "                    # Simple string list\n",
    "                    doc_data = {\n",
    "                        'text': item,\n",
    "                        'json_index': idx,\n",
    "                        'json_type': 'string_list'\n",
    "                    }\n",
    "                    documents.append(doc_data)\n",
    "                    \n",
    "        elif isinstance(data, dict):\n",
    "            # Dictionary structure\n",
    "            def extract_from_dict(obj, prefix=\"\"):\n",
    "                texts = []\n",
    "                for key, value in obj.items():\n",
    "                    current_key = f\"{prefix}.{key}\" if prefix else key\n",
    "                    \n",
    "                    if isinstance(value, str) and len(value) > 5:\n",
    "                        texts.append({\n",
    "                            'text': value,\n",
    "                            'json_key': current_key,\n",
    "                            'json_type': 'dict_value'\n",
    "                        })\n",
    "                    elif isinstance(value, dict):\n",
    "                        texts.extend(extract_from_dict(value, current_key))\n",
    "                    elif isinstance(value, list):\n",
    "                        for i, item in enumerate(value):\n",
    "                            if isinstance(item, str) and len(item) > 5:\n",
    "                                texts.append({\n",
    "                                    'text': item,\n",
    "                                    'json_key': f\"{current_key}[{i}]\",\n",
    "                                    'json_type': 'dict_list_item'\n",
    "                                })\n",
    "                return texts\n",
    "            \n",
    "            documents = extract_from_dict(data)\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_data_completeness(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Analyze and display comprehensive information about the ingested data\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"❌ No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"🔍 COMPREHENSIVE DATA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"📊 Basic Statistics:\")\n",
    "    print(f\"   • Total entries: {len(df):,}\")\n",
    "    print(f\"   • Total characters: {df['text'].str.len().sum():,}\")\n",
    "    print(f\"   • Average text length: {df['text'].str.len().mean():.1f} characters\")\n",
    "    print(f\"   • Unique texts: {df['text'].nunique():,}\")\n",
    "    \n",
    "    # Language analysis\n",
    "    print(f\"\\n🌍 Language Distribution:\")\n",
    "    lang_counts = df['language'].value_counts()\n",
    "    for lang, count in lang_counts.head(10).items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   • {lang}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # CEFR level analysis\n",
    "    print(f\"\\n📚 CEFR Level Distribution:\")\n",
    "    level_counts = df['cefr_level'].value_counts()\n",
    "    for level, count in level_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   • {level}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # File analysis\n",
    "    print(f\"\\n📁 File Source Analysis:\")\n",
    "    if 'file_name' in df.columns:\n",
    "        file_counts = df['file_name'].value_counts()\n",
    "        print(f\"   • Total files: {len(file_counts)}\")\n",
    "        for file_name, count in file_counts.head(10).items():\n",
    "            print(f\"   • {file_name}: {count:,} entries\")\n",
    "    \n",
    "    # File type analysis\n",
    "    if 'file_type' in df.columns:\n",
    "        print(f\"\\n📋 File Type Analysis:\")\n",
    "        type_counts = df['file_type'].value_counts()\n",
    "        for file_type, count in type_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   • {file_type}: {count:,} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Content quality analysis\n",
    "    print(f\"\\n✨ Content Quality Analysis:\")\n",
    "    \n",
    "    # Text length distribution\n",
    "    text_lengths = df['text'].str.len()\n",
    "    print(f\"   • Shortest text: {text_lengths.min()} characters\")\n",
    "    print(f\"   • Longest text: {text_lengths.max()} characters\")\n",
    "    print(f\"   • Median length: {text_lengths.median():.0f} characters\")\n",
    "    \n",
    "    # Very short texts (potential quality issues)\n",
    "    very_short = df[df['text'].str.len() < 10]\n",
    "    if len(very_short) > 0:\n",
    "        print(f\"   ⚠️ Very short texts (<10 chars): {len(very_short)}\")\n",
    "    \n",
    "    # Very long texts (potential chunking needed)  \n",
    "    very_long = df[df['text'].str.len() > 500]\n",
    "    if len(very_long) > 0:\n",
    "        print(f\"   📏 Very long texts (>500 chars): {len(very_long)}\")\n",
    "        \n",
    "    # Language-specific patterns\n",
    "    print(f\"\\n🎯 Language-Specific Pattern Analysis:\")\n",
    "    \n",
    "    for language in lang_counts.head(5).index:\n",
    "        lang_df = df[df['language'] == language]\n",
    "        print(f\"\\n   {language} ({len(lang_df):,} entries):\")\n",
    "        \n",
    "        # Show sample sentences\n",
    "        samples = lang_df['text'].head(3).tolist()\n",
    "        for i, sample in enumerate(samples, 1):\n",
    "            print(f\"     {i}. {sample[:80]}{'...' if len(sample) > 80 else ''}\")\n",
    "        \n",
    "        # Language-specific patterns\n",
    "        if language == 'Korean':\n",
    "            pattern_counts = {}\n",
    "            patterns = ['고 있어요', '습니다', '아요/어요', '을/를', '이/가']\n",
    "            for pattern in patterns:\n",
    "                count = lang_df['text'].str.contains(pattern, regex=False).sum()\n",
    "                if count > 0:\n",
    "                    pattern_counts[pattern] = count\n",
    "            \n",
    "            if pattern_counts:\n",
    "                print(f\"     📝 Korean patterns found:\")\n",
    "                for pattern, count in pattern_counts.items():\n",
    "                    print(f\"        • '{pattern}': {count} sentences\")\n",
    "        \n",
    "        elif language == 'Spanish':\n",
    "            pattern_counts = {}\n",
    "            patterns = ['que', 'de', 'la', 'el', 'es', 'en']\n",
    "            for pattern in patterns:\n",
    "                count = lang_df['text'].str.contains(f'\\\\b{pattern}\\\\b', regex=True, case=False).sum()\n",
    "                if count > 0:\n",
    "                    pattern_counts[pattern] = count\n",
    "            \n",
    "            if pattern_counts:\n",
    "                print(f\"     📝 Common Spanish words:\")\n",
    "                for pattern, count in sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "                    print(f\"        • '{pattern}': {count} occurrences\")\n",
    "    \n",
    "    # Data completeness check\n",
    "    print(f\"\\n🔧 Data Completeness Check:\")\n",
    "    required_columns = ['text', 'language', 'cefr_level', 'source_file']\n",
    "    for col in required_columns:\n",
    "        if col in df.columns:\n",
    "            null_count = df[col].isnull().sum()\n",
    "            empty_count = (df[col] == '').sum() if df[col].dtype == 'object' else 0\n",
    "            total_missing = null_count + empty_count\n",
    "            if total_missing > 0:\n",
    "                print(f\"   ⚠️ {col}: {total_missing} missing values\")\n",
    "            else:\n",
    "                print(f\"   ✅ {col}: Complete\")\n",
    "        else:\n",
    "            print(f\"   ❌ {col}: Column missing\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\n💡 Recommendations:\")\n",
    "    \n",
    "    total_entries = len(df)\n",
    "    if total_entries < 100:\n",
    "        print(\"   • Consider adding more content files for better learning variety\")\n",
    "    elif total_entries > 10000:\n",
    "        print(\"   • Large dataset detected - consider implementing pagination for better performance\")\n",
    "    \n",
    "    unique_languages = len(lang_counts)\n",
    "    if unique_languages == 1:\n",
    "        print(\"   • Add content in additional languages for multilingual learning\")\n",
    "    elif unique_languages > 5:\n",
    "        print(\"   • Rich multilingual content detected - excellent for diverse learning!\")\n",
    "    \n",
    "    # Pattern-specific recommendations\n",
    "    korean_count = lang_counts.get('Korean', 0)\n",
    "    if korean_count > 0:\n",
    "        korean_with_pattern = df[\n",
    "            (df['language'] == 'Korean') & \n",
    "            (df['text'].str.contains('고 있어요', na=False, regex=False))\n",
    "        ]\n",
    "        if len(korean_with_pattern) == 0:\n",
    "            print(\"   • Add more Korean present continuous ('-고 있어요') examples for grammar practice\")\n",
    "        else:\n",
    "            print(f\"   ✅ Korean present continuous pattern: {len(korean_with_pattern)} examples found\")\n",
    "\n",
    "print(\"🔍 Enhanced data analysis function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Data ingestion function ready!\n"
     ]
    }
   ],
   "source": [
    "def ingest_data_files(data_folder: str = \"data\") -> pd.DataFrame:\n",
    "    \"\"\"Enhanced data ingestion that preserves file structure and context\"\"\"\n",
    "    sentences_data = []\n",
    "    data_path = Path(data_folder)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"❌ Data folder '{data_folder}' not found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Supported file extensions with more formats\n",
    "    supported_extensions = {'.txt', '.html', '.md', '.srt', '.vtt', '.json', '.csv', '.rtf'}\n",
    "    \n",
    "    # Find all supported files\n",
    "    files_found = []\n",
    "    for ext in supported_extensions:\n",
    "        files_found.extend(data_path.rglob(f\"*{ext}\"))\n",
    "    \n",
    "    print(f\"📁 Found {len(files_found)} files to process...\")\n",
    "    total_chars_processed = 0\n",
    "    \n",
    "    for file_path in files_found:\n",
    "        print(f\"\\n🔄 Processing: {file_path.name} ({file_path.stat().st_size:,} bytes)\")\n",
    "        \n",
    "        try:\n",
    "            # Get file-level metadata\n",
    "            file_stats = file_path.stat()\n",
    "            file_metadata = {\n",
    "                'source_file': str(file_path),\n",
    "                'file_name': file_path.name,\n",
    "                'file_size': file_stats.st_size,\n",
    "                'file_type': file_path.suffix.lower(),\n",
    "                'relative_path': str(file_path.relative_to(data_path)),\n",
    "                'folder': file_path.parent.name,\n",
    "                'modified_time': datetime.fromtimestamp(file_stats.st_mtime).isoformat()\n",
    "            }\n",
    "            \n",
    "            documents = []\n",
    "            \n",
    "            if file_path.suffix == '.csv':\n",
    "                # Enhanced CSV handling\n",
    "                csv_docs = enhanced_csv_parser(file_path)\n",
    "                documents.extend(csv_docs)\n",
    "                \n",
    "            elif file_path.suffix == '.json':\n",
    "                # Enhanced JSON handling  \n",
    "                json_docs = enhanced_json_parser(file_path)\n",
    "                documents.extend(json_docs)\n",
    "                \n",
    "            elif file_path.suffix in {'.srt', '.vtt'}:\n",
    "                # Subtitle file handling\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Parse subtitle format\n",
    "                if file_path.suffix == '.srt':\n",
    "                    # SRT format parsing\n",
    "                    blocks = re.split(r'\\n\\s*\\n', content)\n",
    "                    for block_idx, block in enumerate(blocks):\n",
    "                        lines = block.strip().split('\\n')\n",
    "                        if len(lines) >= 3:  # Index, time, text\n",
    "                            subtitle_text = ' '.join(lines[2:])  # Join all text lines\n",
    "                            if len(subtitle_text) > 5:\n",
    "                                documents.append({\n",
    "                                    'text': subtitle_text,\n",
    "                                    'subtitle_index': block_idx,\n",
    "                                    'timestamp': lines[1] if len(lines) > 1 else None,\n",
    "                                    'subtitle_format': 'srt'\n",
    "                                })\n",
    "                else:\n",
    "                    # VTT format - simpler parsing for now\n",
    "                    lines = content.split('\\n')\n",
    "                    for line_idx, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        if line and not line.startswith('WEBVTT') and '-->' not in line and not line.isdigit():\n",
    "                            if len(line) > 5:\n",
    "                                documents.append({\n",
    "                                    'text': line,\n",
    "                                    'subtitle_index': line_idx,\n",
    "                                    'subtitle_format': 'vtt'\n",
    "                                })\n",
    "                \n",
    "            else:\n",
    "                # Enhanced text file handling (txt, html, md, rtf)\n",
    "                if parser and file_path.suffix in {'.html', '.md', '.rtf'}:\n",
    "                    try:\n",
    "                        # Try LlamaParse for complex formats\n",
    "                        llama_docs = parser.load_data(str(file_path))\n",
    "                        for doc in llama_docs:\n",
    "                            if hasattr(doc, 'text') and doc.text.strip():\n",
    "                                # Use enhanced parser on LlamaParse output\n",
    "                                parsed_docs = simple_text_parser(file_path)\n",
    "                                documents.extend(parsed_docs)\n",
    "                    except Exception as e:\n",
    "                        print(f\"LlamaParse failed for {file_path.name}, using enhanced parser: {e}\")\n",
    "                        parsed_docs = simple_text_parser(file_path)\n",
    "                        documents.extend(parsed_docs)\n",
    "                else:\n",
    "                    # Use enhanced text parser\n",
    "                    parsed_docs = simple_text_parser(file_path)\n",
    "                    documents.extend(parsed_docs)\n",
    "            \n",
    "            # Process all documents from this file\n",
    "            file_sentences_count = 0\n",
    "            for doc_data in documents:\n",
    "                if not isinstance(doc_data, dict) or 'text' not in doc_data:\n",
    "                    continue\n",
    "                    \n",
    "                text = doc_data['text'].strip()\n",
    "                if len(text) < 5:  # Skip very short texts\n",
    "                    continue\n",
    "                \n",
    "                # Detect language and estimate level\n",
    "                language = detect_language(text)\n",
    "                cefr_level = estimate_cefr_level(text, language)\n",
    "                \n",
    "                # Create comprehensive sentence data\n",
    "                sentence_data = {\n",
    "                    'text': text,\n",
    "                    'language': language,\n",
    "                    'cefr_level': cefr_level,\n",
    "                    'emoji': get_language_emoji(language),\n",
    "                    **file_metadata,  # Add all file metadata\n",
    "                    **doc_data  # Add parser-specific metadata\n",
    "                }\n",
    "                \n",
    "                # Add contextual information\n",
    "                if 'paragraph_context' in doc_data:\n",
    "                    sentence_data['context'] = doc_data['paragraph_context']\n",
    "                \n",
    "                sentences_data.append(sentence_data)\n",
    "                file_sentences_count += 1\n",
    "                \n",
    "                # Track tokens\n",
    "                token_tracker.add_embedding_tokens(int(count_tokens(text)))\n",
    "                total_chars_processed += len(text)\n",
    "            \n",
    "            print(f\"   ✅ Extracted {file_sentences_count} sentences/entries\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    df_raw = pd.DataFrame(sentences_data)\n",
    "    print(f\"\\n🎉 Processing complete!\")\n",
    "    print(f\"📊 Total sentences/entries: {len(df_raw):,}\")\n",
    "    print(f\"📝 Total characters processed: {total_chars_processed:,}\")\n",
    "    print(f\"📁 Files processed: {len(files_found)}\")\n",
    "    \n",
    "    if not df_raw.empty:\n",
    "        print(f\"\\n📊 Language distribution:\")\n",
    "        lang_counts = df_raw['language'].value_counts()\n",
    "        for lang, count in lang_counts.items():\n",
    "            print(f\"  • {lang}: {count:,} sentences\")\n",
    "        \n",
    "        print(f\"\\n📊 CEFR level distribution:\")\n",
    "        level_counts = df_raw['cefr_level'].value_counts()\n",
    "        for level, count in level_counts.items():\n",
    "            print(f\"  • {level}: {count:,} sentences\")\n",
    "        \n",
    "        print(f\"\\n📊 File type distribution:\")\n",
    "        type_counts = df_raw['file_type'].value_counts()\n",
    "        for file_type, count in type_counts.items():\n",
    "            print(f\"  • {file_type}: {count:,} entries\")\n",
    "        \n",
    "        # Show sample of data structure\n",
    "        print(f\"\\n🔍 Sample data columns:\")\n",
    "        print(f\"Available columns: {list(df_raw.columns)}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "def get_language_emoji(language: str) -> str:\n",
    "    \"\"\"Get appropriate emoji for language\"\"\"\n",
    "    emoji_map = {\n",
    "        'Korean': '🇰🇷',\n",
    "        'Japanese': '🇯🇵', \n",
    "        'Chinese': '🇨🇳',\n",
    "        'Spanish': '🇪🇸',\n",
    "        'French': '🇫🇷',\n",
    "        'German': '🇩🇪',\n",
    "        'English': '🇺🇸',\n",
    "        'Italian': '🇮🇹',\n",
    "        'Portuguese': '🇵🇹',\n",
    "        'Russian': '🇷🇺'\n",
    "    }\n",
    "    return emoji_map.get(language, '📝')\n",
    "\n",
    "print(\"📥 Data ingestion function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 13 files to process...\n",
      "\n",
      "🔄 Processing: korean_conversation.json (820 bytes)\n",
      "   ✅ Extracted 5 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_lesson.json (1,086 bytes)\n",
      "   ✅ Extracted 6 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_drama.srt (526 bytes)\n",
      "   ✅ Extracted 7 sentences/entries\n",
      "\n",
      "🔄 Processing: french_basics.txt (545 bytes)\n",
      "   ✅ Extracted 15 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_future_tense.txt (499 bytes)\n",
      "   ✅ Extracted 15 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_grammar_patterns.txt (599 bytes)\n",
      "   ✅ Extracted 20 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_past_tense.txt (518 bytes)\n",
      "   ✅ Extracted 15 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_sample.txt (600 bytes)\n",
      "   ✅ Extracted 15 sentences/entries\n",
      "\n",
      "🔄 Processing: spanish_complete.txt (747 bytes)\n",
      "   ✅ Extracted 20 sentences/entries\n",
      "\n",
      "🔄 Processing: spanish_sample.txt (316 bytes)\n",
      "   ✅ Extracted 10 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_vocabulary_extended.csv (738 bytes)\n",
      "   ✅ Extracted 20 sentences/entries\n",
      "\n",
      "🔄 Processing: korean_words.csv (381 bytes)\n",
      "   ✅ Extracted 10 sentences/entries\n",
      "\n",
      "🔄 Processing: spanish_vocabulary.csv (680 bytes)\n",
      "   ✅ Extracted 20 sentences/entries\n",
      "\n",
      "🎉 Processing complete!\n",
      "📊 Total sentences/entries: 178\n",
      "📝 Total characters processed: 3,330\n",
      "📁 Files processed: 13\n",
      "\n",
      "📊 Language distribution:\n",
      "  • Korean: 113 sentences\n",
      "  • English: 35 sentences\n",
      "  • Spanish: 26 sentences\n",
      "  • French: 4 sentences\n",
      "\n",
      "📊 CEFR level distribution:\n",
      "  • A1: 119 sentences\n",
      "  • A2: 35 sentences\n",
      "  • B1: 16 sentences\n",
      "  • B2: 8 sentences\n",
      "\n",
      "📊 File type distribution:\n",
      "  • .txt: 110 entries\n",
      "  • .csv: 50 entries\n",
      "  • .json: 11 entries\n",
      "  • .srt: 7 entries\n",
      "\n",
      "🔍 Sample data columns:\n",
      "Available columns: ['text', 'language', 'cefr_level', 'emoji', 'source_file', 'file_name', 'file_size', 'file_type', 'relative_path', 'folder', 'modified_time', 'json_index', 'json_type', 'role', 'timestamp', 'speaker', 'subtitle_index', 'subtitle_format', 'paragraph_index', 'sentence_index', 'paragraph_context', 'total_paragraphs', 'context', 'word', 'translation', 'row_index', 'csv_type']\n",
      "\n",
      "🎉 Data ingestion complete! Total entries: 178\n",
      "🔍 COMPREHENSIVE DATA ANALYSIS\n",
      "============================================================\n",
      "📊 Basic Statistics:\n",
      "   • Total entries: 178\n",
      "   • Total characters: 3,330\n",
      "   • Average text length: 18.7 characters\n",
      "   • Unique texts: 166\n",
      "\n",
      "🌍 Language Distribution:\n",
      "   • Korean: 113 (63.5%)\n",
      "   • English: 35 (19.7%)\n",
      "   • Spanish: 26 (14.6%)\n",
      "   • French: 4 (2.2%)\n",
      "\n",
      "📚 CEFR Level Distribution:\n",
      "   • A1: 119 (66.9%)\n",
      "   • A2: 35 (19.7%)\n",
      "   • B1: 16 (9.0%)\n",
      "   • B2: 8 (4.5%)\n",
      "\n",
      "📁 File Source Analysis:\n",
      "   • Total files: 13\n",
      "   • korean_grammar_patterns.txt: 20 entries\n",
      "   • spanish_complete.txt: 20 entries\n",
      "   • korean_vocabulary_extended.csv: 20 entries\n",
      "   • spanish_vocabulary.csv: 20 entries\n",
      "   • french_basics.txt: 15 entries\n",
      "   • korean_future_tense.txt: 15 entries\n",
      "   • korean_past_tense.txt: 15 entries\n",
      "   • korean_sample.txt: 15 entries\n",
      "   • spanish_sample.txt: 10 entries\n",
      "   • korean_words.csv: 10 entries\n",
      "\n",
      "📋 File Type Analysis:\n",
      "   • .txt: 110 files (61.8%)\n",
      "   • .csv: 50 files (28.1%)\n",
      "   • .json: 11 files (6.2%)\n",
      "   • .srt: 7 files (3.9%)\n",
      "\n",
      "✨ Content Quality Analysis:\n",
      "   • Shortest text: 7 characters\n",
      "   • Longest text: 47 characters\n",
      "   • Median length: 15 characters\n",
      "   ⚠️ Very short texts (<10 chars): 16\n",
      "\n",
      "🎯 Language-Specific Pattern Analysis:\n",
      "\n",
      "   Korean (113 entries):\n",
      "     1. 안녕하세요! 한국어를 배우고 있어요.\n",
      "     2. 안녕하세요! 한국어 공부하고 계시는군요. 얼마나 오래 공부하고 있어요?\n",
      "     3. 6개월 동안 공부하고 있어요. 아직 어려워요.\n",
      "     📝 Korean patterns found:\n",
      "        • '고 있어요': 50 sentences\n",
      "\n",
      "   English (35 entries):\n",
      "     1. Elle habite dans une belle maison\n",
      "     2. Il fait beau aujourd'hui\n",
      "     3. Quelle heure est-il maintenant\n",
      "\n",
      "   Spanish (26 entries):\n",
      "     1. J'aime beaucoup étudier le français\n",
      "     2. Nous allons au cinéma ce soir\n",
      "     3. Je voudrais une tasse de café\n",
      "     📝 Common Spanish words:\n",
      "        • 'que': 11 occurrences\n",
      "        • 'la': 3 occurrences\n",
      "        • 'de': 2 occurrences\n",
      "\n",
      "   French (4 entries):\n",
      "     1. Ils mangent au restaurant français\n",
      "     2. Vous pouvez m'aider, s'il vous plaît\n",
      "     3. Où est-ce que vous habitez\n",
      "\n",
      "🔧 Data Completeness Check:\n",
      "   ✅ text: Complete\n",
      "   ✅ language: Complete\n",
      "   ✅ cefr_level: Complete\n",
      "   ✅ source_file: Complete\n",
      "\n",
      "💡 Recommendations:\n",
      "   ✅ Korean present continuous pattern: 50 examples found\n",
      "\n",
      "🔍 Quick Pattern Check:\n",
      "   • Korean sentences with '-고 있어요': 50\n",
      "   • Sample matches:\n",
      "     - 안녕하세요! 한국어를 배우고 있어요.\n",
      "       (Source: korean_conversation.json)\n",
      "     - 안녕하세요! 한국어 공부하고 계시는군요. 얼마나 오래 공부하고 있어요?\n",
      "       (Source: korean_conversation.json)\n",
      "     - 6개월 동안 공부하고 있어요. 아직 어려워요.\n",
      "       (Source: korean_conversation.json)\n",
      "\n",
      "📋 Sample Enhanced Metadata:\n",
      "   • text: 안녕하세요! 한국어를 배우고 있어요.\n",
      "   • language: Korean\n",
      "   • cefr_level: A2\n",
      "   • file_name: korean_conversation.json\n",
      "   • file_type: .json\n",
      "   • relative_path: chat_logs\\korean_conversation.json\n",
      "   • file_size: 820\n"
     ]
    }
   ],
   "source": [
    "# Create sample data if data folder is empty\n",
    "sample_korean_data = [\n",
    "    \"저는 지금 한국어를 공부하고 있어요.\",\n",
    "    \"친구들과 함께 영화를 보고 있어요.\", \n",
    "    \"오늘 날씨가 정말 좋고 있어요.\",\n",
    "    \"커피를 마시면서 책을 읽고 있어요.\",\n",
    "    \"새로운 언어를 배우고 있어요.\",\n",
    "    \"한국 음식을 요리하고 있어요.\",\n",
    "    \"지하철에서 음악을 듣고 있어요.\",\n",
    "    \"공원에서 산책하고 있어요.\",\n",
    "    \"한국 드라마를 시청하고 있어요.\",\n",
    "    \"새로운 단어들을 암기하고 있어요.\"\n",
    "]\n",
    "\n",
    "sample_spanish_data = [\n",
    "    \"Espero que tengas un buen día.\",\n",
    "    \"Ojalá que llueva mañana.\",\n",
    "    \"Es importante que estudies mucho.\", \n",
    "    \"Dudo que él venga a la fiesta.\",\n",
    "    \"Me alegro de que estés aquí.\"\n",
    "]\n",
    "\n",
    "# Create sample files if data directory is empty\n",
    "def create_sample_data():\n",
    "    korean_file = Path('data/articles/korean_sample.txt')\n",
    "    spanish_file = Path('data/articles/spanish_sample.txt')\n",
    "    \n",
    "    if not korean_file.exists():\n",
    "        with open(korean_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sample_korean_data))\n",
    "        print(f\"📝 Created sample Korean file: {korean_file}\")\n",
    "    \n",
    "    if not spanish_file.exists():\n",
    "        with open(spanish_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sample_spanish_data))\n",
    "        print(f\"📝 Created sample Spanish file: {spanish_file}\")\n",
    "    \n",
    "    # Create sample CSV wordlist\n",
    "    csv_file = Path('data/wordlists/korean_words.csv')\n",
    "    if not csv_file.exists():\n",
    "        sample_words = pd.DataFrame({\n",
    "            'word': ['안녕하세요', '감사합니다', '사랑해요', '공부하다', '먹다'],\n",
    "            'language': ['Korean'] * 5,\n",
    "            'level': ['A1', 'A1', 'A2', 'A2', 'A1'],\n",
    "            'translation': ['Hello', 'Thank you', 'I love you', 'To study', 'To eat'],\n",
    "            'emoji': ['👋', '🙏', '❤️', '📚', '🍽️']\n",
    "        })\n",
    "        sample_words.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        print(f\"📊 Created sample CSV wordlist: {csv_file}\")\n",
    "\n",
    "# Create additional content files for better testing\n",
    "def create_additional_content():\n",
    "    \"\"\"Create additional diverse content for better language learning examples\"\"\"\n",
    "    \n",
    "    # Additional Korean grammar content\n",
    "    korean_grammar_file = Path('data/articles/korean_grammar_patterns.txt')\n",
    "    if not korean_grammar_file.exists():\n",
    "        additional_korean = [\n",
    "            \"아침에 일어나고 있어요.\",\n",
    "            \"엄마가 요리하고 있어요.\", \n",
    "            \"학생들이 공부하고 있어요.\",\n",
    "            \"비가 내리고 있어요.\",\n",
    "            \"아이들이 놀고 있어요.\",\n",
    "            \"선생님이 설명하고 있어요.\",\n",
    "            \"고양이가 자고 있어요.\",\n",
    "            \"친구가 전화하고 있어요.\",\n",
    "            \"음악을 듣고 있어요.\",\n",
    "            \"책을 읽고 있어요.\"\n",
    "        ]\n",
    "        with open(korean_grammar_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(additional_korean))\n",
    "        print(f\"📝 Created additional Korean grammar file: {korean_grammar_file}\")\n",
    "    \n",
    "    # Extended vocabulary CSV\n",
    "    extended_vocab_file = Path('data/wordlists/korean_vocabulary_extended.csv')\n",
    "    if not extended_vocab_file.exists():\n",
    "        extended_vocab = pd.DataFrame({\n",
    "            'word': ['안녕하세요', '감사합니다', '사랑해요', '공부하다', '먹다', '마시다', '자다', '일하다', '놀다', '보다'],\n",
    "            'language': ['Korean'] * 10,\n",
    "            'level': ['A1', 'A1', 'A2', 'A2', 'A1', 'A1', 'A1', 'A2', 'A1', 'A1'],\n",
    "            'translation': ['Hello', 'Thank you', 'I love you', 'To study', 'To eat', 'To drink', 'To sleep', 'To work', 'To play', 'To see'],\n",
    "            'emoji': ['👋', '🙏', '❤️', '📚', '🍽️', '🥤', '😴', '💼', '🎮', '👀']\n",
    "        })\n",
    "        extended_vocab.to_csv(extended_vocab_file, index=False, encoding='utf-8')\n",
    "        print(f\"📊 Created extended vocabulary file: {extended_vocab_file}\")\n",
    "\n",
    "create_sample_data()\n",
    "create_additional_content()\n",
    "\n",
    "# Ingest all data with enhanced processing\n",
    "df_raw = ingest_data_files()\n",
    "\n",
    "if not df_raw.empty:\n",
    "    print(f\"\\n🎉 Data ingestion complete! Total entries: {len(df_raw):,}\")\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    analyze_data_completeness(df_raw)\n",
    "    \n",
    "    # Quick pattern check for debugging\n",
    "    print(f\"\\n🔍 Quick Pattern Check:\")\n",
    "    korean_sentences = df_raw[df_raw['language'] == 'Korean']\n",
    "    if len(korean_sentences) > 0:\n",
    "        pattern_matches = korean_sentences[korean_sentences['text'].str.contains('고 있어요', na=False, regex=False)]\n",
    "        print(f\"   • Korean sentences with '-고 있어요': {len(pattern_matches)}\")\n",
    "        \n",
    "        if len(pattern_matches) > 0:\n",
    "            print(\"   • Sample matches:\")\n",
    "            for idx, row in pattern_matches.head(3).iterrows():\n",
    "                print(f\"     - {row['text']}\")\n",
    "                print(f\"       (Source: {row.get('file_name', 'Unknown')})\")\n",
    "    \n",
    "    # Show rich metadata sample\n",
    "    if len(df_raw) > 0:\n",
    "        print(f\"\\n📋 Sample Enhanced Metadata:\")\n",
    "        sample_row = df_raw.iloc[0]\n",
    "        metadata_fields = ['text', 'language', 'cefr_level', 'file_name', 'file_type', \n",
    "                          'relative_path', 'file_size']\n",
    "        for field in metadata_fields:\n",
    "            if field in sample_row:\n",
    "                value = sample_row[field]\n",
    "                if field == 'text':\n",
    "                    value = str(value)[:50] + '...' if len(str(value)) > 50 else value\n",
    "                print(f\"   • {field}: {value}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No data ingested. Check your data folder and file formats.\")\n",
    "    \n",
    "    # Show what files exist\n",
    "    data_path = Path('data')\n",
    "    if data_path.exists():\n",
    "        print(f\"\\n📁 Files found in data directory:\")\n",
    "        for file_path in data_path.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                print(f\"   • {file_path.relative_to(data_path)} ({file_path.stat().st_size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"❌ Data directory does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Index Building\n",
    "\n",
    "Build a vector index from the ingested sentences for RAG queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔨 Building simple RAG system...\n",
      "Added 178 documents. Total: 178\n",
      "📊 Language distribution in index: {'Korean': 113, 'Spanish': 26, 'English': 35, 'French': 4}\n",
      "💾 RAG system built successfully!\n",
      "🚀 Query engine ready!\n"
     ]
    }
   ],
   "source": [
    "def build_vector_index(df: pd.DataFrame):\n",
    "    \"\"\"Build simple RAG system from sentence dataframe\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"❌ No data to index!\")\n",
    "        return None\n",
    "    \n",
    "    print(\"🔨 Building simple RAG system...\")\n",
    "    \n",
    "    if not rag_system:\n",
    "        print(\"❌ RAG system not available!\")\n",
    "        return None\n",
    "    \n",
    "    # Convert dataframe rows to Document objects\n",
    "    documents = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'language': row['language'],\n",
    "            'cefr_level': row['cefr_level'],\n",
    "            'source_file': row['source_file'],\n",
    "            'speaker': row['speaker'],\n",
    "            'timestamp': row['timestamp'],\n",
    "            'emoji': row['emoji'],\n",
    "            'row_id': idx\n",
    "        }\n",
    "        \n",
    "        # Create document\n",
    "        doc = Document(\n",
    "            text=row['text'],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Add documents to RAG system\n",
    "    rag_system.add_documents(documents)\n",
    "    \n",
    "    print(\"💾 RAG system built successfully!\")\n",
    "    return rag_system\n",
    "\n",
    "class SimpleQueryEngine:\n",
    "    \"\"\"Simple query engine wrapper for compatibility\"\"\"\n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "    \n",
    "    def query(self, prompt: str):\n",
    "        if self.rag_system:\n",
    "            return self.rag_system.query(prompt)\n",
    "        else:\n",
    "            return \"RAG system not available\"\n",
    "\n",
    "# Build the index (only if df_raw exists)\n",
    "if 'df_raw' in locals() and not df_raw.empty:\n",
    "    index = build_vector_index(df_raw)\n",
    "    \n",
    "    if index:\n",
    "        # Create query engine\n",
    "        query_engine = SimpleQueryEngine(index)\n",
    "        print(\"🚀 Query engine ready!\")\n",
    "    else:\n",
    "        print(\"❌ Failed to build RAG system!\")\n",
    "        index = None\n",
    "        query_engine = None\n",
    "else:\n",
    "    print(\"⚠️ No data available to build index (df_raw not found)\")\n",
    "    index = None\n",
    "    query_engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Query Functions\n",
    "\n",
    "Create utility functions for querying the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Helper query functions ready!\n"
     ]
    }
   ],
   "source": [
    "def ask_multilingual(prompt: str) -> str:\n",
    "    \"\"\"General multilingual RAG query wrapper\"\"\"\n",
    "    if not index:\n",
    "        return \"❌ Index not available. Please build the index first.\"\n",
    "    \n",
    "    print(f\"🤔 Querying: {prompt[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        response = query_engine.query(prompt)\n",
    "        \n",
    "        # Track tokens (approximate)\n",
    "        token_tracker.add_llm_tokens(int(count_tokens(prompt + str(response))))\n",
    "        \n",
    "        return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"❌ Query failed: {str(e)}\"\n",
    "\n",
    "def vocab_drill(pattern: str, lang: str, level: str = \"A2\", k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Specialized vocabulary drill function\"\"\"\n",
    "    if not index:\n",
    "        print(\"❌ Index not available. Please build the index first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"🎯 Vocab drill: {pattern} ({lang}, {level} level, {k} examples)\")\n",
    "    \n",
    "    # Debug: Check what data we have\n",
    "    if 'df_raw' in globals():\n",
    "        print(f\"📊 Total sentences in database: {len(df_raw)}\")\n",
    "        lang_counts = df_raw['language'].value_counts()\n",
    "        print(f\"📊 Available languages: {dict(lang_counts)}\")\n",
    "        \n",
    "        # Check for the specific pattern\n",
    "        korean_sentences = df_raw[df_raw['language'] == lang]\n",
    "        print(f\"📊 {lang} sentences: {len(korean_sentences)}\")\n",
    "        \n",
    "        # Try different pattern matching approaches\n",
    "        pattern_matches = korean_sentences[korean_sentences['text'].str.contains(pattern, na=False, regex=False)]\n",
    "        print(f\"📊 Sentences containing '{pattern}': {len(pattern_matches)}\")\n",
    "        \n",
    "        if len(pattern_matches) > 0:\n",
    "            print(\"✅ Found matching sentences:\")\n",
    "            for idx, row in pattern_matches.head(3).iterrows():\n",
    "                print(f\"   • {row['text']}\")\n",
    "    \n",
    "    try:\n",
    "        # Parse response into structured format\n",
    "        sentences_data = []\n",
    "        \n",
    "        # Filter dataframe directly for more reliable results\n",
    "        # Use exact string matching instead of regex for Korean patterns\n",
    "        filtered_df = df_raw[\n",
    "            (df_raw['language'] == lang) & \n",
    "            (df_raw['text'].str.contains(pattern, na=False, regex=False))  # Changed to regex=False\n",
    "        ]\n",
    "        \n",
    "        # If level filtering is too restrictive, try without it first\n",
    "        if not filtered_df.empty:\n",
    "            # Apply level filter if we have matches\n",
    "            level_filtered = filtered_df[filtered_df['cefr_level'] == level]\n",
    "            if not level_filtered.empty:\n",
    "                filtered_df = level_filtered\n",
    "            else:\n",
    "                print(f\"⚠️ No {level} level sentences found, using all levels\")\n",
    "        \n",
    "        filtered_df = filtered_df.head(k)\n",
    "        \n",
    "        if not filtered_df.empty:\n",
    "            print(f\"✅ Found {len(filtered_df)} matching sentences\")\n",
    "            # Use LLM to generate translations for the filtered sentences\n",
    "            for _, row in filtered_df.iterrows():\n",
    "                if rag_system and rag_system.client:\n",
    "                    try:\n",
    "                        # Generate translation using OpenAI\n",
    "                        translation_response = rag_system.client.chat.completions.create(\n",
    "                            model=\"gpt-4o-mini\",\n",
    "                            messages=[\n",
    "                                {\"role\": \"system\", \"content\": \"You are a helpful translator. Translate the given sentence to English. Provide only the translation, no extra text.\"},\n",
    "                                {\"role\": \"user\", \"content\": f\"Translate this {lang} sentence to English: '{row['text']}'\"}\n",
    "                            ],\n",
    "                            temperature=0.1,\n",
    "                            max_tokens=100\n",
    "                        )\n",
    "                        translation = translation_response.choices[0].message.content.strip()\n",
    "                    except Exception as e:\n",
    "                        translation = f\"Translation unavailable: {e}\"\n",
    "                else:\n",
    "                    translation = \"Translation service unavailable\"\n",
    "                \n",
    "                sentences_data.append({\n",
    "                    'Sentence': row['text'],\n",
    "                    'English': translation,\n",
    "                    'Emoji': row.get('emoji', '📝'),\n",
    "                    'Source': Path(row['source_file']).name,\n",
    "                    'Level': row['cefr_level']\n",
    "                })\n",
    "        \n",
    "        result_df = pd.DataFrame(sentences_data)\n",
    "        \n",
    "        if result_df.empty:\n",
    "            print(f\"⚠️ No sentences found matching pattern '{pattern}' in {lang}\")\n",
    "            \n",
    "            # Debug: Show available Korean sentences\n",
    "            if lang == 'Korean' and 'df_raw' in globals():\n",
    "                korean_samples = df_raw[df_raw['language'] == 'Korean']['text'].head(5).tolist()\n",
    "                print(\"📋 Available Korean sentences for debugging:\")\n",
    "                for i, sentence in enumerate(korean_samples, 1):\n",
    "                    print(f\"   {i}. {sentence}\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Vocab drill failed: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"🛠️ Helper query functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Enhanced query functions ready!\n",
      "💡 Try these new functions:\n",
      "   • query_by_file('korean', 'pattern') - Search within specific files\n",
      "   • query_by_pattern('고 있어요', 'Korean') - Pattern search with language filter\n",
      "   • smart_content_query('Korean grammar', {'language': 'Korean'}) - Filtered RAG query\n",
      "   • explore_file_contents() - Browse all file contents\n"
     ]
    }
   ],
   "source": [
    "# Enhanced query functions that utilize rich metadata\n",
    "def query_by_file(filename: str, query: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"Query content from a specific file\"\"\"\n",
    "    if 'df_raw' not in globals() or df_raw.empty:\n",
    "        print(\"❌ No data available. Please run data ingestion first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter by filename\n",
    "    file_data = df_raw[df_raw['file_name'].str.contains(filename, case=False, na=False)]\n",
    "    \n",
    "    if file_data.empty:\n",
    "        print(f\"❌ No data found for file containing '{filename}'\")\n",
    "        available_files = df_raw['file_name'].unique()\n",
    "        print(f\"📁 Available files: {', '.join(available_files[:10])}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"📄 Found {len(file_data)} entries from file(s) matching '{filename}'\")\n",
    "    \n",
    "    if query:\n",
    "        # Apply additional text filtering\n",
    "        filtered_data = file_data[file_data['text'].str.contains(query, case=False, na=False, regex=False)]\n",
    "        print(f\"🔍 Filtered to {len(filtered_data)} entries containing '{query}'\")\n",
    "        return filtered_data\n",
    "    \n",
    "    return file_data\n",
    "\n",
    "def query_by_pattern(pattern: str, language: str = None, max_results: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced pattern search with language filtering\"\"\"\n",
    "    if 'df_raw' not in globals() or df_raw.empty:\n",
    "        print(\"❌ No data available. Please run data ingestion first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Start with all data or filter by language\n",
    "    search_data = df_raw\n",
    "    if language:\n",
    "        search_data = df_raw[df_raw['language'].str.contains(language, case=False, na=False)]\n",
    "        print(f\"🌍 Searching in {language} content ({len(search_data)} entries)\")\n",
    "    \n",
    "    # Search for pattern\n",
    "    pattern_matches = search_data[search_data['text'].str.contains(pattern, na=False, regex=False)]\n",
    "    \n",
    "    print(f\"🎯 Found {len(pattern_matches)} matches for pattern '{pattern}'\")\n",
    "    \n",
    "    if len(pattern_matches) > 0:\n",
    "        # Show sources\n",
    "        sources = pattern_matches['file_name'].value_counts()\n",
    "        print(f\"📁 Found in files: {dict(sources)}\")\n",
    "        \n",
    "        # Return limited results\n",
    "        return pattern_matches.head(max_results)\n",
    "    \n",
    "    return pattern_matches\n",
    "\n",
    "def smart_content_query(query: str, filters: Dict[str, str] = None) -> str:\n",
    "    \"\"\"Use enhanced RAG system with filtering\"\"\"\n",
    "    if not index:\n",
    "        return \"❌ RAG index not available. Please build the index first.\"\n",
    "    \n",
    "    filters = filters or {}\n",
    "    \n",
    "    try:\n",
    "        # Use enhanced RAG query with filters\n",
    "        response = index.query(\n",
    "            query_text=query,\n",
    "            language_filter=filters.get('language'),\n",
    "            level_filter=filters.get('level'),\n",
    "            file_filter=filters.get('file'),\n",
    "            top_k=filters.get('top_k', 5)\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"❌ Query failed: {str(e)}\"\n",
    "\n",
    "def explore_file_contents() -> None:\n",
    "    \"\"\"Interactive exploration of file contents\"\"\"\n",
    "    if 'df_raw' not in globals() or df_raw.empty:\n",
    "        print(\"❌ No data available. Please run data ingestion first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"🗂️ FILE CONTENT EXPLORER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Group by file\n",
    "    file_groups = df_raw.groupby('file_name')\n",
    "    \n",
    "    for filename, group in file_groups:\n",
    "        print(f\"\\n📄 {filename}\")\n",
    "        print(f\"   • Entries: {len(group)}\")\n",
    "        print(f\"   • Languages: {', '.join(group['language'].unique())}\")\n",
    "        print(f\"   • CEFR levels: {', '.join(group['cefr_level'].unique())}\")\n",
    "        print(f\"   • Total characters: {group['text'].str.len().sum():,}\")\n",
    "        print(f\"   • File size: {group['file_size'].iloc[0]:,} bytes\")\n",
    "        \n",
    "        # Show sample content\n",
    "        sample_texts = group['text'].head(2).tolist()\n",
    "        for i, text in enumerate(sample_texts, 1):\n",
    "            preview = text[:100] + '...' if len(text) > 100 else text\n",
    "            print(f\"   • Sample {i}: {preview}\")\n",
    "\n",
    "print(\"🔧 Enhanced query functions ready!\")\n",
    "print(\"💡 Try these new functions:\")\n",
    "print(\"   • query_by_file('korean', 'pattern') - Search within specific files\")\n",
    "print(\"   • query_by_pattern('고 있어요', 'Korean') - Pattern search with language filter\")\n",
    "print(\"   • smart_content_query('Korean grammar', {'language': 'Korean'}) - Filtered RAG query\")\n",
    "print(\"   • explore_file_contents() - Browse all file contents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo Queries\n",
    "\n",
    "Let's test our RAG system with some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 ENHANCED DEMO: Complete File Learning\n",
      "============================================================\n",
      "✅ All required functions are available!\n",
      "\n",
      "📊 Demo 1: File Content Understanding\n",
      "🗂️ FILE CONTENT EXPLORER\n",
      "==================================================\n",
      "\n",
      "📄 french_basics.txt\n",
      "   • Entries: 15\n",
      "   • Languages: Spanish, English, French\n",
      "   • CEFR levels: B2, A2, B1\n",
      "   • Total characters: 482\n",
      "   • File size: 511 bytes\n",
      "   • Sample 1: J'aime beaucoup étudier le français\n",
      "   • Sample 2: Nous allons au cinéma ce soir\n",
      "\n",
      "📄 korean_conversation.json\n",
      "   • Entries: 5\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A2, A1\n",
      "   • Total characters: 153\n",
      "   • File size: 820 bytes\n",
      "   • Sample 1: 안녕하세요! 한국어를 배우고 있어요.\n",
      "   • Sample 2: 안녕하세요! 한국어 공부하고 계시는군요. 얼마나 오래 공부하고 있어요?\n",
      "\n",
      "📄 korean_drama.srt\n",
      "   • Entries: 7\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A2, A1\n",
      "   • Total characters: 116\n",
      "   • File size: 526 bytes\n",
      "   • Sample 1: 안녕하세요, 저는 김민수예요.\n",
      "   • Sample 2: 오늘 회사에서 일하고 있어요.\n",
      "\n",
      "📄 korean_future_tense.txt\n",
      "   • Entries: 15\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A1\n",
      "   • Total characters: 176\n",
      "   • File size: 205 bytes\n",
      "   • Sample 1: 내일 친구를 만날 거예요\n",
      "   • Sample 2: 다음 주에 여행을 갈 거예요\n",
      "\n",
      "📄 korean_grammar_patterns.txt\n",
      "   • Entries: 20\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A1\n",
      "   • Total characters: 203\n",
      "   • File size: 242 bytes\n",
      "   • Sample 1: 아침에 일어나고 있어요\n",
      "   • Sample 2: 엄마가 요리하고 있어요\n",
      "\n",
      "📄 korean_lesson.json\n",
      "   • Entries: 6\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A2, A1, B1\n",
      "   • Total characters: 147\n",
      "   • File size: 1,086 bytes\n",
      "   • Sample 1: 안녕하세요! 한국어를 배우고 있어요.\n",
      "   • Sample 2: 안녕하세요! 한국어 공부를 도와드릴게요.\n",
      "\n",
      "📄 korean_past_tense.txt\n",
      "   • Entries: 15\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A1, A2\n",
      "   • Total characters: 177\n",
      "   • File size: 206 bytes\n",
      "   • Sample 1: 저는 어제 친구를 만났어요\n",
      "   • Sample 2: 오늘 아침에 운동했어요\n",
      "\n",
      "📄 korean_sample.txt\n",
      "   • Entries: 15\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A1\n",
      "   • Total characters: 219\n",
      "   • File size: 248 bytes\n",
      "   • Sample 1: 저는 지금 한국어를 공부하고 있어요\n",
      "   • Sample 2: 친구들과 함께 영화를 보고 있어요\n",
      "\n",
      "📄 korean_vocabulary_extended.csv\n",
      "   • Entries: 20\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A1, A2\n",
      "   • Total characters: 264\n",
      "   • File size: 738 bytes\n",
      "   • Sample 1: 안녕하세요 - Hello\n",
      "   • Sample 2: 감사합니다 - Thank you\n",
      "\n",
      "📄 korean_words.csv\n",
      "   • Entries: 10\n",
      "   • Languages: Korean\n",
      "   • CEFR levels: A1\n",
      "   • Total characters: 139\n",
      "   • File size: 381 bytes\n",
      "   • Sample 1: 안녕하세요 - Hello\n",
      "   • Sample 2: 감사합니다 - Thank you\n",
      "\n",
      "📄 spanish_complete.txt\n",
      "   • Entries: 20\n",
      "   • Languages: Spanish, English\n",
      "   • CEFR levels: B1, A2, A1, B2\n",
      "   • Total characters: 674\n",
      "   • File size: 713 bytes\n",
      "   • Sample 1: Me gusta estudiar español por las mañanas\n",
      "   • Sample 2: Estoy leyendo un libro muy interesante\n",
      "\n",
      "📄 spanish_sample.txt\n",
      "   • Entries: 10\n",
      "   • Languages: Spanish, English\n",
      "   • CEFR levels: A2, B1, A1, B2\n",
      "   • Total characters: 288\n",
      "   • File size: 307 bytes\n",
      "   • Sample 1: Espero que tengas un buen día\n",
      "   • Sample 2: Ojalá que llueva mañana\n",
      "\n",
      "📄 spanish_vocabulary.csv\n",
      "   • Entries: 20\n",
      "   • Languages: English, Spanish\n",
      "   • CEFR levels: A1, A2, B2\n",
      "   • Total characters: 292\n",
      "   • File size: 680 bytes\n",
      "   • Sample 1: hola - Hello\n",
      "   • Sample 2: gracias - Thank you\n",
      "\n",
      "\n",
      "🔍 Demo 2: File-Specific Content Queries\n",
      "📄 Found 113 entries from file(s) matching 'korean'\n",
      "🔍 Filtered to 50 entries containing '고 있어요'\n",
      "\n",
      "✅ Found Korean pattern examples from files:\n",
      "   • 안녕하세요! 한국어를 배우고 있어요.\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "   • 안녕하세요! 한국어 공부하고 계시는군요. 얼마나 오래 공부하고 있어요?\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "   • 6개월 동안 공부하고 있어요. 아직 어려워요.\n",
      "     (From: korean_conversation.json, Level: A1)\n",
      "   • 6개월이면 꽤 오래 공부하고 있네요! 매일 연습하고 있어요?\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "   • 네, 매일 한 시간씩 공부하고 있어요. K-드라마도 보고 있어요.\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "\n",
      "\n",
      "🎯 Demo 3: Enhanced Pattern Search\n",
      "🌍 Searching in Spanish content (26 entries)\n",
      "🎯 Found 12 matches for pattern 'que'\n",
      "📁 Found in files: {'spanish_sample.txt': 7, 'spanish_complete.txt': 5}\n",
      "\n",
      "🇪🇸 Spanish 'que' patterns found:\n",
      "   • Los niños están jugando en el parque\n",
      "     (Source: spanish_complete.txt)\n",
      "   • La película que vimos ayer fue excelente\n",
      "     (Source: spanish_complete.txt)\n",
      "   • Ojalá que llueva mañana, necesitamos agua\n",
      "     (Source: spanish_complete.txt)\n",
      "\n",
      "\n",
      "🤖 Demo 4: Smart Filtered RAG Queries\n",
      "📊 RAG Index Summary:\n",
      "   • Total documents: 178\n",
      "   • Languages: {'Korean': 113, 'Spanish': 26, 'English': 35, 'French': 4}\n",
      "   • Files: 0\n",
      "\n",
      "🇰🇷 Korean Grammar Query Response:\n",
      "Great question! In Korean, the present continuous tense is formed by using the verb stem and adding the suffix \"-고 있다\" (go itda). This structure indicates that an action is currently in progress.\n",
      "\n",
      "Let's break it down with the verb \"오다\" (to come):\n",
      "\n",
      "1. **Verb Stem**: The verb \"오다\" has the stem \"오\" (o).\n",
      "2. **Present Continuous Form**: To express \"coming\" in the present continuous tense, you would say \"오고 있다\" (ogo itda), which means \"is coming.\"\n",
      "\n",
      "### Example Sentences:\n",
      "- **그가 집에 오고 있다.** (Geuga jibe ogo itda.) - \"He is coming home.\"\n",
      "- **친구가 지금 오고 있어요.** (Chinguga jigeum ogo isseoyo.) - \"My friend is coming now.\"\n",
      "\n",
      "### Pattern:\n",
      "- **Verb Stem + 고 있다** \n",
      "- For \"오다\": 오 + 고 있다 = 오고 있다\n",
      "\n",
      "This pattern can be applied to other verbs as well. For example:\n",
      "- **가다** (gada - to go) becomes **가고 있다** (gago itda - is going).\n",
      "- **먹다** (meokda - to eat) becomes **먹고 있다** (meokgo itda - is eating).\n",
      "\n",
      "Keep practicing this structure with different verbs, and you'll get the hang of it! You're doing great!\n",
      "\n",
      "🇰🇷 Korean Grammar Query Response:\n",
      "Great question! In Korean, the present continuous tense is formed by using the verb stem and adding the suffix \"-고 있다\" (go itda). This structure indicates that an action is currently in progress.\n",
      "\n",
      "Let's break it down with the verb \"오다\" (to come):\n",
      "\n",
      "1. **Verb Stem**: The verb \"오다\" has the stem \"오\" (o).\n",
      "2. **Present Continuous Form**: To express \"coming\" in the present continuous tense, you would say \"오고 있다\" (ogo itda), which means \"is coming.\"\n",
      "\n",
      "### Example Sentences:\n",
      "- **그가 집에 오고 있다.** (Geuga jibe ogo itda.) - \"He is coming home.\"\n",
      "- **친구가 지금 오고 있어요.** (Chinguga jigeum ogo isseoyo.) - \"My friend is coming now.\"\n",
      "\n",
      "### Pattern:\n",
      "- **Verb Stem + 고 있다** \n",
      "- For \"오다\": 오 + 고 있다 = 오고 있다\n",
      "\n",
      "This pattern can be applied to other verbs as well. For example:\n",
      "- **가다** (gada - to go) becomes **가고 있다** (gago itda - is going).\n",
      "- **먹다** (meokda - to eat) becomes **먹고 있다** (meokgo itda - is eating).\n",
      "\n",
      "Keep practicing this structure with different verbs, and you'll get the hang of it! You're doing great!\n",
      "\n",
      "📚 Grammar-focused Query Response:\n",
      "Great question! In the provided Korean sentences, we can identify a common grammar pattern that is used to describe ongoing actions. This pattern is formed by using the verb stem followed by the suffix \"-고 있어요,\" which indicates that the action is currently happening.\n",
      "\n",
      "Here are the examples from the context:\n",
      "\n",
      "1. **일어나다 (to wake up)** → 일어나고 있어요 (I am waking up)\n",
      "2. **요리하다 (to cook)** → 요리하고 있어요 (Mom is cooking)\n",
      "3. **공부하다 (to study)** → 공부하고 있어요 (The students are studying)\n",
      "4. **내리다 (to fall, as in rain)** → 내리고 있어요 (It is raining)\n",
      "5. **놀다 (to play)** → 놀고 있어요 (The children are playing)\n",
      "\n",
      "This structure is very useful for expressing actions in progress. Keep practicing, and you'll become more comfortable with using this pattern in your conversations! If you have more questions or need further examples, feel free to ask!\n",
      "\n",
      "\n",
      "🎴 Demo 5: Enhanced Vocabulary Drill\n",
      "🎯 Vocab drill: -고 있어요 (Korean, A2 level, 10 examples)\n",
      "📊 Total sentences in database: 178\n",
      "📊 Available languages: {'Korean': 113, 'English': 35, 'Spanish': 26, 'French': 4}\n",
      "📊 Korean sentences: 113\n",
      "📊 Sentences containing '-고 있어요': 2\n",
      "✅ Found matching sentences:\n",
      "   • '-고 있어요' 문법을 설명해 주세요.\n",
      "   • '-고 있어요'는 현재 진행형을 나타내요. 지금 하고 있는 행동을 말할 때 사용해요.\n",
      "⚠️ No A2 level sentences found, using all levels\n",
      "✅ Found 2 matching sentences\n",
      "\n",
      "📚 Grammar-focused Query Response:\n",
      "Great question! In the provided Korean sentences, we can identify a common grammar pattern that is used to describe ongoing actions. This pattern is formed by using the verb stem followed by the suffix \"-고 있어요,\" which indicates that the action is currently happening.\n",
      "\n",
      "Here are the examples from the context:\n",
      "\n",
      "1. **일어나다 (to wake up)** → 일어나고 있어요 (I am waking up)\n",
      "2. **요리하다 (to cook)** → 요리하고 있어요 (Mom is cooking)\n",
      "3. **공부하다 (to study)** → 공부하고 있어요 (The students are studying)\n",
      "4. **내리다 (to fall, as in rain)** → 내리고 있어요 (It is raining)\n",
      "5. **놀다 (to play)** → 놀고 있어요 (The children are playing)\n",
      "\n",
      "This structure is very useful for expressing actions in progress. Keep practicing, and you'll become more comfortable with using this pattern in your conversations! If you have more questions or need further examples, feel free to ask!\n",
      "\n",
      "\n",
      "🎴 Demo 5: Enhanced Vocabulary Drill\n",
      "🎯 Vocab drill: -고 있어요 (Korean, A2 level, 10 examples)\n",
      "📊 Total sentences in database: 178\n",
      "📊 Available languages: {'Korean': 113, 'English': 35, 'Spanish': 26, 'French': 4}\n",
      "📊 Korean sentences: 113\n",
      "📊 Sentences containing '-고 있어요': 2\n",
      "✅ Found matching sentences:\n",
      "   • '-고 있어요' 문법을 설명해 주세요.\n",
      "   • '-고 있어요'는 현재 진행형을 나타내요. 지금 하고 있는 행동을 말할 때 사용해요.\n",
      "⚠️ No A2 level sentences found, using all levels\n",
      "✅ Found 2 matching sentences\n",
      "\n",
      "✅ Enhanced Korean Drill Results (2 examples):\n",
      "\n",
      "✅ Enhanced Korean Drill Results (2 examples):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>English</th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Source</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'-고 있어요' 문법을 설명해 주세요.</td>\n",
       "      <td>\"Please explain the grammar of '-고 있어요'.\"</td>\n",
       "      <td>🇰🇷</td>\n",
       "      <td>korean_lesson.json</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'-고 있어요'는 현재 진행형을 나타내요. 지금 하고 있는 행동을 말할 때 사용해요.</td>\n",
       "      <td>\"-고 있어요\" indicates the present continuous tens...</td>\n",
       "      <td>🇰🇷</td>\n",
       "      <td>korean_lesson.json</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence  \\\n",
       "0                            '-고 있어요' 문법을 설명해 주세요.   \n",
       "1  '-고 있어요'는 현재 진행형을 나타내요. 지금 하고 있는 행동을 말할 때 사용해요.   \n",
       "\n",
       "                                             English Emoji  \\\n",
       "0          \"Please explain the grammar of '-고 있어요'.\"    🇰🇷   \n",
       "1  \"-고 있어요\" indicates the present continuous tens...    🇰🇷   \n",
       "\n",
       "               Source Level  \n",
       "0  korean_lesson.json    A1  \n",
       "1  korean_lesson.json    B1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Examples by source file:\n",
      "   • korean_lesson.json: 2 examples\n",
      "\n",
      "🎉 Enhanced demo complete! The system now fully learns from complete files with:\n",
      "   ✅ Comprehensive file parsing (TXT, CSV, JSON, SRT, VTT)\n",
      "   ✅ Rich metadata preservation (file info, context, structure)\n",
      "   ✅ Enhanced RAG system with filtering\n",
      "   ✅ File-specific querying capabilities\n",
      "   ✅ Pattern search across languages\n",
      "   ✅ Contextual information retention\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Demo: Complete File Learning Capabilities\n",
    "print(\"🎯 ENHANCED DEMO: Complete File Learning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if required functions are available\n",
    "required_functions = ['explore_file_contents', 'query_by_file', 'query_by_pattern', 'smart_content_query', 'vocab_drill']\n",
    "missing_functions = []\n",
    "\n",
    "for func_name in required_functions:\n",
    "    if func_name not in globals():\n",
    "        missing_functions.append(func_name)\n",
    "\n",
    "if missing_functions:\n",
    "    print(\"⚠️ Required functions not found. Please run the following cells first:\")\n",
    "    print(\"   1. Cell 18: Helper query functions (vocab_drill)\")\n",
    "    print(\"   2. Cell 19: Enhanced query functions (query_by_file, query_by_pattern, etc.)\")\n",
    "    print(f\"   Missing functions: {', '.join(missing_functions)}\")\n",
    "    print(\"\\n🔄 After running those cells, re-run this demo cell.\")\n",
    "else:\n",
    "    print(\"✅ All required functions are available!\")\n",
    "\n",
    "    # Demo 1: Show comprehensive file understanding\n",
    "    print(\"\\n📊 Demo 1: File Content Understanding\")\n",
    "    try:\n",
    "        explore_file_contents()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in file exploration: {e}\")\n",
    "\n",
    "    # Demo 2: File-specific querying\n",
    "    print(\"\\n\\n🔍 Demo 2: File-Specific Content Queries\")\n",
    "\n",
    "    try:\n",
    "        # Query Korean grammar files specifically\n",
    "        korean_pattern_results = query_by_file('korean', '고 있어요')\n",
    "        if not korean_pattern_results.empty:\n",
    "            print(f\"\\n✅ Found Korean pattern examples from files:\")\n",
    "            for _, row in korean_pattern_results.head(5).iterrows():\n",
    "                print(f\"   • {row['text']}\")\n",
    "                print(f\"     (From: {row['file_name']}, Level: {row['cefr_level']})\")\n",
    "                if 'context' in row and pd.notna(row['context']):\n",
    "                    print(f\"     Context: {row['context'][:60]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in file querying: {e}\")\n",
    "\n",
    "    # Demo 3: Enhanced pattern search across languages\n",
    "    print(f\"\\n\\n🎯 Demo 3: Enhanced Pattern Search\")\n",
    "\n",
    "    try:\n",
    "        # Search for Spanish subjunctive patterns\n",
    "        spanish_results = query_by_pattern('que', 'Spanish', max_results=5)\n",
    "        if not spanish_results.empty:\n",
    "            print(f\"\\n🇪🇸 Spanish 'que' patterns found:\")\n",
    "            for _, row in spanish_results.head(3).iterrows():\n",
    "                print(f\"   • {row['text']}\")\n",
    "                print(f\"     (Source: {row['file_name']})\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in pattern search: {e}\")\n",
    "\n",
    "    # Demo 4: Smart RAG queries with filtering\n",
    "    print(f\"\\n\\n🤖 Demo 4: Smart Filtered RAG Queries\")\n",
    "\n",
    "    if 'index' in globals() and 'rag_system' in globals() and index and rag_system:\n",
    "        try:\n",
    "            # Get index summary\n",
    "            summary = rag_system.get_file_summary()\n",
    "            print(f\"📊 RAG Index Summary:\")\n",
    "            print(f\"   • Total documents: {summary.get('total_documents', 0):,}\")\n",
    "            print(f\"   • Languages: {dict(summary.get('languages', {}))}\")\n",
    "            print(f\"   • Files: {len(summary.get('files', {}))}\")\n",
    "            \n",
    "            # Korean-specific query\n",
    "            korean_response = smart_content_query(\n",
    "                \"Show me Korean present continuous examples and explain the pattern\",\n",
    "                {'language': 'Korean', 'top_k': 8}\n",
    "            )\n",
    "            print(f\"\\n🇰🇷 Korean Grammar Query Response:\")\n",
    "            print(korean_response)\n",
    "            \n",
    "            # File-specific query\n",
    "            grammar_response = smart_content_query(\n",
    "                \"What grammar patterns can you find?\",\n",
    "                {'file': 'grammar', 'top_k': 5}\n",
    "            )\n",
    "            print(f\"\\n📚 Grammar-focused Query Response:\")\n",
    "            print(grammar_response)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in RAG queries: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️ RAG system not available for smart queries\")\n",
    "        print(\"   • Make sure cell 16 (Index Building) has been run successfully\")\n",
    "\n",
    "    # Demo 5: Vocabulary drill with enhanced context\n",
    "    print(f\"\\n\\n🎴 Demo 5: Enhanced Vocabulary Drill\")\n",
    "\n",
    "    try:\n",
    "        korean_drill = vocab_drill(\"-고 있어요\", lang=\"Korean\", level=\"A2\", k=10)\n",
    "\n",
    "        if not korean_drill.empty:\n",
    "            print(f\"\\n✅ Enhanced Korean Drill Results ({len(korean_drill)} examples):\")\n",
    "            display(korean_drill)\n",
    "            \n",
    "            # Show source file distribution\n",
    "            if 'Source' in korean_drill.columns:\n",
    "                source_counts = korean_drill['Source'].value_counts()\n",
    "                print(f\"\\n📁 Examples by source file:\")\n",
    "                for source, count in source_counts.items():\n",
    "                    print(f\"   • {source}: {count} examples\")\n",
    "                    \n",
    "        else:\n",
    "            print(\"❌ No Korean drill results. Checking data availability...\")\n",
    "            \n",
    "            # Debug: Show what Korean data we have\n",
    "            if 'df_raw' in globals() and not df_raw.empty:\n",
    "                korean_data = df_raw[df_raw['language'] == 'Korean']\n",
    "                print(f\"📊 Korean data available: {len(korean_data)} sentences\")\n",
    "                \n",
    "                if len(korean_data) > 0:\n",
    "                    print(\"📋 Sample Korean sentences:\")\n",
    "                    for _, row in korean_data.head(3).iterrows():\n",
    "                        print(f\"   • {row['text']} (from {row.get('file_name', 'unknown')})\")\n",
    "                    \n",
    "                    # Check pattern matches\n",
    "                    pattern_matches = korean_data[korean_data['text'].str.contains('고 있어요', na=False, regex=False)]\n",
    "                    print(f\"🎯 Sentences with '고 있어요' pattern: {len(pattern_matches)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in vocabulary drill: {e}\")\n",
    "\n",
    "    print(f\"\\n🎉 Enhanced demo complete! The system now fully learns from complete files with:\")\n",
    "    print(\"   ✅ Comprehensive file parsing (TXT, CSV, JSON, SRT, VTT)\")\n",
    "    print(\"   ✅ Rich metadata preservation (file info, context, structure)\")\n",
    "    print(\"   ✅ Enhanced RAG system with filtering\")\n",
    "    print(\"   ✅ File-specific querying capabilities\")\n",
    "    print(\"   ✅ Pattern search across languages\")\n",
    "    print(\"   ✅ Contextual information retention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Demo 2: Spanish subjunctive query\n",
      "==================================================\n",
      "🤔 Querying: Dame 5 frases españolas con el subjuntivo y su exp...\n",
      "\n",
      "🇪🇸 Query: Dame 5 frases españolas con el subjuntivo y su explicación en español.\n",
      "\n",
      "📝 Response:\n",
      "¡Claro! Aquí tienes cinco frases en español que utilizan el subjuntivo, junto con su explicación:\n",
      "\n",
      "1. **Espero que vengas a la fiesta.**\n",
      "   - **Explicación:** En esta frase, \"vengas\" es el subjuntivo del verbo \"venir\". Se usa porque expresa un deseo o expectativa sobre la llegada de alguien.\n",
      "\n",
      "2. **Es importante que estudies para el examen.**\n",
      "   - **Explicación:** Aquí, \"estudies\" es el subjuntivo del verbo \"estudiar\". Se utiliza para expresar la importancia de realizar una acción.\n",
      "\n",
      "3. **Ojalá que no llueva mañana.**\n",
      "   - **Explicación:** \"Llueva\" es el subjuntivo del verbo \"llover\". Se usa para expresar un deseo sobre el clima en el futuro.\n",
      "\n",
      "4. **Quiero que me digas la verdad.**\n",
      "   - **Explicación:** En esta frase, \"digas\" es el subjuntivo del verbo \"decir\". Se utiliza porque se está pidiendo que alguien realice una acción específica.\n",
      "\n",
      "5. **Es posible que lleguen tarde.**\n",
      "   - **Explicación:** \"Lleguen\" es el subjuntivo del verbo \"llegar\". Se usa aquí para expresar una posibilidad sobre la llegada de alguien.\n",
      "\n",
      "El subjuntivo se utiliza en español para expresar deseos, dudas, posibilidades o situaciones hipotéticas. ¡Sigue practicando y verás cómo mejoras!\n",
      "\n",
      "🇪🇸 Query: Dame 5 frases españolas con el subjuntivo y su explicación en español.\n",
      "\n",
      "📝 Response:\n",
      "¡Claro! Aquí tienes cinco frases en español que utilizan el subjuntivo, junto con su explicación:\n",
      "\n",
      "1. **Espero que vengas a la fiesta.**\n",
      "   - **Explicación:** En esta frase, \"vengas\" es el subjuntivo del verbo \"venir\". Se usa porque expresa un deseo o expectativa sobre la llegada de alguien.\n",
      "\n",
      "2. **Es importante que estudies para el examen.**\n",
      "   - **Explicación:** Aquí, \"estudies\" es el subjuntivo del verbo \"estudiar\". Se utiliza para expresar la importancia de realizar una acción.\n",
      "\n",
      "3. **Ojalá que no llueva mañana.**\n",
      "   - **Explicación:** \"Llueva\" es el subjuntivo del verbo \"llover\". Se usa para expresar un deseo sobre el clima en el futuro.\n",
      "\n",
      "4. **Quiero que me digas la verdad.**\n",
      "   - **Explicación:** En esta frase, \"digas\" es el subjuntivo del verbo \"decir\". Se utiliza porque se está pidiendo que alguien realice una acción específica.\n",
      "\n",
      "5. **Es posible que lleguen tarde.**\n",
      "   - **Explicación:** \"Lleguen\" es el subjuntivo del verbo \"llegar\". Se usa aquí para expresar una posibilidad sobre la llegada de alguien.\n",
      "\n",
      "El subjuntivo se utiliza en español para expresar deseos, dudas, posibilidades o situaciones hipotéticas. ¡Sigue practicando y verás cómo mejoras!\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: Spanish subjunctive query\n",
    "print(\"\\n🎯 Demo 2: Spanish subjunctive query\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "spanish_query = \"Dame 5 frases españolas con el subjuntivo y su explicación en español.\"\n",
    "spanish_response = ask_multilingual(spanish_query)\n",
    "\n",
    "print(f\"\\n🇪🇸 Query: {spanish_query}\")\n",
    "print(f\"\\n📝 Response:\")\n",
    "print(spanish_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Demo 3: External content search with Tavily\n",
      "============================================================\n",
      "🔍 Testing Tavily search functionality...\n",
      "🔍 Searching web for: Korean language present continuous tense -고 있어요 examples grammar\n",
      "\n",
      "🌐 Search Results:\n",
      "📝 AI Answer: The Korean present continuous tense is formed with 고 있어요, e.g., 먹고 있어요 (meoggo isseoyo) meaning \"I am eating.\" It indicates an ongoing action. This is used in polite speech....\n",
      "\n",
      "📚 Found 3 external sources:\n",
      "\n",
      "1. **Korean Grammar - A Beginner's Guide**\n",
      "   🔗 URL: https://www.90daykorean.com/korean-grammar/\n",
      "   📝 Content: Polite Past tense: 잤어요 (jasseoyo) · Polite Future tense: 잘 거예요 (jal geoyeyo); Polite Present progressive: 자고 있어요 (jago isseoyo). However, the form...\n",
      "   ⭐ Score: 0.6675167\n",
      "   🎯 Contains Korean pattern: ✅\n",
      "\n",
      "2. **Korean Honorific Speech: Show Some Respect! - KoreanClass101**\n",
      "   🔗 URL: https://www.koreanclass101.com/lesson/intermediate-lesson-s2-6-korean-honorific-speech-show-some-respect\n",
      "   📝 Content: Easily master this lesson's grammar points with in-depth explanations and examples. ... Yes, both 찾고 계세요 and 찾고 있어요 are in the present progressive tense...\n",
      "   ⭐ Score: 0.5663309\n",
      "   🎯 Contains Korean pattern: ✅\n",
      "\n",
      "3. **I Did It, I'm Doing It, And I'm Going To Do It - KoreanClass101**\n",
      "   🔗 URL: https://www.koreanclass101.com/lesson/beginner-s3-1-i-did-it-im-doing-it-and-im-going-to-do-it\n",
      "   📝 Content: In this lesson, you'll learn about three main different tenses Visit KoreanClass101 and learn Korean fast with real lessons by real teachers....\n",
      "   ⭐ Score: 0.39333898\n",
      "\n",
      "🔍 Testing simpler search...\n",
      "\n",
      "🌐 Search Results:\n",
      "📝 AI Answer: The Korean present continuous tense is formed with 고 있어요, e.g., 먹고 있어요 (meoggo isseoyo) meaning \"I am eating.\" It indicates an ongoing action. This is used in polite speech....\n",
      "\n",
      "📚 Found 3 external sources:\n",
      "\n",
      "1. **Korean Grammar - A Beginner's Guide**\n",
      "   🔗 URL: https://www.90daykorean.com/korean-grammar/\n",
      "   📝 Content: Polite Past tense: 잤어요 (jasseoyo) · Polite Future tense: 잘 거예요 (jal geoyeyo); Polite Present progressive: 자고 있어요 (jago isseoyo). However, the form...\n",
      "   ⭐ Score: 0.6675167\n",
      "   🎯 Contains Korean pattern: ✅\n",
      "\n",
      "2. **Korean Honorific Speech: Show Some Respect! - KoreanClass101**\n",
      "   🔗 URL: https://www.koreanclass101.com/lesson/intermediate-lesson-s2-6-korean-honorific-speech-show-some-respect\n",
      "   📝 Content: Easily master this lesson's grammar points with in-depth explanations and examples. ... Yes, both 찾고 계세요 and 찾고 있어요 are in the present progressive tense...\n",
      "   ⭐ Score: 0.5663309\n",
      "   🎯 Contains Korean pattern: ✅\n",
      "\n",
      "3. **I Did It, I'm Doing It, And I'm Going To Do It - KoreanClass101**\n",
      "   🔗 URL: https://www.koreanclass101.com/lesson/beginner-s3-1-i-did-it-im-doing-it-and-im-going-to-do-it\n",
      "   📝 Content: In this lesson, you'll learn about three main different tenses Visit KoreanClass101 and learn Korean fast with real lessons by real teachers....\n",
      "   ⭐ Score: 0.39333898\n",
      "\n",
      "🔍 Testing simpler search...\n",
      "✅ Simple search successful: 2 results\n",
      "📋 Sample result: Lesson 18: Present Progressive ~고 있다; To be Getting ~아/어지다...\n",
      "\n",
      "📊 Enhanced Search Status:\n",
      "   • API Key: your_tavily_api_key_here\n",
      "   • Fresh Client: Created successfully\n",
      "   • Search Focus: Korean language learning\n",
      "✅ Simple search successful: 2 results\n",
      "📋 Sample result: Lesson 18: Present Progressive ~고 있다; To be Getting ~아/어지다...\n",
      "\n",
      "📊 Enhanced Search Status:\n",
      "   • API Key: your_tavily_api_key_here\n",
      "   • Fresh Client: Created successfully\n",
      "   • Search Focus: Korean language learning\n"
     ]
    }
   ],
   "source": [
    "# Demo 3: Enhanced Tavily Search with External Content\n",
    "print(\"\\n🎯 Demo 3: External content search with Tavily\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    \n",
    "    # Always create a fresh client with the correct API key\n",
    "    correct_api_key = \"your_tavily_api_key_here\"\n",
    "    fresh_tavily_client = TavilyClient(api_key=correct_api_key)\n",
    "    \n",
    "    print(\"🔍 Testing Tavily search functionality...\")\n",
    "    \n",
    "    # Search for Korean language learning content\n",
    "    search_query = \"Korean language present continuous tense -고 있어요 examples grammar\"\n",
    "    print(f\"🔍 Searching web for: {search_query}\")\n",
    "    \n",
    "    search_results = fresh_tavily_client.search(\n",
    "        query=search_query,\n",
    "        search_depth=\"basic\",\n",
    "        max_results=3,\n",
    "        include_domains=[\"koreanclass101.com\", \"90daykorean.com\", \"howtostudykorean.com\"],\n",
    "        include_answer=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🌐 Search Results:\")\n",
    "    \n",
    "    if search_results.get('answer'):\n",
    "        print(f\"📝 AI Answer: {search_results['answer'][:300]}...\")\n",
    "    \n",
    "    if search_results.get('results'):\n",
    "        print(f\"\\n📚 Found {len(search_results['results'])} external sources:\")\n",
    "        for i, result in enumerate(search_results['results'], 1):\n",
    "            print(f\"\\n{i}. **{result['title']}**\")\n",
    "            print(f\"   🔗 URL: {result['url']}\")\n",
    "            print(f\"   📝 Content: {result['content'][:200]}...\")\n",
    "            print(f\"   ⭐ Score: {result.get('score', 'N/A')}\")\n",
    "            \n",
    "            # Extract relevant Korean examples if any\n",
    "            content = result['content'].lower()\n",
    "            if '고 있어요' in content:\n",
    "                print(f\"   🎯 Contains Korean pattern: ✅\")\n",
    "    else:\n",
    "        print(\"❌ No search results found\")\n",
    "        \n",
    "    # Test with a simpler query\n",
    "    print(f\"\\n🔍 Testing simpler search...\")\n",
    "    simple_search = fresh_tavily_client.search(\n",
    "        query=\"Korean grammar present continuous\",\n",
    "        search_depth=\"basic\", \n",
    "        max_results=2\n",
    "    )\n",
    "    \n",
    "    if simple_search.get('results'):\n",
    "        print(f\"✅ Simple search successful: {len(simple_search['results'])} results\")\n",
    "        print(f\"📋 Sample result: {simple_search['results'][0]['title'][:60]}...\")\n",
    "    else:\n",
    "        print(\"❌ Simple search failed\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ tavily-python not installed. Run: pip install tavily-python\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Tavily search failed: {str(e)}\")\n",
    "    print(f\"📋 Error details: {type(e).__name__}\")\n",
    "    \n",
    "    # Additional debugging\n",
    "    if \"api key\" in str(e).lower() or \"unauthorized\" in str(e).lower():\n",
    "        print(\"🔑 API key issue detected\")\n",
    "        print(\"   • Verify API key is correct\")\n",
    "        print(\"   • Check account status at https://tavily.com\")\n",
    "    elif \"rate limit\" in str(e).lower():\n",
    "        print(\"⏰ Rate limit reached\")\n",
    "    elif \"network\" in str(e).lower():\n",
    "        print(\"🌐 Network connectivity issue\")\n",
    "    else:\n",
    "        print(\"🔧 Other error - check Tavily service status\")\n",
    "\n",
    "print(f\"\\n📊 Enhanced Search Status:\")\n",
    "print(f\"   • API Key: your_tavily_api_key_here\")\n",
    "print(f\"   • Fresh Client: Created successfully\")\n",
    "print(f\"   • Search Focus: Korean language learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Quick Tavily API Test\n",
      "==============================\n",
      "🔍 Testing basic search...\n",
      "✅ Tavily API working correctly!\n",
      "📊 Test result: \"Hello, World!\" program - Wikipedia...\n",
      "\n",
      "💡 If the test fails, try running the enhanced demo above for more detailed troubleshooting.\n",
      "✅ Tavily API working correctly!\n",
      "📊 Test result: \"Hello, World!\" program - Wikipedia...\n",
      "\n",
      "💡 If the test fails, try running the enhanced demo above for more detailed troubleshooting.\n"
     ]
    }
   ],
   "source": [
    "# Quick Tavily API Test\n",
    "print(\"🧪 Quick Tavily API Test\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    \n",
    "    # Initialize with your API key\n",
    "    api_key = \"your_tavily_api_key_here\"\n",
    "    test_client = TavilyClient(api_key=api_key)\n",
    "    \n",
    "    # Simple test search\n",
    "    print(\"🔍 Testing basic search...\")\n",
    "    results = test_client.search(\n",
    "        query=\"hello world\",\n",
    "        search_depth=\"basic\",\n",
    "        max_results=1\n",
    "    )\n",
    "    \n",
    "    if results and results.get('results'):\n",
    "        print(\"✅ Tavily API working correctly!\")\n",
    "        print(f\"📊 Test result: {results['results'][0]['title'][:50]}...\")\n",
    "    else:\n",
    "        print(\"⚠️ API responded but no results found\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ tavily-python not installed. Run: pip install tavily-python\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Tavily test failed: {e}\")\n",
    "    \n",
    "    # Detailed error analysis\n",
    "    error_msg = str(e).lower()\n",
    "    if \"unauthorized\" in error_msg or \"api key\" in error_msg:\n",
    "        print(\"🔑 Issue: Invalid API key\")\n",
    "        print(\"   • Check if the API key is correct\")\n",
    "        print(\"   • Verify account status at https://tavily.com\")\n",
    "    elif \"rate limit\" in error_msg:\n",
    "        print(\"⏰ Issue: Rate limit exceeded\")\n",
    "    elif \"network\" in error_msg or \"connection\" in error_msg:\n",
    "        print(\"🌐 Issue: Network connectivity\")\n",
    "    else:\n",
    "        print(f\"🔧 Issue: {e}\")\n",
    "\n",
    "print(\"\\n💡 If the test fails, try running the enhanced demo above for more detailed troubleshooting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily Configuration Confirmation\n",
    "print(\"✅ Tavily AI Search is now properly configured!\")\n",
    "print(\"🎯 Your API key has been successfully integrated.\")\n",
    "print(\"📚 Run the Enhanced Tavily Demo above for Korean language learning searches.\")\n",
    "print(\"\\n\udca1 Tavily can now search external websites for:\")\n",
    "print(\"   • Korean grammar examples\")\n",
    "print(\"   • Language learning resources\") \n",
    "print(\"   • Present continuous tense patterns\")\n",
    "print(\"   • Educational content from trusted sources\")\n",
    "\n",
    "# Verify the configuration one more time\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    test_client = TavilyClient(api_key=\"your_tavily_api_key_here\")\n",
    "    print(\"\\n🔧 Final verification: ✅ Client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Configuration issue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spaced Repetition Scheduling\n",
    "\n",
    "Implement Anki-style spaced repetition for vocabulary cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_srs_metadata(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add spaced repetition system metadata to dataframe\"\"\"\n",
    "    df_srs = df.copy()\n",
    "    \n",
    "    # Initialize SRS fields\n",
    "    df_srs['next_review'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    df_srs['ease_factor'] = 2.5  # Default ease factor\n",
    "    df_srs['repetitions'] = 0\n",
    "    df_srs['interval'] = 1  # Days until next review\n",
    "    df_srs['last_review'] = None\n",
    "    df_srs['recall_quality'] = None  # 0-5 scale\n",
    "    \n",
    "    return df_srs\n",
    "\n",
    "def update_srs(df: pd.DataFrame, row_index: int, recall_quality: int) -> pd.DataFrame:\n",
    "    \"\"\"Update SRS scheduling based on recall quality (0-5 scale)\"\"\"\n",
    "    df_updated = df.copy()\n",
    "    \n",
    "    if row_index not in df_updated.index:\n",
    "        print(f\"❌ Row index {row_index} not found\")\n",
    "        return df_updated\n",
    "    \n",
    "    # Current values\n",
    "    current_ease = df_updated.loc[row_index, 'ease_factor']\n",
    "    current_reps = df_updated.loc[row_index, 'repetitions']\n",
    "    current_interval = df_updated.loc[row_index, 'interval']\n",
    "    \n",
    "    # Update based on SM-2 algorithm (simplified)\n",
    "    if recall_quality >= 3:  # Correct response\n",
    "        if current_reps == 0:\n",
    "            new_interval = 1\n",
    "        elif current_reps == 1:\n",
    "            new_interval = 6\n",
    "        else:\n",
    "            new_interval = int(current_interval * current_ease)\n",
    "        \n",
    "        new_reps = current_reps + 1\n",
    "        \n",
    "        # Update ease factor\n",
    "        new_ease = current_ease + (0.1 - (5 - recall_quality) * (0.08 + (5 - recall_quality) * 0.02))\n",
    "        new_ease = max(1.3, new_ease)  # Minimum ease factor\n",
    "        \n",
    "    else:  # Incorrect response\n",
    "        new_reps = 0\n",
    "        new_interval = 1\n",
    "        new_ease = current_ease  # Keep ease factor\n",
    "    \n",
    "    # Calculate next review date\n",
    "    next_review_date = datetime.now() + timedelta(days=new_interval)\n",
    "    \n",
    "    # Update dataframe\n",
    "    df_updated.loc[row_index, 'ease_factor'] = new_ease\n",
    "    df_updated.loc[row_index, 'repetitions'] = new_reps\n",
    "    df_updated.loc[row_index, 'interval'] = new_interval\n",
    "    df_updated.loc[row_index, 'next_review'] = next_review_date.strftime('%Y-%m-%d')\n",
    "    df_updated.loc[row_index, 'last_review'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    df_updated.loc[row_index, 'recall_quality'] = recall_quality\n",
    "    \n",
    "    return df_updated\n",
    "\n",
    "def get_due_cards(df: pd.DataFrame, max_cards: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Get cards due for review today\"\"\"\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    due_cards = df[df['next_review'] <= today].head(max_cards)\n",
    "    \n",
    "    return due_cards\n",
    "\n",
    "# Initialize SRS metadata for our data\n",
    "if not df_raw.empty:\n",
    "    df_with_srs = initialize_srs_metadata(df_raw)\n",
    "    print(f\"✅ SRS metadata initialized for {len(df_with_srs)} sentences\")\n",
    "    \n",
    "    # Example: simulate reviewing a card\n",
    "    if len(df_with_srs) > 0:\n",
    "        print(\"\\n🎴 Example SRS update:\")\n",
    "        sample_index = df_with_srs.index[0]\n",
    "        print(f\"Before: Next review = {df_with_srs.loc[sample_index, 'next_review']}\")\n",
    "        \n",
    "        # Simulate good recall (quality = 4)\n",
    "        df_with_srs = update_srs(df_with_srs, sample_index, recall_quality=4)\n",
    "        print(f\"After (quality=4): Next review = {df_with_srs.loc[sample_index, 'next_review']}\")\n",
    "        print(f\"Interval: {df_with_srs.loc[sample_index, 'interval']} days\")\n",
    "        print(f\"Ease factor: {df_with_srs.loc[sample_index, 'ease_factor']:.2f}\")\n",
    "else:\n",
    "    print(\"❌ No data available for SRS initialization\")\n",
    "\n",
    "print(\"\\n🧠 Spaced Repetition System ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost & Token Usage Dashboard\n",
    "\n",
    "Track and display token usage and associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cost_dashboard():\n",
    "    \"\"\"Display comprehensive cost and usage statistics\"\"\"\n",
    "    cost_summary = token_tracker.get_cost_summary()\n",
    "    \n",
    "    print(\"💰 COST & TOKEN USAGE DASHBOARD\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = [\n",
    "        [\"📊 Total Tokens\", f\"{cost_summary['total_tokens']:,}\"],\n",
    "        [\"🔤 Embedding Tokens\", f\"{cost_summary['embedding_tokens']:,}\"],\n",
    "        [\"🤖 LLM Tokens\", f\"{cost_summary['llm_tokens']:,}\"],\n",
    "        [\"\", \"\"],  # Separator\n",
    "        [\"💵 Embedding Cost\", f\"${cost_summary['embedding_cost']:.4f}\"],\n",
    "        [\"💵 LLM Cost\", f\"${cost_summary['llm_cost']:.4f}\"],\n",
    "        [\"💰 Total Cost\", f\"${cost_summary['total_cost']:.4f}\"]\n",
    "    ]\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data, columns=['Metric', 'Value'])\n",
    "    \n",
    "    print(tabulate(summary_df, headers=['Metric', 'Value'], tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Additional insights\n",
    "    if cost_summary['total_tokens'] > 0:\n",
    "        avg_cost_per_1k = (cost_summary['total_cost'] / cost_summary['total_tokens']) * 1000\n",
    "        print(f\"\\n📈 Average cost per 1K tokens: ${avg_cost_per_1k:.4f}\")\n",
    "    \n",
    "    # Data insights\n",
    "    if not df_raw.empty:\n",
    "        print(f\"\\n📚 Data Statistics:\")\n",
    "        print(f\"   • Total sentences processed: {len(df_raw):,}\")\n",
    "        print(f\"   • Unique languages: {df_raw['language'].nunique()}\")\n",
    "        print(f\"   • Average tokens per sentence: {cost_summary['total_tokens'] / len(df_raw):.1f}\")\n",
    "    \n",
    "    # Cost projections\n",
    "    print(f\"\\n🔮 Cost Projections:\")\n",
    "    sentences_per_dollar = (1 / cost_summary['total_cost'] * len(df_raw)) if cost_summary['total_cost'] > 0 else float('inf')\n",
    "    print(f\"   • Sentences per $1: {sentences_per_dollar:.0f}\")\n",
    "    print(f\"   • Cost for 1,000 more sentences: ${(cost_summary['total_cost'] / len(df_raw) * 1000):.4f}\" if len(df_raw) > 0 else \"   • N/A\")\n",
    "    \n",
    "    return cost_summary\n",
    "\n",
    "# Display the dashboard\n",
    "final_costs = display_cost_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save & Export\n",
    "\n",
    "Save processed data and provide export options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary_cards(df: pd.DataFrame, filename: str = \"vocab_cards.csv\"):\n",
    "    \"\"\"Save the augmented dataframe with SRS metadata\"\"\"\n",
    "    try:\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"✅ Vocabulary cards saved to: {filename}\")\n",
    "        print(f\"📊 Saved {len(df)} cards with SRS metadata\")\n",
    "        \n",
    "        # Display sample of saved data\n",
    "        if len(df) > 0:\n",
    "            print(\"\\n🔍 Sample of saved data:\")\n",
    "            sample_cols = ['text', 'language', 'cefr_level', 'next_review', 'ease_factor']\n",
    "            available_cols = [col for col in sample_cols if col in df.columns]\n",
    "            display(df[available_cols].head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save vocabulary cards: {str(e)}\")\n",
    "\n",
    "def export_to_anki_format(df: pd.DataFrame, output_file: str = \"anki_cards.txt\"):\n",
    "    \"\"\"Export cards in Anki import format (tab-separated)\"\"\"\n",
    "    try:\n",
    "        anki_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Front of card (original sentence)\n",
    "            front = row['text']\n",
    "            \n",
    "            # Back of card (translation + metadata)\n",
    "            back_parts = []\n",
    "            \n",
    "            # Try to get translation if available\n",
    "            if 'translation' in row and pd.notna(row['translation']):\n",
    "                back_parts.append(f\"Translation: {row['translation']}\")\n",
    "            \n",
    "            back_parts.extend([\n",
    "                f\"Language: {row['language']}\",\n",
    "                f\"Level: {row['cefr_level']}\",\n",
    "                f\"Source: {Path(row['source_file']).name if 'source_file' in row else 'Unknown'}\"\n",
    "            ])\n",
    "            \n",
    "            if 'emoji' in row and pd.notna(row['emoji']):\n",
    "                back_parts.append(f\"Emoji: {row['emoji']}\")\n",
    "            \n",
    "            back = \"<br>\".join(back_parts)\n",
    "            \n",
    "            # Tags\n",
    "            tags = [row['language'], row['cefr_level']]\n",
    "            if 'source_file' in row:\n",
    "                tags.append(Path(row['source_file']).stem)\n",
    "            \n",
    "            # Anki format: Front \\t Back \\t Tags\n",
    "            anki_line = f\"{front}\\t{back}\\t{' '.join(tags)}\"\n",
    "            anki_data.append(anki_line)\n",
    "        \n",
    "        # Write to file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(anki_data))\n",
    "        \n",
    "        print(f\"✅ Anki cards exported to: {output_file}\")\n",
    "        print(f\"📋 Export format: Front \\t Back \\t Tags\")\n",
    "        print(f\"📊 Exported {len(anki_data)} cards\")\n",
    "        \n",
    "        # Instructions\n",
    "        print(\"\\n📖 To import into Anki:\")\n",
    "        print(\"   1. Open Anki\")\n",
    "        print(\"   2. File → Import\")\n",
    "        print(f\"   3. Select {output_file}\")\n",
    "        print(\"   4. Configure field mapping\")\n",
    "        print(\"   5. Import cards\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to export Anki cards: {str(e)}\")\n",
    "\n",
    "# Save vocabulary cards with SRS metadata\n",
    "if 'df_with_srs' in locals() and not df_with_srs.empty:\n",
    "    save_vocabulary_cards(df_with_srs)\n",
    "    export_to_anki_format(df_with_srs)\n",
    "elif not df_raw.empty:\n",
    "    save_vocabulary_cards(df_raw)\n",
    "    export_to_anki_format(df_raw)\n",
    "else:\n",
    "    print(\"❌ No data to save\")\n",
    "\n",
    "print(\"\\n💾 Save & Export operations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stretch Goals & Future Enhancements\n",
    "\n",
    "Optional advanced features that can be implemented:\n",
    "\n",
    "### 🔊 Audio TTS Integration\n",
    "- Use OpenAI's `audio.speech` API to generate pronunciation audio for each sentence\n",
    "- Store audio files locally and link them to vocabulary cards\n",
    "- Implement playback functionality in notebook interface\n",
    "\n",
    "### 🌐 FastAPI Web Dashboard  \n",
    "- Create a REST API backend serving the RAG index\n",
    "- Build React frontend for interactive vocabulary practice\n",
    "- Features: real-time queries, SRS scheduling, progress tracking\n",
    "- Deploy as containerized application\n",
    "\n",
    "### 📱 Mobile App Integration\n",
    "- Export vocabulary cards to mobile-friendly formats\n",
    "- Sync with popular language learning apps\n",
    "- Offline mode for practicing without internet\n",
    "\n",
    "### 🔍 Advanced NLP Features\n",
    "- Grammar pattern extraction and explanation\n",
    "- Automatic difficulty assessment using linguistic features\n",
    "- Contextual word sense disambiguation\n",
    "- Automated CEFR level classification\n",
    "\n",
    "### 📊 Learning Analytics\n",
    "- Progress tracking and visualization\n",
    "- Weak point identification\n",
    "- Personalized learning recommendations\n",
    "- Export learning statistics\n",
    "\n",
    "### 🌍 Multi-modal Learning\n",
    "- Image context for vocabulary (visual associations)\n",
    "- Video subtitle integration\n",
    "- Cultural context explanations\n",
    "- Native speaker audio samples\n",
    "\n",
    "To implement any of these features, simply add new cells below and extend the existing codebase. The modular design makes it easy to add new functionality without breaking existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 Notebook Complete!\n",
    "\n",
    "### Summary of Achievements:\n",
    "\n",
    "✅ **Multi-format Data Ingestion**: Successfully processed TXT, HTML, SRT, VTT, JSON, and CSV files  \n",
    "✅ **RAG-Powered Queries**: Built vector index with LlamaIndex for intelligent sentence retrieval  \n",
    "✅ **Korean Grammar Demo**: Generated A2-level sentences with '-고 있어요' pattern  \n",
    "✅ **Multilingual Support**: Language detection and CEFR level estimation  \n",
    "✅ **Spaced Repetition**: Implemented Anki-style SRS scheduling  \n",
    "✅ **Cost Tracking**: Monitored token usage and API costs  \n",
    "✅ **Export Options**: Saved vocabulary cards and Anki import format  \n",
    "\n",
    "### Files Generated:\n",
    "- `vocab_cards.csv` - Complete vocabulary database with SRS metadata\n",
    "- `anki_cards.txt` - Ready-to-import Anki flashcards\n",
    "- `storage/` - Persisted vector index for future sessions\n",
    "\n",
    "### Next Steps:\n",
    "1. Add more multilingual content to your `data/` folder\n",
    "2. Run vocabulary drills for different languages and patterns\n",
    "3. Use the SRS system to track your learning progress\n",
    "4. Import cards into Anki for mobile practice\n",
    "5. Extend with stretch goal features as needed\n",
    "\n",
    "**Happy Learning! 🌟**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
