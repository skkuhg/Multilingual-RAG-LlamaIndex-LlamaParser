{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual Language-Learning Notebook üåè (RAG-powered with LlamaIndex + LlamaParser)\n",
    "\n",
    "This notebook implements an end-to-end Retrieval-Augmented-Generation (RAG) workflow for organizing and practicing multilingual vocabulary and example sentences. Using LlamaIndex and LlamaParser, it ingests various text sources (articles, subtitles, chat logs, wordlists), builds a vector index, and provides intelligent query capabilities for language learning.\n",
    "\n",
    "**Key Features:**\n",
    "- Multi-format data ingestion (TXT, HTML, SRT, VTT, JSON, CSV)\n",
    "- RAG-powered vocabulary drills with CEFR level filtering\n",
    "- Spaced repetition scheduling\n",
    "- Cost tracking and token usage monitoring\n",
    "- Export capabilities for Anki integration\n",
    "\n",
    "**How to run:** \n",
    "1. Create a `.env` file with your API keys\n",
    "2. Add your multilingual content to the `data/` folder\n",
    "3. Run all cells sequentially\n",
    "4. Use the demo queries to explore your language learning corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "First, let's install the required dependencies and set up our environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet llama-index llama-parser tavily-python openai python-dotenv pandas tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Variables Setup\n",
    "\n",
    "Create a `.env` file in the same directory as this notebook with the following content:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=your_openai_api_key_here\n",
    "LLAMACLOUD_API_KEY=your_llamacloud_api_key_here\n",
    "TAVILY_API_KEY=your_tavily_api_key_here  # Optional for web search\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API Key: Set\n",
      "‚úÖ LlamaCloud API Key: Set\n",
      "üì° Tavily API Key: Set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys are loaded - with Tavily key directly set if needed\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "llama_key = os.getenv('LLAMACLOUD_API_KEY')\n",
    "tavily_key = os.getenv('TAVILY_API_KEY')\n",
    "\n",
    "# If Tavily key is not in .env, set it directly\n",
    "if not tavily_key:\n",
    "    tavily_key = \"your_tavily_api_key_here\"\n",
    "    os.environ['TAVILY_API_KEY'] = tavily_key\n",
    "\n",
    "print(f\"‚úÖ OpenAI API Key: {'Set' if openai_key else '‚ùå Missing'}\")\n",
    "print(f\"‚úÖ LlamaCloud API Key: {'Set' if llama_key else '‚ùå Missing'}\")\n",
    "print(f\"üì° Tavily API Key: {'Set' if tavily_key else 'Optional - Not set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI library imported successfully\n",
      "‚ö†Ô∏è LlamaParse not available - will use basic text parsing\n",
      "‚úÖ Tavily imported and configured successfully\n",
      "üì¶ All imports successful!\n",
      "üöÄ Simple RAG system ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Simple OpenAI imports - avoiding LlamaIndex complexity\n",
    "try:\n",
    "    import openai\n",
    "    openai_available = True\n",
    "    print(\"‚úÖ OpenAI library imported successfully\")\n",
    "except ImportError:\n",
    "    openai_available = False\n",
    "    print(\"‚ùå OpenAI library not available\")\n",
    "\n",
    "# LlamaParser - keep this separate\n",
    "try:\n",
    "    from llama_parse import LlamaParse\n",
    "    llamaparse_available = True\n",
    "    print(\"‚úÖ LlamaParse imported successfully\")\n",
    "except ImportError:\n",
    "    llamaparse_available = False\n",
    "    print(\"‚ö†Ô∏è LlamaParse not available - will use basic text parsing\")\n",
    "\n",
    "# Optional: Tavily for web search\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    # Test Tavily availability with the key\n",
    "    if tavily_key:\n",
    "        try:\n",
    "            test_client = TavilyClient(api_key=tavily_key)\n",
    "            tavily_available = True\n",
    "            print(\"‚úÖ Tavily imported and configured successfully\")\n",
    "        except Exception as e:\n",
    "            tavily_available = False\n",
    "            print(f\"‚ö†Ô∏è Tavily import successful but configuration failed: {e}\")\n",
    "    else:\n",
    "        tavily_available = False\n",
    "        print(\"‚ö†Ô∏è Tavily imported but no API key available\")\n",
    "except ImportError as e:\n",
    "    tavily_available = False\n",
    "    print(f\"‚ùå Tavily import failed: {e}\")\n",
    "except Exception as e:\n",
    "    tavily_available = False\n",
    "    print(f\"‚ùå Tavily setup failed: {e}\")\n",
    "\n",
    "# Create simple replacement classes for RAG functionality\n",
    "class Document:\n",
    "    def __init__(self, text: str, metadata: Dict = None):\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "class SimpleRAGSystem:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.document_metadata = []\n",
    "        \n",
    "        if openai_available:\n",
    "            self.client = openai.OpenAI(api_key=api_key)\n",
    "        else:\n",
    "            self.client = None\n",
    "    \n",
    "    def add_documents(self, documents: List[Document]):\n",
    "        \"\"\"Add documents to the RAG system with enhanced metadata tracking\"\"\"\n",
    "        self.documents.extend(documents)\n",
    "        \n",
    "        # Track metadata separately for better querying\n",
    "        for doc in documents:\n",
    "            self.document_metadata.append(doc.metadata)\n",
    "        \n",
    "        print(f\"Added {len(documents)} documents. Total: {len(self.documents)}\")\n",
    "        \n",
    "        # Show metadata summary\n",
    "        if self.document_metadata:\n",
    "            languages = [meta.get('language', 'Unknown') for meta in self.document_metadata]\n",
    "            from collections import Counter\n",
    "            lang_counter = Counter(languages)\n",
    "            print(f\"üìä Language distribution in index: {dict(lang_counter)}\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Get embedding for text using OpenAI\"\"\"\n",
    "        if not self.client:\n",
    "            return [0.0] * 1536  # Dummy embedding\n",
    "        \n",
    "        try:\n",
    "            response = self.client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=text\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embedding: {e}\")\n",
    "            return [0.0] * 1536\n",
    "    \n",
    "    def query(self, query_text: str, top_k: int = 5, language_filter: str = None, \n",
    "              level_filter: str = None, file_filter: str = None) -> str:\n",
    "        \"\"\"Enhanced query with filtering capabilities\"\"\"\n",
    "        if not self.documents:\n",
    "            return \"No documents available for querying.\"\n",
    "        \n",
    "        if not self.client:\n",
    "            return \"OpenAI client not available. Using sample response.\"\n",
    "        \n",
    "        # Filter documents based on criteria\n",
    "        filtered_docs = []\n",
    "        filtered_metadata = []\n",
    "        \n",
    "        for doc, meta in zip(self.documents, self.document_metadata):\n",
    "            # Apply filters\n",
    "            if language_filter and meta.get('language', '').lower() != language_filter.lower():\n",
    "                continue\n",
    "            if level_filter and meta.get('cefr_level', '') != level_filter:\n",
    "                continue  \n",
    "            if file_filter and file_filter.lower() not in meta.get('source_file', '').lower():\n",
    "                continue\n",
    "                \n",
    "            filtered_docs.append(doc)\n",
    "            filtered_metadata.append(meta)\n",
    "        \n",
    "        if not filtered_docs:\n",
    "            return f\"No documents found matching filters (language={language_filter}, level={level_filter}, file={file_filter})\"\n",
    "        \n",
    "        # Enhanced keyword-based retrieval\n",
    "        relevant_docs = []\n",
    "        query_lower = query_text.lower()\n",
    "        query_words = query_lower.split()\n",
    "        \n",
    "        # Score documents based on keyword matches and metadata relevance\n",
    "        doc_scores = []\n",
    "        for doc, meta in zip(filtered_docs, filtered_metadata):\n",
    "            score = 0\n",
    "            doc_text_lower = doc.text.lower()\n",
    "            \n",
    "            # Keyword matching\n",
    "            for word in query_words:\n",
    "                if word in doc_text_lower:\n",
    "                    score += 2  # Direct word match\n",
    "                    \n",
    "            # Boost score for exact phrase matches\n",
    "            if query_lower in doc_text_lower:\n",
    "                score += 5\n",
    "                \n",
    "            # Metadata-based scoring\n",
    "            if 'context' in meta and query_lower in meta['context'].lower():\n",
    "                score += 1\n",
    "                \n",
    "            doc_scores.append((score, doc, meta))\n",
    "        \n",
    "        # Sort by score and select top documents\n",
    "        doc_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Take top scoring docs, or fallback to first few if no scores\n",
    "        if any(score > 0 for score, _, _ in doc_scores):\n",
    "            relevant_docs = [(doc, meta) for score, doc, meta in doc_scores[:top_k] if score > 0]\n",
    "        else:\n",
    "            relevant_docs = [(doc, meta) for _, doc, meta in doc_scores[:top_k]]\n",
    "        \n",
    "        if not relevant_docs:\n",
    "            relevant_docs = [(filtered_docs[0], filtered_metadata[0])]  # At least one doc\n",
    "        \n",
    "        # Create enhanced context with metadata\n",
    "        context_parts = []\n",
    "        for doc, meta in relevant_docs:\n",
    "            context_part = f\"Text: {doc.text}\\n\"\n",
    "            if meta.get('language'):\n",
    "                context_part += f\"Language: {meta['language']}\\n\"\n",
    "            if meta.get('cefr_level'):\n",
    "                context_part += f\"Level: {meta['cefr_level']}\\n\"\n",
    "            if meta.get('file_name'):\n",
    "                context_part += f\"Source: {meta['file_name']}\\n\"\n",
    "            if meta.get('context'):\n",
    "                context_part += f\"Context: {meta['context'][:100]}...\\n\"\n",
    "            context_part += \"---\\n\"\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate response using OpenAI with enhanced prompt\n",
    "        try:\n",
    "            system_prompt = \"\"\"You are a helpful multilingual language learning assistant. \n",
    "            Use the provided context to answer questions about language learning content.\n",
    "            Pay attention to the metadata (language, level, source) when providing answers.\n",
    "            If asking about specific patterns or grammar, provide examples from the context.\n",
    "            Always be encouraging and educational in your responses.\"\"\"\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query_text}\"}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {e}\"\n",
    "    \n",
    "    def get_file_summary(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get summary of all files in the RAG system\"\"\"\n",
    "        if not self.document_metadata:\n",
    "            return {}\n",
    "        \n",
    "        from collections import defaultdict, Counter\n",
    "        \n",
    "        summary = {\n",
    "            'total_documents': len(self.documents),\n",
    "            'files': Counter(),\n",
    "            'languages': Counter(),\n",
    "            'levels': Counter(),\n",
    "            'file_types': Counter()\n",
    "        }\n",
    "        \n",
    "        for meta in self.document_metadata:\n",
    "            if 'file_name' in meta:\n",
    "                summary['files'][meta['file_name']] += 1\n",
    "            if 'language' in meta:\n",
    "                summary['languages'][meta['language']] += 1\n",
    "            if 'cefr_level' in meta:\n",
    "                summary['levels'][meta['cefr_level']] += 1\n",
    "            if 'file_type' in meta:\n",
    "                summary['file_types'][meta['file_type']] += 1\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Global variables that will be used throughout the notebook\n",
    "rag_system = None\n",
    "index = None  # For compatibility with existing code\n",
    "query_engine = None  # For compatibility with existing code\n",
    "\n",
    "print(\"üì¶ All imports successful!\")\n",
    "print(\"üöÄ Simple RAG system ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Simple RAG system configured with OpenAI!\n",
      "‚öôÔ∏è Configuration complete!\n"
     ]
    }
   ],
   "source": [
    "# Configure Simple RAG System\n",
    "if openai_available and openai_key:\n",
    "    rag_system = SimpleRAGSystem(api_key=openai_key)\n",
    "    print(\"ü§ñ Simple RAG system configured with OpenAI!\")\n",
    "else:\n",
    "    rag_system = None\n",
    "    print(\"‚ùå OpenAI not available - RAG functionality will be limited\")\n",
    "\n",
    "# Create compatibility objects for existing code\n",
    "class Settings:\n",
    "    llm = None\n",
    "    embed_model = None\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ Token tracking system initialized!\n"
     ]
    }
   ],
   "source": [
    "# Token and cost tracking\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.total_tokens = 0\n",
    "        self.embedding_tokens = 0\n",
    "        self.llm_tokens = 0\n",
    "        \n",
    "        # Pricing (per 1K tokens)\n",
    "        self.embedding_cost_per_1k = 0.00002  # text-embedding-3-small\n",
    "        self.llm_cost_per_1k = 0.00015  # gpt-4o-mini input\n",
    "        \n",
    "    def add_embedding_tokens(self, count: int):\n",
    "        self.embedding_tokens += count\n",
    "        self.total_tokens += count\n",
    "        \n",
    "    def add_llm_tokens(self, count: int):\n",
    "        self.llm_tokens += count\n",
    "        self.total_tokens += count\n",
    "        \n",
    "    def get_cost_summary(self) -> Dict[str, Any]:\n",
    "        embedding_cost = (self.embedding_tokens / 1000) * self.embedding_cost_per_1k\n",
    "        llm_cost = (self.llm_tokens / 1000) * self.llm_cost_per_1k\n",
    "        total_cost = embedding_cost + llm_cost\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': self.total_tokens,\n",
    "            'embedding_tokens': self.embedding_tokens,\n",
    "            'llm_tokens': self.llm_tokens,\n",
    "            'embedding_cost': embedding_cost,\n",
    "            'llm_cost': llm_cost,\n",
    "            'total_cost': total_cost\n",
    "        }\n",
    "\n",
    "# Initialize token tracker\n",
    "token_tracker = TokenTracker()\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Approximate token count for text\"\"\"\n",
    "    return len(text.split()) * 1.3  # Rough approximation\n",
    "\n",
    "print(\"üí∞ Token tracking system initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion\n",
    "\n",
    "This section handles ingesting various file types and parsing them with LlamaParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Data directories created!\n",
      "\n",
      "üìã Supported file types:\n",
      "  ‚Ä¢ Articles: .txt, .html, .md\n",
      "  ‚Ä¢ Subtitles: .srt, .vtt\n",
      "  ‚Ä¢ Chat logs: .json, .txt\n",
      "  ‚Ä¢ Word lists: .csv\n"
     ]
    }
   ],
   "source": [
    "# Create data directories if they don't exist\n",
    "data_dirs = ['data/articles', 'data/subtitles', 'data/chat_logs', 'data/wordlists', 'storage']\n",
    "for dir_path in data_dirs:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"üìÅ Data directories created!\")\n",
    "print(\"\\nüìã Supported file types:\")\n",
    "print(\"  ‚Ä¢ Articles: .txt, .html, .md\")\n",
    "print(\"  ‚Ä¢ Subtitles: .srt, .vtt\")\n",
    "print(\"  ‚Ä¢ Chat logs: .json, .txt\")\n",
    "print(\"  ‚Ä¢ Word lists: .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è LlamaParse not available - will use basic text parsing\n",
      "üîç Enhanced data analysis function ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LlamaParser if available\n",
    "if llamaparse_available and llama_key:\n",
    "    parser = LlamaParse(\n",
    "        api_key=llama_key,\n",
    "        result_type=\"text\",\n",
    "        verbose=True,\n",
    "        language=\"mixed\"  # Support for multilingual content\n",
    "    )\n",
    "    print(\"‚úÖ LlamaParse initialized successfully\")\n",
    "else:\n",
    "    parser = None\n",
    "    print(\"‚ö†Ô∏è LlamaParse not available - will use basic text parsing\")\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Simple language detection based on character patterns\"\"\"\n",
    "    # Korean characters\n",
    "    if re.search(r'[Í∞Ä-Ìû£]', text):\n",
    "        return 'Korean'\n",
    "    # Japanese characters\n",
    "    elif re.search(r'[„Å≤„Çâ„Åå„Å™„Ç´„Çø„Ç´„ÉäÊº¢Â≠ó]', text):\n",
    "        return 'Japanese'\n",
    "    # Chinese characters\n",
    "    elif re.search(r'[\\u4e00-\\u9fff]', text):\n",
    "        return 'Chinese'\n",
    "    # Spanish indicators\n",
    "    elif re.search(r'[√±√°√©√≠√≥√∫√º]', text.lower()):\n",
    "        return 'Spanish'\n",
    "    # French indicators\n",
    "    elif re.search(r'[√†√¢√§√ß√©√®√™√´√Ø√Æ√¥√π√ª√º√ø]', text.lower()):\n",
    "        return 'French'\n",
    "    # German indicators\n",
    "    elif re.search(r'[√§√∂√º√ü]', text.lower()):\n",
    "        return 'German'\n",
    "    else:\n",
    "        return 'English'  # Default\n",
    "\n",
    "def estimate_cefr_level(text: str, language: str) -> str:\n",
    "    \"\"\"Rough CEFR level estimation based on text complexity\"\"\"\n",
    "    words = text.split()\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "    sentence_length = len(words)\n",
    "    \n",
    "    # Simple heuristic\n",
    "    if avg_word_length < 4 and sentence_length < 8:\n",
    "        return 'A1'\n",
    "    elif avg_word_length < 5 and sentence_length < 12:\n",
    "        return 'A2'\n",
    "    elif avg_word_length < 6 and sentence_length < 16:\n",
    "        return 'B1'\n",
    "    elif avg_word_length < 7 and sentence_length < 20:\n",
    "        return 'B2'\n",
    "    else:\n",
    "        return 'C1'\n",
    "\n",
    "def simple_text_parser(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"Enhanced text parser that preserves document structure and context\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Clean the content\n",
    "        content = content.strip()\n",
    "        if not content:\n",
    "            return []\n",
    "        \n",
    "        # Split into paragraphs first to preserve structure\n",
    "        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "        if not paragraphs:\n",
    "            # Fallback to line-by-line if no paragraph breaks\n",
    "            paragraphs = [p.strip() for p in content.split('\\n') if p.strip()]\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for para_idx, paragraph in enumerate(paragraphs):\n",
    "            # Enhanced sentence splitting that handles multiple languages\n",
    "            # Korean: ., !, ?, „ÄÇ\n",
    "            # Spanish/French: ., !, ?, ¬°, ¬ø\n",
    "            # General punctuation\n",
    "            sentence_endings = r'[.!?„ÄÇ¬°¬ø]+(?:\\s+|$)'\n",
    "            sentences = re.split(sentence_endings, paragraph)\n",
    "            \n",
    "            for sent_idx, sentence in enumerate(sentences):\n",
    "                sentence = sentence.strip()\n",
    "                if len(sentence) < 5:  # More lenient minimum length\n",
    "                    continue\n",
    "                \n",
    "                # Create document with enhanced metadata\n",
    "                doc_data = {\n",
    "                    'text': sentence,\n",
    "                    'paragraph_index': para_idx,\n",
    "                    'sentence_index': sent_idx,\n",
    "                    'paragraph_context': paragraph[:200] + '...' if len(paragraph) > 200 else paragraph,\n",
    "                    'file_size': len(content),\n",
    "                    'total_paragraphs': len(paragraphs)\n",
    "                }\n",
    "                documents.append(doc_data)\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def enhanced_csv_parser(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"Enhanced CSV parser that handles various CSV formats\"\"\"\n",
    "    try:\n",
    "        # Try different encodings\n",
    "        encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']\n",
    "        df = None\n",
    "        \n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path, encoding=encoding)\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if df is None:\n",
    "            print(f\"Could not read CSV file {file_path} with any encoding\")\n",
    "            return []\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        # Handle different CSV structures\n",
    "        if 'word' in df.columns and 'translation' in df.columns:\n",
    "            # Vocabulary list format\n",
    "            for idx, row in df.iterrows():\n",
    "                word = str(row.get('word', ''))\n",
    "                translation = str(row.get('translation', ''))\n",
    "                \n",
    "                if word and word != 'nan':\n",
    "                    doc_data = {\n",
    "                        'text': f\"{word} - {translation}\",\n",
    "                        'word': word,\n",
    "                        'translation': translation,\n",
    "                        'row_index': idx,\n",
    "                        'csv_type': 'vocabulary'\n",
    "                    }\n",
    "                    documents.append(doc_data)\n",
    "                    \n",
    "        elif 'text' in df.columns or 'sentence' in df.columns:\n",
    "            # Sentence list format\n",
    "            text_col = 'text' if 'text' in df.columns else 'sentence'\n",
    "            for idx, row in df.iterrows():\n",
    "                text = str(row.get(text_col, ''))\n",
    "                if text and text != 'nan' and len(text) > 5:\n",
    "                    doc_data = {\n",
    "                        'text': text,\n",
    "                        'row_index': idx,\n",
    "                        'csv_type': 'sentences'\n",
    "                    }\n",
    "                    # Add any additional columns as metadata\n",
    "                    for col in df.columns:\n",
    "                        if col != text_col:\n",
    "                            doc_data[col] = row.get(col)\n",
    "                    documents.append(doc_data)\n",
    "        else:\n",
    "            # Generic CSV - use first text-like column\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':  # Text column\n",
    "                    for idx, row in df.iterrows():\n",
    "                        text = str(row.get(col, ''))\n",
    "                        if text and text != 'nan' and len(text) > 5:\n",
    "                            doc_data = {\n",
    "                                'text': text,\n",
    "                                'row_index': idx,\n",
    "                                'csv_type': 'generic',\n",
    "                                'source_column': col\n",
    "                            }\n",
    "                            documents.append(doc_data)\n",
    "                    break\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing CSV file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def enhanced_json_parser(file_path: Path) -> List[Dict]:\n",
    "    \"\"\"Enhanced JSON parser that handles various JSON structures\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            # List of objects/messages\n",
    "            for idx, item in enumerate(data):\n",
    "                if isinstance(item, dict):\n",
    "                    # Extract text from various possible fields\n",
    "                    text_fields = ['content', 'text', 'message', 'sentence', 'body']\n",
    "                    text = None\n",
    "                    \n",
    "                    for field in text_fields:\n",
    "                        if field in item and item[field]:\n",
    "                            text = str(item[field])\n",
    "                            break\n",
    "                    \n",
    "                    if text and len(text) > 5:\n",
    "                        doc_data = {\n",
    "                            'text': text,\n",
    "                            'json_index': idx,\n",
    "                            'json_type': 'list_item'\n",
    "                        }\n",
    "                        # Add other fields as metadata\n",
    "                        for key, value in item.items():\n",
    "                            if key not in text_fields:\n",
    "                                doc_data[key] = value\n",
    "                        documents.append(doc_data)\n",
    "                        \n",
    "                elif isinstance(item, str) and len(item) > 5:\n",
    "                    # Simple string list\n",
    "                    doc_data = {\n",
    "                        'text': item,\n",
    "                        'json_index': idx,\n",
    "                        'json_type': 'string_list'\n",
    "                    }\n",
    "                    documents.append(doc_data)\n",
    "                    \n",
    "        elif isinstance(data, dict):\n",
    "            # Dictionary structure\n",
    "            def extract_from_dict(obj, prefix=\"\"):\n",
    "                texts = []\n",
    "                for key, value in obj.items():\n",
    "                    current_key = f\"{prefix}.{key}\" if prefix else key\n",
    "                    \n",
    "                    if isinstance(value, str) and len(value) > 5:\n",
    "                        texts.append({\n",
    "                            'text': value,\n",
    "                            'json_key': current_key,\n",
    "                            'json_type': 'dict_value'\n",
    "                        })\n",
    "                    elif isinstance(value, dict):\n",
    "                        texts.extend(extract_from_dict(value, current_key))\n",
    "                    elif isinstance(value, list):\n",
    "                        for i, item in enumerate(value):\n",
    "                            if isinstance(item, str) and len(item) > 5:\n",
    "                                texts.append({\n",
    "                                    'text': item,\n",
    "                                    'json_key': f\"{current_key}[{i}]\",\n",
    "                                    'json_type': 'dict_list_item'\n",
    "                                })\n",
    "                return texts\n",
    "            \n",
    "            documents = extract_from_dict(data)\n",
    "        \n",
    "        return documents\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_data_completeness(df: pd.DataFrame) -> None:\n",
    "    \"\"\"Analyze and display comprehensive information about the ingested data\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå No data to analyze\")\n",
    "        return\n",
    "    \n",
    "    print(\"üîç COMPREHENSIVE DATA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"üìä Basic Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total entries: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Total characters: {df['text'].str.len().sum():,}\")\n",
    "    print(f\"   ‚Ä¢ Average text length: {df['text'].str.len().mean():.1f} characters\")\n",
    "    print(f\"   ‚Ä¢ Unique texts: {df['text'].nunique():,}\")\n",
    "    \n",
    "    # Language analysis\n",
    "    print(f\"\\nüåç Language Distribution:\")\n",
    "    lang_counts = df['language'].value_counts()\n",
    "    for lang, count in lang_counts.head(10).items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   ‚Ä¢ {lang}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # CEFR level analysis\n",
    "    print(f\"\\nüìö CEFR Level Distribution:\")\n",
    "    level_counts = df['cefr_level'].value_counts()\n",
    "    for level, count in level_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   ‚Ä¢ {level}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # File analysis\n",
    "    print(f\"\\nüìÅ File Source Analysis:\")\n",
    "    if 'file_name' in df.columns:\n",
    "        file_counts = df['file_name'].value_counts()\n",
    "        print(f\"   ‚Ä¢ Total files: {len(file_counts)}\")\n",
    "        for file_name, count in file_counts.head(10).items():\n",
    "            print(f\"   ‚Ä¢ {file_name}: {count:,} entries\")\n",
    "    \n",
    "    # File type analysis\n",
    "    if 'file_type' in df.columns:\n",
    "        print(f\"\\nüìã File Type Analysis:\")\n",
    "        type_counts = df['file_type'].value_counts()\n",
    "        for file_type, count in type_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"   ‚Ä¢ {file_type}: {count:,} files ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Content quality analysis\n",
    "    print(f\"\\n‚ú® Content Quality Analysis:\")\n",
    "    \n",
    "    # Text length distribution\n",
    "    text_lengths = df['text'].str.len()\n",
    "    print(f\"   ‚Ä¢ Shortest text: {text_lengths.min()} characters\")\n",
    "    print(f\"   ‚Ä¢ Longest text: {text_lengths.max()} characters\")\n",
    "    print(f\"   ‚Ä¢ Median length: {text_lengths.median():.0f} characters\")\n",
    "    \n",
    "    # Very short texts (potential quality issues)\n",
    "    very_short = df[df['text'].str.len() < 10]\n",
    "    if len(very_short) > 0:\n",
    "        print(f\"   ‚ö†Ô∏è Very short texts (<10 chars): {len(very_short)}\")\n",
    "    \n",
    "    # Very long texts (potential chunking needed)  \n",
    "    very_long = df[df['text'].str.len() > 500]\n",
    "    if len(very_long) > 0:\n",
    "        print(f\"   üìè Very long texts (>500 chars): {len(very_long)}\")\n",
    "        \n",
    "    # Language-specific patterns\n",
    "    print(f\"\\nüéØ Language-Specific Pattern Analysis:\")\n",
    "    \n",
    "    for language in lang_counts.head(5).index:\n",
    "        lang_df = df[df['language'] == language]\n",
    "        print(f\"\\n   {language} ({len(lang_df):,} entries):\")\n",
    "        \n",
    "        # Show sample sentences\n",
    "        samples = lang_df['text'].head(3).tolist()\n",
    "        for i, sample in enumerate(samples, 1):\n",
    "            print(f\"     {i}. {sample[:80]}{'...' if len(sample) > 80 else ''}\")\n",
    "        \n",
    "        # Language-specific patterns\n",
    "        if language == 'Korean':\n",
    "            pattern_counts = {}\n",
    "            patterns = ['Í≥† ÏûàÏñ¥Ïöî', 'ÏäµÎãàÎã§', 'ÏïÑÏöî/Ïñ¥Ïöî', 'ÏùÑ/Î•º', 'Ïù¥/Í∞Ä']\n",
    "            for pattern in patterns:\n",
    "                count = lang_df['text'].str.contains(pattern, regex=False).sum()\n",
    "                if count > 0:\n",
    "                    pattern_counts[pattern] = count\n",
    "            \n",
    "            if pattern_counts:\n",
    "                print(f\"     üìù Korean patterns found:\")\n",
    "                for pattern, count in pattern_counts.items():\n",
    "                    print(f\"        ‚Ä¢ '{pattern}': {count} sentences\")\n",
    "        \n",
    "        elif language == 'Spanish':\n",
    "            pattern_counts = {}\n",
    "            patterns = ['que', 'de', 'la', 'el', 'es', 'en']\n",
    "            for pattern in patterns:\n",
    "                count = lang_df['text'].str.contains(f'\\\\b{pattern}\\\\b', regex=True, case=False).sum()\n",
    "                if count > 0:\n",
    "                    pattern_counts[pattern] = count\n",
    "            \n",
    "            if pattern_counts:\n",
    "                print(f\"     üìù Common Spanish words:\")\n",
    "                for pattern, count in sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "                    print(f\"        ‚Ä¢ '{pattern}': {count} occurrences\")\n",
    "    \n",
    "    # Data completeness check\n",
    "    print(f\"\\nüîß Data Completeness Check:\")\n",
    "    required_columns = ['text', 'language', 'cefr_level', 'source_file']\n",
    "    for col in required_columns:\n",
    "        if col in df.columns:\n",
    "            null_count = df[col].isnull().sum()\n",
    "            empty_count = (df[col] == '').sum() if df[col].dtype == 'object' else 0\n",
    "            total_missing = null_count + empty_count\n",
    "            if total_missing > 0:\n",
    "                print(f\"   ‚ö†Ô∏è {col}: {total_missing} missing values\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ {col}: Complete\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå {col}: Column missing\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    \n",
    "    total_entries = len(df)\n",
    "    if total_entries < 100:\n",
    "        print(\"   ‚Ä¢ Consider adding more content files for better learning variety\")\n",
    "    elif total_entries > 10000:\n",
    "        print(\"   ‚Ä¢ Large dataset detected - consider implementing pagination for better performance\")\n",
    "    \n",
    "    unique_languages = len(lang_counts)\n",
    "    if unique_languages == 1:\n",
    "        print(\"   ‚Ä¢ Add content in additional languages for multilingual learning\")\n",
    "    elif unique_languages > 5:\n",
    "        print(\"   ‚Ä¢ Rich multilingual content detected - excellent for diverse learning!\")\n",
    "    \n",
    "    # Pattern-specific recommendations\n",
    "    korean_count = lang_counts.get('Korean', 0)\n",
    "    if korean_count > 0:\n",
    "        korean_with_pattern = df[\n",
    "            (df['language'] == 'Korean') & \n",
    "            (df['text'].str.contains('Í≥† ÏûàÏñ¥Ïöî', na=False, regex=False))\n",
    "        ]\n",
    "        if len(korean_with_pattern) == 0:\n",
    "            print(\"   ‚Ä¢ Add more Korean present continuous ('-Í≥† ÏûàÏñ¥Ïöî') examples for grammar practice\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Korean present continuous pattern: {len(korean_with_pattern)} examples found\")\n",
    "\n",
    "print(\"üîç Enhanced data analysis function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Data ingestion function ready!\n"
     ]
    }
   ],
   "source": [
    "def ingest_data_files(data_folder: str = \"data\") -> pd.DataFrame:\n",
    "    \"\"\"Enhanced data ingestion that preserves file structure and context\"\"\"\n",
    "    sentences_data = []\n",
    "    data_path = Path(data_folder)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"‚ùå Data folder '{data_folder}' not found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Supported file extensions with more formats\n",
    "    supported_extensions = {'.txt', '.html', '.md', '.srt', '.vtt', '.json', '.csv', '.rtf'}\n",
    "    \n",
    "    # Find all supported files\n",
    "    files_found = []\n",
    "    for ext in supported_extensions:\n",
    "        files_found.extend(data_path.rglob(f\"*{ext}\"))\n",
    "    \n",
    "    print(f\"üìÅ Found {len(files_found)} files to process...\")\n",
    "    total_chars_processed = 0\n",
    "    \n",
    "    for file_path in files_found:\n",
    "        print(f\"\\nüîÑ Processing: {file_path.name} ({file_path.stat().st_size:,} bytes)\")\n",
    "        \n",
    "        try:\n",
    "            # Get file-level metadata\n",
    "            file_stats = file_path.stat()\n",
    "            file_metadata = {\n",
    "                'source_file': str(file_path),\n",
    "                'file_name': file_path.name,\n",
    "                'file_size': file_stats.st_size,\n",
    "                'file_type': file_path.suffix.lower(),\n",
    "                'relative_path': str(file_path.relative_to(data_path)),\n",
    "                'folder': file_path.parent.name,\n",
    "                'modified_time': datetime.fromtimestamp(file_stats.st_mtime).isoformat()\n",
    "            }\n",
    "            \n",
    "            documents = []\n",
    "            \n",
    "            if file_path.suffix == '.csv':\n",
    "                # Enhanced CSV handling\n",
    "                csv_docs = enhanced_csv_parser(file_path)\n",
    "                documents.extend(csv_docs)\n",
    "                \n",
    "            elif file_path.suffix == '.json':\n",
    "                # Enhanced JSON handling  \n",
    "                json_docs = enhanced_json_parser(file_path)\n",
    "                documents.extend(json_docs)\n",
    "                \n",
    "            elif file_path.suffix in {'.srt', '.vtt'}:\n",
    "                # Subtitle file handling\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                \n",
    "                # Parse subtitle format\n",
    "                if file_path.suffix == '.srt':\n",
    "                    # SRT format parsing\n",
    "                    blocks = re.split(r'\\n\\s*\\n', content)\n",
    "                    for block_idx, block in enumerate(blocks):\n",
    "                        lines = block.strip().split('\\n')\n",
    "                        if len(lines) >= 3:  # Index, time, text\n",
    "                            subtitle_text = ' '.join(lines[2:])  # Join all text lines\n",
    "                            if len(subtitle_text) > 5:\n",
    "                                documents.append({\n",
    "                                    'text': subtitle_text,\n",
    "                                    'subtitle_index': block_idx,\n",
    "                                    'timestamp': lines[1] if len(lines) > 1 else None,\n",
    "                                    'subtitle_format': 'srt'\n",
    "                                })\n",
    "                else:\n",
    "                    # VTT format - simpler parsing for now\n",
    "                    lines = content.split('\\n')\n",
    "                    for line_idx, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        if line and not line.startswith('WEBVTT') and '-->' not in line and not line.isdigit():\n",
    "                            if len(line) > 5:\n",
    "                                documents.append({\n",
    "                                    'text': line,\n",
    "                                    'subtitle_index': line_idx,\n",
    "                                    'subtitle_format': 'vtt'\n",
    "                                })\n",
    "                \n",
    "            else:\n",
    "                # Enhanced text file handling (txt, html, md, rtf)\n",
    "                if parser and file_path.suffix in {'.html', '.md', '.rtf'}:\n",
    "                    try:\n",
    "                        # Try LlamaParse for complex formats\n",
    "                        llama_docs = parser.load_data(str(file_path))\n",
    "                        for doc in llama_docs:\n",
    "                            if hasattr(doc, 'text') and doc.text.strip():\n",
    "                                # Use enhanced parser on LlamaParse output\n",
    "                                parsed_docs = simple_text_parser(file_path)\n",
    "                                documents.extend(parsed_docs)\n",
    "                    except Exception as e:\n",
    "                        print(f\"LlamaParse failed for {file_path.name}, using enhanced parser: {e}\")\n",
    "                        parsed_docs = simple_text_parser(file_path)\n",
    "                        documents.extend(parsed_docs)\n",
    "                else:\n",
    "                    # Use enhanced text parser\n",
    "                    parsed_docs = simple_text_parser(file_path)\n",
    "                    documents.extend(parsed_docs)\n",
    "            \n",
    "            # Process all documents from this file\n",
    "            file_sentences_count = 0\n",
    "            for doc_data in documents:\n",
    "                if not isinstance(doc_data, dict) or 'text' not in doc_data:\n",
    "                    continue\n",
    "                    \n",
    "                text = doc_data['text'].strip()\n",
    "                if len(text) < 5:  # Skip very short texts\n",
    "                    continue\n",
    "                \n",
    "                # Detect language and estimate level\n",
    "                language = detect_language(text)\n",
    "                cefr_level = estimate_cefr_level(text, language)\n",
    "                \n",
    "                # Create comprehensive sentence data\n",
    "                sentence_data = {\n",
    "                    'text': text,\n",
    "                    'language': language,\n",
    "                    'cefr_level': cefr_level,\n",
    "                    'emoji': get_language_emoji(language),\n",
    "                    **file_metadata,  # Add all file metadata\n",
    "                    **doc_data  # Add parser-specific metadata\n",
    "                }\n",
    "                \n",
    "                # Add contextual information\n",
    "                if 'paragraph_context' in doc_data:\n",
    "                    sentence_data['context'] = doc_data['paragraph_context']\n",
    "                \n",
    "                sentences_data.append(sentence_data)\n",
    "                file_sentences_count += 1\n",
    "                \n",
    "                # Track tokens\n",
    "                token_tracker.add_embedding_tokens(int(count_tokens(text)))\n",
    "                total_chars_processed += len(text)\n",
    "            \n",
    "            print(f\"   ‚úÖ Extracted {file_sentences_count} sentences/entries\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {file_path.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    df_raw = pd.DataFrame(sentences_data)\n",
    "    print(f\"\\nüéâ Processing complete!\")\n",
    "    print(f\"üìä Total sentences/entries: {len(df_raw):,}\")\n",
    "    print(f\"üìù Total characters processed: {total_chars_processed:,}\")\n",
    "    print(f\"üìÅ Files processed: {len(files_found)}\")\n",
    "    \n",
    "    if not df_raw.empty:\n",
    "        print(f\"\\nüìä Language distribution:\")\n",
    "        lang_counts = df_raw['language'].value_counts()\n",
    "        for lang, count in lang_counts.items():\n",
    "            print(f\"  ‚Ä¢ {lang}: {count:,} sentences\")\n",
    "        \n",
    "        print(f\"\\nüìä CEFR level distribution:\")\n",
    "        level_counts = df_raw['cefr_level'].value_counts()\n",
    "        for level, count in level_counts.items():\n",
    "            print(f\"  ‚Ä¢ {level}: {count:,} sentences\")\n",
    "        \n",
    "        print(f\"\\nüìä File type distribution:\")\n",
    "        type_counts = df_raw['file_type'].value_counts()\n",
    "        for file_type, count in type_counts.items():\n",
    "            print(f\"  ‚Ä¢ {file_type}: {count:,} entries\")\n",
    "        \n",
    "        # Show sample of data structure\n",
    "        print(f\"\\nüîç Sample data columns:\")\n",
    "        print(f\"Available columns: {list(df_raw.columns)}\")\n",
    "    \n",
    "    return df_raw\n",
    "\n",
    "def get_language_emoji(language: str) -> str:\n",
    "    \"\"\"Get appropriate emoji for language\"\"\"\n",
    "    emoji_map = {\n",
    "        'Korean': 'üá∞üá∑',\n",
    "        'Japanese': 'üáØüáµ', \n",
    "        'Chinese': 'üá®üá≥',\n",
    "        'Spanish': 'üá™üá∏',\n",
    "        'French': 'üá´üá∑',\n",
    "        'German': 'üá©üá™',\n",
    "        'English': 'üá∫üá∏',\n",
    "        'Italian': 'üáÆüáπ',\n",
    "        'Portuguese': 'üáµüáπ',\n",
    "        'Russian': 'üá∑üá∫'\n",
    "    }\n",
    "    return emoji_map.get(language, 'üìù')\n",
    "\n",
    "print(\"üì• Data ingestion function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 13 files to process...\n",
      "\n",
      "üîÑ Processing: korean_conversation.json (820 bytes)\n",
      "   ‚úÖ Extracted 5 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_lesson.json (1,086 bytes)\n",
      "   ‚úÖ Extracted 6 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_drama.srt (526 bytes)\n",
      "   ‚úÖ Extracted 7 sentences/entries\n",
      "\n",
      "üîÑ Processing: french_basics.txt (545 bytes)\n",
      "   ‚úÖ Extracted 15 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_future_tense.txt (499 bytes)\n",
      "   ‚úÖ Extracted 15 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_grammar_patterns.txt (599 bytes)\n",
      "   ‚úÖ Extracted 20 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_past_tense.txt (518 bytes)\n",
      "   ‚úÖ Extracted 15 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_sample.txt (600 bytes)\n",
      "   ‚úÖ Extracted 15 sentences/entries\n",
      "\n",
      "üîÑ Processing: spanish_complete.txt (747 bytes)\n",
      "   ‚úÖ Extracted 20 sentences/entries\n",
      "\n",
      "üîÑ Processing: spanish_sample.txt (316 bytes)\n",
      "   ‚úÖ Extracted 10 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_vocabulary_extended.csv (738 bytes)\n",
      "   ‚úÖ Extracted 20 sentences/entries\n",
      "\n",
      "üîÑ Processing: korean_words.csv (381 bytes)\n",
      "   ‚úÖ Extracted 10 sentences/entries\n",
      "\n",
      "üîÑ Processing: spanish_vocabulary.csv (680 bytes)\n",
      "   ‚úÖ Extracted 20 sentences/entries\n",
      "\n",
      "üéâ Processing complete!\n",
      "üìä Total sentences/entries: 178\n",
      "üìù Total characters processed: 3,330\n",
      "üìÅ Files processed: 13\n",
      "\n",
      "üìä Language distribution:\n",
      "  ‚Ä¢ Korean: 113 sentences\n",
      "  ‚Ä¢ English: 35 sentences\n",
      "  ‚Ä¢ Spanish: 26 sentences\n",
      "  ‚Ä¢ French: 4 sentences\n",
      "\n",
      "üìä CEFR level distribution:\n",
      "  ‚Ä¢ A1: 119 sentences\n",
      "  ‚Ä¢ A2: 35 sentences\n",
      "  ‚Ä¢ B1: 16 sentences\n",
      "  ‚Ä¢ B2: 8 sentences\n",
      "\n",
      "üìä File type distribution:\n",
      "  ‚Ä¢ .txt: 110 entries\n",
      "  ‚Ä¢ .csv: 50 entries\n",
      "  ‚Ä¢ .json: 11 entries\n",
      "  ‚Ä¢ .srt: 7 entries\n",
      "\n",
      "üîç Sample data columns:\n",
      "Available columns: ['text', 'language', 'cefr_level', 'emoji', 'source_file', 'file_name', 'file_size', 'file_type', 'relative_path', 'folder', 'modified_time', 'json_index', 'json_type', 'role', 'timestamp', 'speaker', 'subtitle_index', 'subtitle_format', 'paragraph_index', 'sentence_index', 'paragraph_context', 'total_paragraphs', 'context', 'word', 'translation', 'row_index', 'csv_type']\n",
      "\n",
      "üéâ Data ingestion complete! Total entries: 178\n",
      "üîç COMPREHENSIVE DATA ANALYSIS\n",
      "============================================================\n",
      "üìä Basic Statistics:\n",
      "   ‚Ä¢ Total entries: 178\n",
      "   ‚Ä¢ Total characters: 3,330\n",
      "   ‚Ä¢ Average text length: 18.7 characters\n",
      "   ‚Ä¢ Unique texts: 166\n",
      "\n",
      "üåç Language Distribution:\n",
      "   ‚Ä¢ Korean: 113 (63.5%)\n",
      "   ‚Ä¢ English: 35 (19.7%)\n",
      "   ‚Ä¢ Spanish: 26 (14.6%)\n",
      "   ‚Ä¢ French: 4 (2.2%)\n",
      "\n",
      "üìö CEFR Level Distribution:\n",
      "   ‚Ä¢ A1: 119 (66.9%)\n",
      "   ‚Ä¢ A2: 35 (19.7%)\n",
      "   ‚Ä¢ B1: 16 (9.0%)\n",
      "   ‚Ä¢ B2: 8 (4.5%)\n",
      "\n",
      "üìÅ File Source Analysis:\n",
      "   ‚Ä¢ Total files: 13\n",
      "   ‚Ä¢ korean_grammar_patterns.txt: 20 entries\n",
      "   ‚Ä¢ spanish_complete.txt: 20 entries\n",
      "   ‚Ä¢ korean_vocabulary_extended.csv: 20 entries\n",
      "   ‚Ä¢ spanish_vocabulary.csv: 20 entries\n",
      "   ‚Ä¢ french_basics.txt: 15 entries\n",
      "   ‚Ä¢ korean_future_tense.txt: 15 entries\n",
      "   ‚Ä¢ korean_past_tense.txt: 15 entries\n",
      "   ‚Ä¢ korean_sample.txt: 15 entries\n",
      "   ‚Ä¢ spanish_sample.txt: 10 entries\n",
      "   ‚Ä¢ korean_words.csv: 10 entries\n",
      "\n",
      "üìã File Type Analysis:\n",
      "   ‚Ä¢ .txt: 110 files (61.8%)\n",
      "   ‚Ä¢ .csv: 50 files (28.1%)\n",
      "   ‚Ä¢ .json: 11 files (6.2%)\n",
      "   ‚Ä¢ .srt: 7 files (3.9%)\n",
      "\n",
      "‚ú® Content Quality Analysis:\n",
      "   ‚Ä¢ Shortest text: 7 characters\n",
      "   ‚Ä¢ Longest text: 47 characters\n",
      "   ‚Ä¢ Median length: 15 characters\n",
      "   ‚ö†Ô∏è Very short texts (<10 chars): 16\n",
      "\n",
      "üéØ Language-Specific Pattern Analysis:\n",
      "\n",
      "   Korean (113 entries):\n",
      "     1. ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏñ¥Ïöî.\n",
      "     2. ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥ Í≥µÎ∂ÄÌïòÍ≥† Í≥ÑÏãúÎäîÍµ∞Ïöî. ÏñºÎßàÎÇò Ïò§Îûò Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî?\n",
      "     3. 6Í∞úÏõî ÎèôÏïà Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî. ÏïÑÏßÅ Ïñ¥Î†§ÏõåÏöî.\n",
      "     üìù Korean patterns found:\n",
      "        ‚Ä¢ 'Í≥† ÏûàÏñ¥Ïöî': 50 sentences\n",
      "\n",
      "   English (35 entries):\n",
      "     1. Elle habite dans une belle maison\n",
      "     2. Il fait beau aujourd'hui\n",
      "     3. Quelle heure est-il maintenant\n",
      "\n",
      "   Spanish (26 entries):\n",
      "     1. J'aime beaucoup √©tudier le fran√ßais\n",
      "     2. Nous allons au cin√©ma ce soir\n",
      "     3. Je voudrais une tasse de caf√©\n",
      "     üìù Common Spanish words:\n",
      "        ‚Ä¢ 'que': 11 occurrences\n",
      "        ‚Ä¢ 'la': 3 occurrences\n",
      "        ‚Ä¢ 'de': 2 occurrences\n",
      "\n",
      "   French (4 entries):\n",
      "     1. Ils mangent au restaurant fran√ßais\n",
      "     2. Vous pouvez m'aider, s'il vous pla√Æt\n",
      "     3. O√π est-ce que vous habitez\n",
      "\n",
      "üîß Data Completeness Check:\n",
      "   ‚úÖ text: Complete\n",
      "   ‚úÖ language: Complete\n",
      "   ‚úÖ cefr_level: Complete\n",
      "   ‚úÖ source_file: Complete\n",
      "\n",
      "üí° Recommendations:\n",
      "   ‚úÖ Korean present continuous pattern: 50 examples found\n",
      "\n",
      "üîç Quick Pattern Check:\n",
      "   ‚Ä¢ Korean sentences with '-Í≥† ÏûàÏñ¥Ïöî': 50\n",
      "   ‚Ä¢ Sample matches:\n",
      "     - ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏñ¥Ïöî.\n",
      "       (Source: korean_conversation.json)\n",
      "     - ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥ Í≥µÎ∂ÄÌïòÍ≥† Í≥ÑÏãúÎäîÍµ∞Ïöî. ÏñºÎßàÎÇò Ïò§Îûò Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî?\n",
      "       (Source: korean_conversation.json)\n",
      "     - 6Í∞úÏõî ÎèôÏïà Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî. ÏïÑÏßÅ Ïñ¥Î†§ÏõåÏöî.\n",
      "       (Source: korean_conversation.json)\n",
      "\n",
      "üìã Sample Enhanced Metadata:\n",
      "   ‚Ä¢ text: ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏñ¥Ïöî.\n",
      "   ‚Ä¢ language: Korean\n",
      "   ‚Ä¢ cefr_level: A2\n",
      "   ‚Ä¢ file_name: korean_conversation.json\n",
      "   ‚Ä¢ file_type: .json\n",
      "   ‚Ä¢ relative_path: chat_logs\\korean_conversation.json\n",
      "   ‚Ä¢ file_size: 820\n"
     ]
    }
   ],
   "source": [
    "# Create sample data if data folder is empty\n",
    "sample_korean_data = [\n",
    "    \"Ï†ÄÎäî ÏßÄÍ∏à ÌïúÍµ≠Ïñ¥Î•º Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"ÏπúÍµ¨Îì§Í≥º Ìï®Íªò ÏòÅÌôîÎ•º Î≥¥Í≥† ÏûàÏñ¥Ïöî.\", \n",
    "    \"Ïò§Îäò ÎÇ†Ïî®Í∞Ä Ï†ïÎßê Ï¢ãÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"Ïª§ÌîºÎ•º ÎßàÏãúÎ©¥ÏÑú Ï±ÖÏùÑ ÏùΩÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"ÏÉàÎ°úÏö¥ Ïñ∏Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"ÌïúÍµ≠ ÏùåÏãùÏùÑ ÏöîÎ¶¨ÌïòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"ÏßÄÌïòÏ≤†ÏóêÏÑú ÏùåÏïÖÏùÑ Îì£Í≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"Í≥µÏõêÏóêÏÑú ÏÇ∞Ï±ÖÌïòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"ÌïúÍµ≠ ÎìúÎùºÎßàÎ•º ÏãúÏ≤≠ÌïòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "    \"ÏÉàÎ°úÏö¥ Îã®Ïñ¥Îì§ÏùÑ ÏïîÍ∏∞ÌïòÍ≥† ÏûàÏñ¥Ïöî.\"\n",
    "]\n",
    "\n",
    "sample_spanish_data = [\n",
    "    \"Espero que tengas un buen d√≠a.\",\n",
    "    \"Ojal√° que llueva ma√±ana.\",\n",
    "    \"Es importante que estudies mucho.\", \n",
    "    \"Dudo que √©l venga a la fiesta.\",\n",
    "    \"Me alegro de que est√©s aqu√≠.\"\n",
    "]\n",
    "\n",
    "# Create sample files if data directory is empty\n",
    "def create_sample_data():\n",
    "    korean_file = Path('data/articles/korean_sample.txt')\n",
    "    spanish_file = Path('data/articles/spanish_sample.txt')\n",
    "    \n",
    "    if not korean_file.exists():\n",
    "        with open(korean_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sample_korean_data))\n",
    "        print(f\"üìù Created sample Korean file: {korean_file}\")\n",
    "    \n",
    "    if not spanish_file.exists():\n",
    "        with open(spanish_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(sample_spanish_data))\n",
    "        print(f\"üìù Created sample Spanish file: {spanish_file}\")\n",
    "    \n",
    "    # Create sample CSV wordlist\n",
    "    csv_file = Path('data/wordlists/korean_words.csv')\n",
    "    if not csv_file.exists():\n",
    "        sample_words = pd.DataFrame({\n",
    "            'word': ['ÏïàÎÖïÌïòÏÑ∏Ïöî', 'Í∞êÏÇ¨Ìï©ÎãàÎã§', 'ÏÇ¨ÎûëÌï¥Ïöî', 'Í≥µÎ∂ÄÌïòÎã§', 'Î®πÎã§'],\n",
    "            'language': ['Korean'] * 5,\n",
    "            'level': ['A1', 'A1', 'A2', 'A2', 'A1'],\n",
    "            'translation': ['Hello', 'Thank you', 'I love you', 'To study', 'To eat'],\n",
    "            'emoji': ['üëã', 'üôè', '‚ù§Ô∏è', 'üìö', 'üçΩÔ∏è']\n",
    "        })\n",
    "        sample_words.to_csv(csv_file, index=False, encoding='utf-8')\n",
    "        print(f\"üìä Created sample CSV wordlist: {csv_file}\")\n",
    "\n",
    "# Create additional content files for better testing\n",
    "def create_additional_content():\n",
    "    \"\"\"Create additional diverse content for better language learning examples\"\"\"\n",
    "    \n",
    "    # Additional Korean grammar content\n",
    "    korean_grammar_file = Path('data/articles/korean_grammar_patterns.txt')\n",
    "    if not korean_grammar_file.exists():\n",
    "        additional_korean = [\n",
    "            \"ÏïÑÏπ®Ïóê ÏùºÏñ¥ÎÇòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"ÏóÑÎßàÍ∞Ä ÏöîÎ¶¨ÌïòÍ≥† ÏûàÏñ¥Ïöî.\", \n",
    "            \"ÌïôÏÉùÎì§Ïù¥ Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"ÎπÑÍ∞Ä ÎÇ¥Î¶¨Í≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"ÏïÑÏù¥Îì§Ïù¥ ÎÜÄÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"ÏÑ†ÏÉùÎãòÏù¥ ÏÑ§Î™ÖÌïòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"Í≥†ÏñëÏù¥Í∞Ä ÏûêÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"ÏπúÍµ¨Í∞Ä Ï†ÑÌôîÌïòÍ≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"ÏùåÏïÖÏùÑ Îì£Í≥† ÏûàÏñ¥Ïöî.\",\n",
    "            \"Ï±ÖÏùÑ ÏùΩÍ≥† ÏûàÏñ¥Ïöî.\"\n",
    "        ]\n",
    "        with open(korean_grammar_file, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n'.join(additional_korean))\n",
    "        print(f\"üìù Created additional Korean grammar file: {korean_grammar_file}\")\n",
    "    \n",
    "    # Extended vocabulary CSV\n",
    "    extended_vocab_file = Path('data/wordlists/korean_vocabulary_extended.csv')\n",
    "    if not extended_vocab_file.exists():\n",
    "        extended_vocab = pd.DataFrame({\n",
    "            'word': ['ÏïàÎÖïÌïòÏÑ∏Ïöî', 'Í∞êÏÇ¨Ìï©ÎãàÎã§', 'ÏÇ¨ÎûëÌï¥Ïöî', 'Í≥µÎ∂ÄÌïòÎã§', 'Î®πÎã§', 'ÎßàÏãúÎã§', 'ÏûêÎã§', 'ÏùºÌïòÎã§', 'ÎÜÄÎã§', 'Î≥¥Îã§'],\n",
    "            'language': ['Korean'] * 10,\n",
    "            'level': ['A1', 'A1', 'A2', 'A2', 'A1', 'A1', 'A1', 'A2', 'A1', 'A1'],\n",
    "            'translation': ['Hello', 'Thank you', 'I love you', 'To study', 'To eat', 'To drink', 'To sleep', 'To work', 'To play', 'To see'],\n",
    "            'emoji': ['üëã', 'üôè', '‚ù§Ô∏è', 'üìö', 'üçΩÔ∏è', 'ü•§', 'üò¥', 'üíº', 'üéÆ', 'üëÄ']\n",
    "        })\n",
    "        extended_vocab.to_csv(extended_vocab_file, index=False, encoding='utf-8')\n",
    "        print(f\"üìä Created extended vocabulary file: {extended_vocab_file}\")\n",
    "\n",
    "create_sample_data()\n",
    "create_additional_content()\n",
    "\n",
    "# Ingest all data with enhanced processing\n",
    "df_raw = ingest_data_files()\n",
    "\n",
    "if not df_raw.empty:\n",
    "    print(f\"\\nüéâ Data ingestion complete! Total entries: {len(df_raw):,}\")\n",
    "    \n",
    "    # Run comprehensive analysis\n",
    "    analyze_data_completeness(df_raw)\n",
    "    \n",
    "    # Quick pattern check for debugging\n",
    "    print(f\"\\nüîç Quick Pattern Check:\")\n",
    "    korean_sentences = df_raw[df_raw['language'] == 'Korean']\n",
    "    if len(korean_sentences) > 0:\n",
    "        pattern_matches = korean_sentences[korean_sentences['text'].str.contains('Í≥† ÏûàÏñ¥Ïöî', na=False, regex=False)]\n",
    "        print(f\"   ‚Ä¢ Korean sentences with '-Í≥† ÏûàÏñ¥Ïöî': {len(pattern_matches)}\")\n",
    "        \n",
    "        if len(pattern_matches) > 0:\n",
    "            print(\"   ‚Ä¢ Sample matches:\")\n",
    "            for idx, row in pattern_matches.head(3).iterrows():\n",
    "                print(f\"     - {row['text']}\")\n",
    "                print(f\"       (Source: {row.get('file_name', 'Unknown')})\")\n",
    "    \n",
    "    # Show rich metadata sample\n",
    "    if len(df_raw) > 0:\n",
    "        print(f\"\\nüìã Sample Enhanced Metadata:\")\n",
    "        sample_row = df_raw.iloc[0]\n",
    "        metadata_fields = ['text', 'language', 'cefr_level', 'file_name', 'file_type', \n",
    "                          'relative_path', 'file_size']\n",
    "        for field in metadata_fields:\n",
    "            if field in sample_row:\n",
    "                value = sample_row[field]\n",
    "                if field == 'text':\n",
    "                    value = str(value)[:50] + '...' if len(str(value)) > 50 else value\n",
    "                print(f\"   ‚Ä¢ {field}: {value}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No data ingested. Check your data folder and file formats.\")\n",
    "    \n",
    "    # Show what files exist\n",
    "    data_path = Path('data')\n",
    "    if data_path.exists():\n",
    "        print(f\"\\nüìÅ Files found in data directory:\")\n",
    "        for file_path in data_path.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                print(f\"   ‚Ä¢ {file_path.relative_to(data_path)} ({file_path.stat().st_size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"‚ùå Data directory does not exist!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Index Building\n",
    "\n",
    "Build a vector index from the ingested sentences for RAG queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Building simple RAG system...\n",
      "Added 178 documents. Total: 178\n",
      "üìä Language distribution in index: {'Korean': 113, 'Spanish': 26, 'English': 35, 'French': 4}\n",
      "üíæ RAG system built successfully!\n",
      "üöÄ Query engine ready!\n"
     ]
    }
   ],
   "source": [
    "def build_vector_index(df: pd.DataFrame):\n",
    "    \"\"\"Build simple RAG system from sentence dataframe\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"‚ùå No data to index!\")\n",
    "        return None\n",
    "    \n",
    "    print(\"üî® Building simple RAG system...\")\n",
    "    \n",
    "    if not rag_system:\n",
    "        print(\"‚ùå RAG system not available!\")\n",
    "        return None\n",
    "    \n",
    "    # Convert dataframe rows to Document objects\n",
    "    documents = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Create metadata\n",
    "        metadata = {\n",
    "            'language': row['language'],\n",
    "            'cefr_level': row['cefr_level'],\n",
    "            'source_file': row['source_file'],\n",
    "            'speaker': row['speaker'],\n",
    "            'timestamp': row['timestamp'],\n",
    "            'emoji': row['emoji'],\n",
    "            'row_id': idx\n",
    "        }\n",
    "        \n",
    "        # Create document\n",
    "        doc = Document(\n",
    "            text=row['text'],\n",
    "            metadata=metadata\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Add documents to RAG system\n",
    "    rag_system.add_documents(documents)\n",
    "    \n",
    "    print(\"üíæ RAG system built successfully!\")\n",
    "    return rag_system\n",
    "\n",
    "class SimpleQueryEngine:\n",
    "    \"\"\"Simple query engine wrapper for compatibility\"\"\"\n",
    "    def __init__(self, rag_system):\n",
    "        self.rag_system = rag_system\n",
    "    \n",
    "    def query(self, prompt: str):\n",
    "        if self.rag_system:\n",
    "            return self.rag_system.query(prompt)\n",
    "        else:\n",
    "            return \"RAG system not available\"\n",
    "\n",
    "# Build the index (only if df_raw exists)\n",
    "if 'df_raw' in locals() and not df_raw.empty:\n",
    "    index = build_vector_index(df_raw)\n",
    "    \n",
    "    if index:\n",
    "        # Create query engine\n",
    "        query_engine = SimpleQueryEngine(index)\n",
    "        print(\"üöÄ Query engine ready!\")\n",
    "    else:\n",
    "        print(\"‚ùå Failed to build RAG system!\")\n",
    "        index = None\n",
    "        query_engine = None\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available to build index (df_raw not found)\")\n",
    "    index = None\n",
    "    query_engine = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Query Functions\n",
    "\n",
    "Create utility functions for querying the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Helper query functions ready!\n"
     ]
    }
   ],
   "source": [
    "def ask_multilingual(prompt: str) -> str:\n",
    "    \"\"\"General multilingual RAG query wrapper\"\"\"\n",
    "    if not index:\n",
    "        return \"‚ùå Index not available. Please build the index first.\"\n",
    "    \n",
    "    print(f\"ü§î Querying: {prompt[:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        response = query_engine.query(prompt)\n",
    "        \n",
    "        # Track tokens (approximate)\n",
    "        token_tracker.add_llm_tokens(int(count_tokens(prompt + str(response))))\n",
    "        \n",
    "        return str(response)\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Query failed: {str(e)}\"\n",
    "\n",
    "def vocab_drill(pattern: str, lang: str, level: str = \"A2\", k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Specialized vocabulary drill function\"\"\"\n",
    "    if not index:\n",
    "        print(\"‚ùå Index not available. Please build the index first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üéØ Vocab drill: {pattern} ({lang}, {level} level, {k} examples)\")\n",
    "    \n",
    "    # Debug: Check what data we have\n",
    "    if 'df_raw' in globals():\n",
    "        print(f\"üìä Total sentences in database: {len(df_raw)}\")\n",
    "        lang_counts = df_raw['language'].value_counts()\n",
    "        print(f\"üìä Available languages: {dict(lang_counts)}\")\n",
    "        \n",
    "        # Check for the specific pattern\n",
    "        korean_sentences = df_raw[df_raw['language'] == lang]\n",
    "        print(f\"üìä {lang} sentences: {len(korean_sentences)}\")\n",
    "        \n",
    "        # Try different pattern matching approaches\n",
    "        pattern_matches = korean_sentences[korean_sentences['text'].str.contains(pattern, na=False, regex=False)]\n",
    "        print(f\"üìä Sentences containing '{pattern}': {len(pattern_matches)}\")\n",
    "        \n",
    "        if len(pattern_matches) > 0:\n",
    "            print(\"‚úÖ Found matching sentences:\")\n",
    "            for idx, row in pattern_matches.head(3).iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['text']}\")\n",
    "    \n",
    "    try:\n",
    "        # Parse response into structured format\n",
    "        sentences_data = []\n",
    "        \n",
    "        # Filter dataframe directly for more reliable results\n",
    "        # Use exact string matching instead of regex for Korean patterns\n",
    "        filtered_df = df_raw[\n",
    "            (df_raw['language'] == lang) & \n",
    "            (df_raw['text'].str.contains(pattern, na=False, regex=False))  # Changed to regex=False\n",
    "        ]\n",
    "        \n",
    "        # If level filtering is too restrictive, try without it first\n",
    "        if not filtered_df.empty:\n",
    "            # Apply level filter if we have matches\n",
    "            level_filtered = filtered_df[filtered_df['cefr_level'] == level]\n",
    "            if not level_filtered.empty:\n",
    "                filtered_df = level_filtered\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No {level} level sentences found, using all levels\")\n",
    "        \n",
    "        filtered_df = filtered_df.head(k)\n",
    "        \n",
    "        if not filtered_df.empty:\n",
    "            print(f\"‚úÖ Found {len(filtered_df)} matching sentences\")\n",
    "            # Use LLM to generate translations for the filtered sentences\n",
    "            for _, row in filtered_df.iterrows():\n",
    "                if rag_system and rag_system.client:\n",
    "                    try:\n",
    "                        # Generate translation using OpenAI\n",
    "                        translation_response = rag_system.client.chat.completions.create(\n",
    "                            model=\"gpt-4o-mini\",\n",
    "                            messages=[\n",
    "                                {\"role\": \"system\", \"content\": \"You are a helpful translator. Translate the given sentence to English. Provide only the translation, no extra text.\"},\n",
    "                                {\"role\": \"user\", \"content\": f\"Translate this {lang} sentence to English: '{row['text']}'\"}\n",
    "                            ],\n",
    "                            temperature=0.1,\n",
    "                            max_tokens=100\n",
    "                        )\n",
    "                        translation = translation_response.choices[0].message.content.strip()\n",
    "                    except Exception as e:\n",
    "                        translation = f\"Translation unavailable: {e}\"\n",
    "                else:\n",
    "                    translation = \"Translation service unavailable\"\n",
    "                \n",
    "                sentences_data.append({\n",
    "                    'Sentence': row['text'],\n",
    "                    'English': translation,\n",
    "                    'Emoji': row.get('emoji', 'üìù'),\n",
    "                    'Source': Path(row['source_file']).name,\n",
    "                    'Level': row['cefr_level']\n",
    "                })\n",
    "        \n",
    "        result_df = pd.DataFrame(sentences_data)\n",
    "        \n",
    "        if result_df.empty:\n",
    "            print(f\"‚ö†Ô∏è No sentences found matching pattern '{pattern}' in {lang}\")\n",
    "            \n",
    "            # Debug: Show available Korean sentences\n",
    "            if lang == 'Korean' and 'df_raw' in globals():\n",
    "                korean_samples = df_raw[df_raw['language'] == 'Korean']['text'].head(5).tolist()\n",
    "                print(\"üìã Available Korean sentences for debugging:\")\n",
    "                for i, sentence in enumerate(korean_samples, 1):\n",
    "                    print(f\"   {i}. {sentence}\")\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Vocab drill failed: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"üõ†Ô∏è Helper query functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Enhanced query functions ready!\n",
      "üí° Try these new functions:\n",
      "   ‚Ä¢ query_by_file('korean', 'pattern') - Search within specific files\n",
      "   ‚Ä¢ query_by_pattern('Í≥† ÏûàÏñ¥Ïöî', 'Korean') - Pattern search with language filter\n",
      "   ‚Ä¢ smart_content_query('Korean grammar', {'language': 'Korean'}) - Filtered RAG query\n",
      "   ‚Ä¢ explore_file_contents() - Browse all file contents\n"
     ]
    }
   ],
   "source": [
    "# Enhanced query functions that utilize rich metadata\n",
    "def query_by_file(filename: str, query: str = \"\") -> pd.DataFrame:\n",
    "    \"\"\"Query content from a specific file\"\"\"\n",
    "    if 'df_raw' not in globals() or df_raw.empty:\n",
    "        print(\"‚ùå No data available. Please run data ingestion first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter by filename\n",
    "    file_data = df_raw[df_raw['file_name'].str.contains(filename, case=False, na=False)]\n",
    "    \n",
    "    if file_data.empty:\n",
    "        print(f\"‚ùå No data found for file containing '{filename}'\")\n",
    "        available_files = df_raw['file_name'].unique()\n",
    "        print(f\"üìÅ Available files: {', '.join(available_files[:10])}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"üìÑ Found {len(file_data)} entries from file(s) matching '{filename}'\")\n",
    "    \n",
    "    if query:\n",
    "        # Apply additional text filtering\n",
    "        filtered_data = file_data[file_data['text'].str.contains(query, case=False, na=False, regex=False)]\n",
    "        print(f\"üîç Filtered to {len(filtered_data)} entries containing '{query}'\")\n",
    "        return filtered_data\n",
    "    \n",
    "    return file_data\n",
    "\n",
    "def query_by_pattern(pattern: str, language: str = None, max_results: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Enhanced pattern search with language filtering\"\"\"\n",
    "    if 'df_raw' not in globals() or df_raw.empty:\n",
    "        print(\"‚ùå No data available. Please run data ingestion first.\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Start with all data or filter by language\n",
    "    search_data = df_raw\n",
    "    if language:\n",
    "        search_data = df_raw[df_raw['language'].str.contains(language, case=False, na=False)]\n",
    "        print(f\"üåç Searching in {language} content ({len(search_data)} entries)\")\n",
    "    \n",
    "    # Search for pattern\n",
    "    pattern_matches = search_data[search_data['text'].str.contains(pattern, na=False, regex=False)]\n",
    "    \n",
    "    print(f\"üéØ Found {len(pattern_matches)} matches for pattern '{pattern}'\")\n",
    "    \n",
    "    if len(pattern_matches) > 0:\n",
    "        # Show sources\n",
    "        sources = pattern_matches['file_name'].value_counts()\n",
    "        print(f\"üìÅ Found in files: {dict(sources)}\")\n",
    "        \n",
    "        # Return limited results\n",
    "        return pattern_matches.head(max_results)\n",
    "    \n",
    "    return pattern_matches\n",
    "\n",
    "def smart_content_query(query: str, filters: Dict[str, str] = None) -> str:\n",
    "    \"\"\"Use enhanced RAG system with filtering\"\"\"\n",
    "    if not index:\n",
    "        return \"‚ùå RAG index not available. Please build the index first.\"\n",
    "    \n",
    "    filters = filters or {}\n",
    "    \n",
    "    try:\n",
    "        # Use enhanced RAG query with filters\n",
    "        response = index.query(\n",
    "            query_text=query,\n",
    "            language_filter=filters.get('language'),\n",
    "            level_filter=filters.get('level'),\n",
    "            file_filter=filters.get('file'),\n",
    "            top_k=filters.get('top_k', 5)\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Query failed: {str(e)}\"\n",
    "\n",
    "def explore_file_contents() -> None:\n",
    "    \"\"\"Interactive exploration of file contents\"\"\"\n",
    "    if 'df_raw' not in globals() or df_raw.empty:\n",
    "        print(\"‚ùå No data available. Please run data ingestion first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"üóÇÔ∏è FILE CONTENT EXPLORER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Group by file\n",
    "    file_groups = df_raw.groupby('file_name')\n",
    "    \n",
    "    for filename, group in file_groups:\n",
    "        print(f\"\\nüìÑ {filename}\")\n",
    "        print(f\"   ‚Ä¢ Entries: {len(group)}\")\n",
    "        print(f\"   ‚Ä¢ Languages: {', '.join(group['language'].unique())}\")\n",
    "        print(f\"   ‚Ä¢ CEFR levels: {', '.join(group['cefr_level'].unique())}\")\n",
    "        print(f\"   ‚Ä¢ Total characters: {group['text'].str.len().sum():,}\")\n",
    "        print(f\"   ‚Ä¢ File size: {group['file_size'].iloc[0]:,} bytes\")\n",
    "        \n",
    "        # Show sample content\n",
    "        sample_texts = group['text'].head(2).tolist()\n",
    "        for i, text in enumerate(sample_texts, 1):\n",
    "            preview = text[:100] + '...' if len(text) > 100 else text\n",
    "            print(f\"   ‚Ä¢ Sample {i}: {preview}\")\n",
    "\n",
    "print(\"üîß Enhanced query functions ready!\")\n",
    "print(\"üí° Try these new functions:\")\n",
    "print(\"   ‚Ä¢ query_by_file('korean', 'pattern') - Search within specific files\")\n",
    "print(\"   ‚Ä¢ query_by_pattern('Í≥† ÏûàÏñ¥Ïöî', 'Korean') - Pattern search with language filter\")\n",
    "print(\"   ‚Ä¢ smart_content_query('Korean grammar', {'language': 'Korean'}) - Filtered RAG query\")\n",
    "print(\"   ‚Ä¢ explore_file_contents() - Browse all file contents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Demo Queries\n",
    "\n",
    "Let's test our RAG system with some example queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ ENHANCED DEMO: Complete File Learning\n",
      "============================================================\n",
      "‚úÖ All required functions are available!\n",
      "\n",
      "üìä Demo 1: File Content Understanding\n",
      "üóÇÔ∏è FILE CONTENT EXPLORER\n",
      "==================================================\n",
      "\n",
      "üìÑ french_basics.txt\n",
      "   ‚Ä¢ Entries: 15\n",
      "   ‚Ä¢ Languages: Spanish, English, French\n",
      "   ‚Ä¢ CEFR levels: B2, A2, B1\n",
      "   ‚Ä¢ Total characters: 482\n",
      "   ‚Ä¢ File size: 511 bytes\n",
      "   ‚Ä¢ Sample 1: J'aime beaucoup √©tudier le fran√ßais\n",
      "   ‚Ä¢ Sample 2: Nous allons au cin√©ma ce soir\n",
      "\n",
      "üìÑ korean_conversation.json\n",
      "   ‚Ä¢ Entries: 5\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A2, A1\n",
      "   ‚Ä¢ Total characters: 153\n",
      "   ‚Ä¢ File size: 820 bytes\n",
      "   ‚Ä¢ Sample 1: ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏñ¥Ïöî.\n",
      "   ‚Ä¢ Sample 2: ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥ Í≥µÎ∂ÄÌïòÍ≥† Í≥ÑÏãúÎäîÍµ∞Ïöî. ÏñºÎßàÎÇò Ïò§Îûò Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî?\n",
      "\n",
      "üìÑ korean_drama.srt\n",
      "   ‚Ä¢ Entries: 7\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A2, A1\n",
      "   ‚Ä¢ Total characters: 116\n",
      "   ‚Ä¢ File size: 526 bytes\n",
      "   ‚Ä¢ Sample 1: ÏïàÎÖïÌïòÏÑ∏Ïöî, Ï†ÄÎäî ÍπÄÎØºÏàòÏòàÏöî.\n",
      "   ‚Ä¢ Sample 2: Ïò§Îäò ÌöåÏÇ¨ÏóêÏÑú ÏùºÌïòÍ≥† ÏûàÏñ¥Ïöî.\n",
      "\n",
      "üìÑ korean_future_tense.txt\n",
      "   ‚Ä¢ Entries: 15\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A1\n",
      "   ‚Ä¢ Total characters: 176\n",
      "   ‚Ä¢ File size: 205 bytes\n",
      "   ‚Ä¢ Sample 1: ÎÇ¥Ïùº ÏπúÍµ¨Î•º ÎßåÎÇ† Í±∞ÏòàÏöî\n",
      "   ‚Ä¢ Sample 2: Îã§Ïùå Ï£ºÏóê Ïó¨ÌñâÏùÑ Í∞à Í±∞ÏòàÏöî\n",
      "\n",
      "üìÑ korean_grammar_patterns.txt\n",
      "   ‚Ä¢ Entries: 20\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A1\n",
      "   ‚Ä¢ Total characters: 203\n",
      "   ‚Ä¢ File size: 242 bytes\n",
      "   ‚Ä¢ Sample 1: ÏïÑÏπ®Ïóê ÏùºÏñ¥ÎÇòÍ≥† ÏûàÏñ¥Ïöî\n",
      "   ‚Ä¢ Sample 2: ÏóÑÎßàÍ∞Ä ÏöîÎ¶¨ÌïòÍ≥† ÏûàÏñ¥Ïöî\n",
      "\n",
      "üìÑ korean_lesson.json\n",
      "   ‚Ä¢ Entries: 6\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A2, A1, B1\n",
      "   ‚Ä¢ Total characters: 147\n",
      "   ‚Ä¢ File size: 1,086 bytes\n",
      "   ‚Ä¢ Sample 1: ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏñ¥Ïöî.\n",
      "   ‚Ä¢ Sample 2: ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥ Í≥µÎ∂ÄÎ•º ÎèÑÏôÄÎìúÎ¶¥Í≤åÏöî.\n",
      "\n",
      "üìÑ korean_past_tense.txt\n",
      "   ‚Ä¢ Entries: 15\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A1, A2\n",
      "   ‚Ä¢ Total characters: 177\n",
      "   ‚Ä¢ File size: 206 bytes\n",
      "   ‚Ä¢ Sample 1: Ï†ÄÎäî Ïñ¥Ï†ú ÏπúÍµ¨Î•º ÎßåÎÇ¨Ïñ¥Ïöî\n",
      "   ‚Ä¢ Sample 2: Ïò§Îäò ÏïÑÏπ®Ïóê Ïö¥ÎèôÌñàÏñ¥Ïöî\n",
      "\n",
      "üìÑ korean_sample.txt\n",
      "   ‚Ä¢ Entries: 15\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A1\n",
      "   ‚Ä¢ Total characters: 219\n",
      "   ‚Ä¢ File size: 248 bytes\n",
      "   ‚Ä¢ Sample 1: Ï†ÄÎäî ÏßÄÍ∏à ÌïúÍµ≠Ïñ¥Î•º Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî\n",
      "   ‚Ä¢ Sample 2: ÏπúÍµ¨Îì§Í≥º Ìï®Íªò ÏòÅÌôîÎ•º Î≥¥Í≥† ÏûàÏñ¥Ïöî\n",
      "\n",
      "üìÑ korean_vocabulary_extended.csv\n",
      "   ‚Ä¢ Entries: 20\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A1, A2\n",
      "   ‚Ä¢ Total characters: 264\n",
      "   ‚Ä¢ File size: 738 bytes\n",
      "   ‚Ä¢ Sample 1: ÏïàÎÖïÌïòÏÑ∏Ïöî - Hello\n",
      "   ‚Ä¢ Sample 2: Í∞êÏÇ¨Ìï©ÎãàÎã§ - Thank you\n",
      "\n",
      "üìÑ korean_words.csv\n",
      "   ‚Ä¢ Entries: 10\n",
      "   ‚Ä¢ Languages: Korean\n",
      "   ‚Ä¢ CEFR levels: A1\n",
      "   ‚Ä¢ Total characters: 139\n",
      "   ‚Ä¢ File size: 381 bytes\n",
      "   ‚Ä¢ Sample 1: ÏïàÎÖïÌïòÏÑ∏Ïöî - Hello\n",
      "   ‚Ä¢ Sample 2: Í∞êÏÇ¨Ìï©ÎãàÎã§ - Thank you\n",
      "\n",
      "üìÑ spanish_complete.txt\n",
      "   ‚Ä¢ Entries: 20\n",
      "   ‚Ä¢ Languages: Spanish, English\n",
      "   ‚Ä¢ CEFR levels: B1, A2, A1, B2\n",
      "   ‚Ä¢ Total characters: 674\n",
      "   ‚Ä¢ File size: 713 bytes\n",
      "   ‚Ä¢ Sample 1: Me gusta estudiar espa√±ol por las ma√±anas\n",
      "   ‚Ä¢ Sample 2: Estoy leyendo un libro muy interesante\n",
      "\n",
      "üìÑ spanish_sample.txt\n",
      "   ‚Ä¢ Entries: 10\n",
      "   ‚Ä¢ Languages: Spanish, English\n",
      "   ‚Ä¢ CEFR levels: A2, B1, A1, B2\n",
      "   ‚Ä¢ Total characters: 288\n",
      "   ‚Ä¢ File size: 307 bytes\n",
      "   ‚Ä¢ Sample 1: Espero que tengas un buen d√≠a\n",
      "   ‚Ä¢ Sample 2: Ojal√° que llueva ma√±ana\n",
      "\n",
      "üìÑ spanish_vocabulary.csv\n",
      "   ‚Ä¢ Entries: 20\n",
      "   ‚Ä¢ Languages: English, Spanish\n",
      "   ‚Ä¢ CEFR levels: A1, A2, B2\n",
      "   ‚Ä¢ Total characters: 292\n",
      "   ‚Ä¢ File size: 680 bytes\n",
      "   ‚Ä¢ Sample 1: hola - Hello\n",
      "   ‚Ä¢ Sample 2: gracias - Thank you\n",
      "\n",
      "\n",
      "üîç Demo 2: File-Specific Content Queries\n",
      "üìÑ Found 113 entries from file(s) matching 'korean'\n",
      "üîç Filtered to 50 entries containing 'Í≥† ÏûàÏñ¥Ïöî'\n",
      "\n",
      "‚úÖ Found Korean pattern examples from files:\n",
      "   ‚Ä¢ ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥Î•º Î∞∞Ïö∞Í≥† ÏûàÏñ¥Ïöî.\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "   ‚Ä¢ ÏïàÎÖïÌïòÏÑ∏Ïöî! ÌïúÍµ≠Ïñ¥ Í≥µÎ∂ÄÌïòÍ≥† Í≥ÑÏãúÎäîÍµ∞Ïöî. ÏñºÎßàÎÇò Ïò§Îûò Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî?\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "   ‚Ä¢ 6Í∞úÏõî ÎèôÏïà Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî. ÏïÑÏßÅ Ïñ¥Î†§ÏõåÏöî.\n",
      "     (From: korean_conversation.json, Level: A1)\n",
      "   ‚Ä¢ 6Í∞úÏõîÏù¥Î©¥ ÍΩ§ Ïò§Îûò Í≥µÎ∂ÄÌïòÍ≥† ÏûàÎÑ§Ïöî! Îß§Ïùº Ïó∞ÏäµÌïòÍ≥† ÏûàÏñ¥Ïöî?\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "   ‚Ä¢ ÎÑ§, Îß§Ïùº Ìïú ÏãúÍ∞ÑÏî© Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî. K-ÎìúÎùºÎßàÎèÑ Î≥¥Í≥† ÏûàÏñ¥Ïöî.\n",
      "     (From: korean_conversation.json, Level: A2)\n",
      "\n",
      "\n",
      "üéØ Demo 3: Enhanced Pattern Search\n",
      "üåç Searching in Spanish content (26 entries)\n",
      "üéØ Found 12 matches for pattern 'que'\n",
      "üìÅ Found in files: {'spanish_sample.txt': 7, 'spanish_complete.txt': 5}\n",
      "\n",
      "üá™üá∏ Spanish 'que' patterns found:\n",
      "   ‚Ä¢ Los ni√±os est√°n jugando en el parque\n",
      "     (Source: spanish_complete.txt)\n",
      "   ‚Ä¢ La pel√≠cula que vimos ayer fue excelente\n",
      "     (Source: spanish_complete.txt)\n",
      "   ‚Ä¢ Ojal√° que llueva ma√±ana, necesitamos agua\n",
      "     (Source: spanish_complete.txt)\n",
      "\n",
      "\n",
      "ü§ñ Demo 4: Smart Filtered RAG Queries\n",
      "üìä RAG Index Summary:\n",
      "   ‚Ä¢ Total documents: 178\n",
      "   ‚Ä¢ Languages: {'Korean': 113, 'Spanish': 26, 'English': 35, 'French': 4}\n",
      "   ‚Ä¢ Files: 0\n",
      "\n",
      "üá∞üá∑ Korean Grammar Query Response:\n",
      "Great question! In Korean, the present continuous tense is formed by using the verb stem and adding the suffix \"-Í≥† ÏûàÎã§\" (go itda). This structure indicates that an action is currently in progress.\n",
      "\n",
      "Let's break it down with the verb \"Ïò§Îã§\" (to come):\n",
      "\n",
      "1. **Verb Stem**: The verb \"Ïò§Îã§\" has the stem \"Ïò§\" (o).\n",
      "2. **Present Continuous Form**: To express \"coming\" in the present continuous tense, you would say \"Ïò§Í≥† ÏûàÎã§\" (ogo itda), which means \"is coming.\"\n",
      "\n",
      "### Example Sentences:\n",
      "- **Í∑∏Í∞Ä ÏßëÏóê Ïò§Í≥† ÏûàÎã§.** (Geuga jibe ogo itda.) - \"He is coming home.\"\n",
      "- **ÏπúÍµ¨Í∞Ä ÏßÄÍ∏à Ïò§Í≥† ÏûàÏñ¥Ïöî.** (Chinguga jigeum ogo isseoyo.) - \"My friend is coming now.\"\n",
      "\n",
      "### Pattern:\n",
      "- **Verb Stem + Í≥† ÏûàÎã§** \n",
      "- For \"Ïò§Îã§\": Ïò§ + Í≥† ÏûàÎã§ = Ïò§Í≥† ÏûàÎã§\n",
      "\n",
      "This pattern can be applied to other verbs as well. For example:\n",
      "- **Í∞ÄÎã§** (gada - to go) becomes **Í∞ÄÍ≥† ÏûàÎã§** (gago itda - is going).\n",
      "- **Î®πÎã§** (meokda - to eat) becomes **Î®πÍ≥† ÏûàÎã§** (meokgo itda - is eating).\n",
      "\n",
      "Keep practicing this structure with different verbs, and you'll get the hang of it! You're doing great!\n",
      "\n",
      "üá∞üá∑ Korean Grammar Query Response:\n",
      "Great question! In Korean, the present continuous tense is formed by using the verb stem and adding the suffix \"-Í≥† ÏûàÎã§\" (go itda). This structure indicates that an action is currently in progress.\n",
      "\n",
      "Let's break it down with the verb \"Ïò§Îã§\" (to come):\n",
      "\n",
      "1. **Verb Stem**: The verb \"Ïò§Îã§\" has the stem \"Ïò§\" (o).\n",
      "2. **Present Continuous Form**: To express \"coming\" in the present continuous tense, you would say \"Ïò§Í≥† ÏûàÎã§\" (ogo itda), which means \"is coming.\"\n",
      "\n",
      "### Example Sentences:\n",
      "- **Í∑∏Í∞Ä ÏßëÏóê Ïò§Í≥† ÏûàÎã§.** (Geuga jibe ogo itda.) - \"He is coming home.\"\n",
      "- **ÏπúÍµ¨Í∞Ä ÏßÄÍ∏à Ïò§Í≥† ÏûàÏñ¥Ïöî.** (Chinguga jigeum ogo isseoyo.) - \"My friend is coming now.\"\n",
      "\n",
      "### Pattern:\n",
      "- **Verb Stem + Í≥† ÏûàÎã§** \n",
      "- For \"Ïò§Îã§\": Ïò§ + Í≥† ÏûàÎã§ = Ïò§Í≥† ÏûàÎã§\n",
      "\n",
      "This pattern can be applied to other verbs as well. For example:\n",
      "- **Í∞ÄÎã§** (gada - to go) becomes **Í∞ÄÍ≥† ÏûàÎã§** (gago itda - is going).\n",
      "- **Î®πÎã§** (meokda - to eat) becomes **Î®πÍ≥† ÏûàÎã§** (meokgo itda - is eating).\n",
      "\n",
      "Keep practicing this structure with different verbs, and you'll get the hang of it! You're doing great!\n",
      "\n",
      "üìö Grammar-focused Query Response:\n",
      "Great question! In the provided Korean sentences, we can identify a common grammar pattern that is used to describe ongoing actions. This pattern is formed by using the verb stem followed by the suffix \"-Í≥† ÏûàÏñ¥Ïöî,\" which indicates that the action is currently happening.\n",
      "\n",
      "Here are the examples from the context:\n",
      "\n",
      "1. **ÏùºÏñ¥ÎÇòÎã§ (to wake up)** ‚Üí ÏùºÏñ¥ÎÇòÍ≥† ÏûàÏñ¥Ïöî (I am waking up)\n",
      "2. **ÏöîÎ¶¨ÌïòÎã§ (to cook)** ‚Üí ÏöîÎ¶¨ÌïòÍ≥† ÏûàÏñ¥Ïöî (Mom is cooking)\n",
      "3. **Í≥µÎ∂ÄÌïòÎã§ (to study)** ‚Üí Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî (The students are studying)\n",
      "4. **ÎÇ¥Î¶¨Îã§ (to fall, as in rain)** ‚Üí ÎÇ¥Î¶¨Í≥† ÏûàÏñ¥Ïöî (It is raining)\n",
      "5. **ÎÜÄÎã§ (to play)** ‚Üí ÎÜÄÍ≥† ÏûàÏñ¥Ïöî (The children are playing)\n",
      "\n",
      "This structure is very useful for expressing actions in progress. Keep practicing, and you'll become more comfortable with using this pattern in your conversations! If you have more questions or need further examples, feel free to ask!\n",
      "\n",
      "\n",
      "üé¥ Demo 5: Enhanced Vocabulary Drill\n",
      "üéØ Vocab drill: -Í≥† ÏûàÏñ¥Ïöî (Korean, A2 level, 10 examples)\n",
      "üìä Total sentences in database: 178\n",
      "üìä Available languages: {'Korean': 113, 'English': 35, 'Spanish': 26, 'French': 4}\n",
      "üìä Korean sentences: 113\n",
      "üìä Sentences containing '-Í≥† ÏûàÏñ¥Ïöî': 2\n",
      "‚úÖ Found matching sentences:\n",
      "   ‚Ä¢ '-Í≥† ÏûàÏñ¥Ïöî' Î¨∏Î≤ïÏùÑ ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî.\n",
      "   ‚Ä¢ '-Í≥† ÏûàÏñ¥Ïöî'Îäî ÌòÑÏû¨ ÏßÑÌñâÌòïÏùÑ ÎÇòÌÉÄÎÇ¥Ïöî. ÏßÄÍ∏à ÌïòÍ≥† ÏûàÎäî ÌñâÎèôÏùÑ ÎßêÌï† Îïå ÏÇ¨Ïö©Ìï¥Ïöî.\n",
      "‚ö†Ô∏è No A2 level sentences found, using all levels\n",
      "‚úÖ Found 2 matching sentences\n",
      "\n",
      "üìö Grammar-focused Query Response:\n",
      "Great question! In the provided Korean sentences, we can identify a common grammar pattern that is used to describe ongoing actions. This pattern is formed by using the verb stem followed by the suffix \"-Í≥† ÏûàÏñ¥Ïöî,\" which indicates that the action is currently happening.\n",
      "\n",
      "Here are the examples from the context:\n",
      "\n",
      "1. **ÏùºÏñ¥ÎÇòÎã§ (to wake up)** ‚Üí ÏùºÏñ¥ÎÇòÍ≥† ÏûàÏñ¥Ïöî (I am waking up)\n",
      "2. **ÏöîÎ¶¨ÌïòÎã§ (to cook)** ‚Üí ÏöîÎ¶¨ÌïòÍ≥† ÏûàÏñ¥Ïöî (Mom is cooking)\n",
      "3. **Í≥µÎ∂ÄÌïòÎã§ (to study)** ‚Üí Í≥µÎ∂ÄÌïòÍ≥† ÏûàÏñ¥Ïöî (The students are studying)\n",
      "4. **ÎÇ¥Î¶¨Îã§ (to fall, as in rain)** ‚Üí ÎÇ¥Î¶¨Í≥† ÏûàÏñ¥Ïöî (It is raining)\n",
      "5. **ÎÜÄÎã§ (to play)** ‚Üí ÎÜÄÍ≥† ÏûàÏñ¥Ïöî (The children are playing)\n",
      "\n",
      "This structure is very useful for expressing actions in progress. Keep practicing, and you'll become more comfortable with using this pattern in your conversations! If you have more questions or need further examples, feel free to ask!\n",
      "\n",
      "\n",
      "üé¥ Demo 5: Enhanced Vocabulary Drill\n",
      "üéØ Vocab drill: -Í≥† ÏûàÏñ¥Ïöî (Korean, A2 level, 10 examples)\n",
      "üìä Total sentences in database: 178\n",
      "üìä Available languages: {'Korean': 113, 'English': 35, 'Spanish': 26, 'French': 4}\n",
      "üìä Korean sentences: 113\n",
      "üìä Sentences containing '-Í≥† ÏûàÏñ¥Ïöî': 2\n",
      "‚úÖ Found matching sentences:\n",
      "   ‚Ä¢ '-Í≥† ÏûàÏñ¥Ïöî' Î¨∏Î≤ïÏùÑ ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî.\n",
      "   ‚Ä¢ '-Í≥† ÏûàÏñ¥Ïöî'Îäî ÌòÑÏû¨ ÏßÑÌñâÌòïÏùÑ ÎÇòÌÉÄÎÇ¥Ïöî. ÏßÄÍ∏à ÌïòÍ≥† ÏûàÎäî ÌñâÎèôÏùÑ ÎßêÌï† Îïå ÏÇ¨Ïö©Ìï¥Ïöî.\n",
      "‚ö†Ô∏è No A2 level sentences found, using all levels\n",
      "‚úÖ Found 2 matching sentences\n",
      "\n",
      "‚úÖ Enhanced Korean Drill Results (2 examples):\n",
      "\n",
      "‚úÖ Enhanced Korean Drill Results (2 examples):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>English</th>\n",
       "      <th>Emoji</th>\n",
       "      <th>Source</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'-Í≥† ÏûàÏñ¥Ïöî' Î¨∏Î≤ïÏùÑ ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî.</td>\n",
       "      <td>\"Please explain the grammar of '-Í≥† ÏûàÏñ¥Ïöî'.\"</td>\n",
       "      <td>üá∞üá∑</td>\n",
       "      <td>korean_lesson.json</td>\n",
       "      <td>A1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'-Í≥† ÏûàÏñ¥Ïöî'Îäî ÌòÑÏû¨ ÏßÑÌñâÌòïÏùÑ ÎÇòÌÉÄÎÇ¥Ïöî. ÏßÄÍ∏à ÌïòÍ≥† ÏûàÎäî ÌñâÎèôÏùÑ ÎßêÌï† Îïå ÏÇ¨Ïö©Ìï¥Ïöî.</td>\n",
       "      <td>\"-Í≥† ÏûàÏñ¥Ïöî\" indicates the present continuous tens...</td>\n",
       "      <td>üá∞üá∑</td>\n",
       "      <td>korean_lesson.json</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Sentence  \\\n",
       "0                            '-Í≥† ÏûàÏñ¥Ïöî' Î¨∏Î≤ïÏùÑ ÏÑ§Î™ÖÌï¥ Ï£ºÏÑ∏Ïöî.   \n",
       "1  '-Í≥† ÏûàÏñ¥Ïöî'Îäî ÌòÑÏû¨ ÏßÑÌñâÌòïÏùÑ ÎÇòÌÉÄÎÇ¥Ïöî. ÏßÄÍ∏à ÌïòÍ≥† ÏûàÎäî ÌñâÎèôÏùÑ ÎßêÌï† Îïå ÏÇ¨Ïö©Ìï¥Ïöî.   \n",
       "\n",
       "                                             English Emoji  \\\n",
       "0          \"Please explain the grammar of '-Í≥† ÏûàÏñ¥Ïöî'.\"    üá∞üá∑   \n",
       "1  \"-Í≥† ÏûàÏñ¥Ïöî\" indicates the present continuous tens...    üá∞üá∑   \n",
       "\n",
       "               Source Level  \n",
       "0  korean_lesson.json    A1  \n",
       "1  korean_lesson.json    B1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Examples by source file:\n",
      "   ‚Ä¢ korean_lesson.json: 2 examples\n",
      "\n",
      "üéâ Enhanced demo complete! The system now fully learns from complete files with:\n",
      "   ‚úÖ Comprehensive file parsing (TXT, CSV, JSON, SRT, VTT)\n",
      "   ‚úÖ Rich metadata preservation (file info, context, structure)\n",
      "   ‚úÖ Enhanced RAG system with filtering\n",
      "   ‚úÖ File-specific querying capabilities\n",
      "   ‚úÖ Pattern search across languages\n",
      "   ‚úÖ Contextual information retention\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Demo: Complete File Learning Capabilities\n",
    "print(\"üéØ ENHANCED DEMO: Complete File Learning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if required functions are available\n",
    "required_functions = ['explore_file_contents', 'query_by_file', 'query_by_pattern', 'smart_content_query', 'vocab_drill']\n",
    "missing_functions = []\n",
    "\n",
    "for func_name in required_functions:\n",
    "    if func_name not in globals():\n",
    "        missing_functions.append(func_name)\n",
    "\n",
    "if missing_functions:\n",
    "    print(\"‚ö†Ô∏è Required functions not found. Please run the following cells first:\")\n",
    "    print(\"   1. Cell 18: Helper query functions (vocab_drill)\")\n",
    "    print(\"   2. Cell 19: Enhanced query functions (query_by_file, query_by_pattern, etc.)\")\n",
    "    print(f\"   Missing functions: {', '.join(missing_functions)}\")\n",
    "    print(\"\\nüîÑ After running those cells, re-run this demo cell.\")\n",
    "else:\n",
    "    print(\"‚úÖ All required functions are available!\")\n",
    "\n",
    "    # Demo 1: Show comprehensive file understanding\n",
    "    print(\"\\nüìä Demo 1: File Content Understanding\")\n",
    "    try:\n",
    "        explore_file_contents()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in file exploration: {e}\")\n",
    "\n",
    "    # Demo 2: File-specific querying\n",
    "    print(\"\\n\\nüîç Demo 2: File-Specific Content Queries\")\n",
    "\n",
    "    try:\n",
    "        # Query Korean grammar files specifically\n",
    "        korean_pattern_results = query_by_file('korean', 'Í≥† ÏûàÏñ¥Ïöî')\n",
    "        if not korean_pattern_results.empty:\n",
    "            print(f\"\\n‚úÖ Found Korean pattern examples from files:\")\n",
    "            for _, row in korean_pattern_results.head(5).iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['text']}\")\n",
    "                print(f\"     (From: {row['file_name']}, Level: {row['cefr_level']})\")\n",
    "                if 'context' in row and pd.notna(row['context']):\n",
    "                    print(f\"     Context: {row['context'][:60]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in file querying: {e}\")\n",
    "\n",
    "    # Demo 3: Enhanced pattern search across languages\n",
    "    print(f\"\\n\\nüéØ Demo 3: Enhanced Pattern Search\")\n",
    "\n",
    "    try:\n",
    "        # Search for Spanish subjunctive patterns\n",
    "        spanish_results = query_by_pattern('que', 'Spanish', max_results=5)\n",
    "        if not spanish_results.empty:\n",
    "            print(f\"\\nüá™üá∏ Spanish 'que' patterns found:\")\n",
    "            for _, row in spanish_results.head(3).iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['text']}\")\n",
    "                print(f\"     (Source: {row['file_name']})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in pattern search: {e}\")\n",
    "\n",
    "    # Demo 4: Smart RAG queries with filtering\n",
    "    print(f\"\\n\\nü§ñ Demo 4: Smart Filtered RAG Queries\")\n",
    "\n",
    "    if 'index' in globals() and 'rag_system' in globals() and index and rag_system:\n",
    "        try:\n",
    "            # Get index summary\n",
    "            summary = rag_system.get_file_summary()\n",
    "            print(f\"üìä RAG Index Summary:\")\n",
    "            print(f\"   ‚Ä¢ Total documents: {summary.get('total_documents', 0):,}\")\n",
    "            print(f\"   ‚Ä¢ Languages: {dict(summary.get('languages', {}))}\")\n",
    "            print(f\"   ‚Ä¢ Files: {len(summary.get('files', {}))}\")\n",
    "            \n",
    "            # Korean-specific query\n",
    "            korean_response = smart_content_query(\n",
    "                \"Show me Korean present continuous examples and explain the pattern\",\n",
    "                {'language': 'Korean', 'top_k': 8}\n",
    "            )\n",
    "            print(f\"\\nüá∞üá∑ Korean Grammar Query Response:\")\n",
    "            print(korean_response)\n",
    "            \n",
    "            # File-specific query\n",
    "            grammar_response = smart_content_query(\n",
    "                \"What grammar patterns can you find?\",\n",
    "                {'file': 'grammar', 'top_k': 5}\n",
    "            )\n",
    "            print(f\"\\nüìö Grammar-focused Query Response:\")\n",
    "            print(grammar_response)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in RAG queries: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è RAG system not available for smart queries\")\n",
    "        print(\"   ‚Ä¢ Make sure cell 16 (Index Building) has been run successfully\")\n",
    "\n",
    "    # Demo 5: Vocabulary drill with enhanced context\n",
    "    print(f\"\\n\\nüé¥ Demo 5: Enhanced Vocabulary Drill\")\n",
    "\n",
    "    try:\n",
    "        korean_drill = vocab_drill(\"-Í≥† ÏûàÏñ¥Ïöî\", lang=\"Korean\", level=\"A2\", k=10)\n",
    "\n",
    "        if not korean_drill.empty:\n",
    "            print(f\"\\n‚úÖ Enhanced Korean Drill Results ({len(korean_drill)} examples):\")\n",
    "            display(korean_drill)\n",
    "            \n",
    "            # Show source file distribution\n",
    "            if 'Source' in korean_drill.columns:\n",
    "                source_counts = korean_drill['Source'].value_counts()\n",
    "                print(f\"\\nüìÅ Examples by source file:\")\n",
    "                for source, count in source_counts.items():\n",
    "                    print(f\"   ‚Ä¢ {source}: {count} examples\")\n",
    "                    \n",
    "        else:\n",
    "            print(\"‚ùå No Korean drill results. Checking data availability...\")\n",
    "            \n",
    "            # Debug: Show what Korean data we have\n",
    "            if 'df_raw' in globals() and not df_raw.empty:\n",
    "                korean_data = df_raw[df_raw['language'] == 'Korean']\n",
    "                print(f\"üìä Korean data available: {len(korean_data)} sentences\")\n",
    "                \n",
    "                if len(korean_data) > 0:\n",
    "                    print(\"üìã Sample Korean sentences:\")\n",
    "                    for _, row in korean_data.head(3).iterrows():\n",
    "                        print(f\"   ‚Ä¢ {row['text']} (from {row.get('file_name', 'unknown')})\")\n",
    "                    \n",
    "                    # Check pattern matches\n",
    "                    pattern_matches = korean_data[korean_data['text'].str.contains('Í≥† ÏûàÏñ¥Ïöî', na=False, regex=False)]\n",
    "                    print(f\"üéØ Sentences with 'Í≥† ÏûàÏñ¥Ïöî' pattern: {len(pattern_matches)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in vocabulary drill: {e}\")\n",
    "\n",
    "    print(f\"\\nüéâ Enhanced demo complete! The system now fully learns from complete files with:\")\n",
    "    print(\"   ‚úÖ Comprehensive file parsing (TXT, CSV, JSON, SRT, VTT)\")\n",
    "    print(\"   ‚úÖ Rich metadata preservation (file info, context, structure)\")\n",
    "    print(\"   ‚úÖ Enhanced RAG system with filtering\")\n",
    "    print(\"   ‚úÖ File-specific querying capabilities\")\n",
    "    print(\"   ‚úÖ Pattern search across languages\")\n",
    "    print(\"   ‚úÖ Contextual information retention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Demo 2: Spanish subjunctive query\n",
      "==================================================\n",
      "ü§î Querying: Dame 5 frases espa√±olas con el subjuntivo y su exp...\n",
      "\n",
      "üá™üá∏ Query: Dame 5 frases espa√±olas con el subjuntivo y su explicaci√≥n en espa√±ol.\n",
      "\n",
      "üìù Response:\n",
      "¬°Claro! Aqu√≠ tienes cinco frases en espa√±ol que utilizan el subjuntivo, junto con su explicaci√≥n:\n",
      "\n",
      "1. **Espero que vengas a la fiesta.**\n",
      "   - **Explicaci√≥n:** En esta frase, \"vengas\" es el subjuntivo del verbo \"venir\". Se usa porque expresa un deseo o expectativa sobre la llegada de alguien.\n",
      "\n",
      "2. **Es importante que estudies para el examen.**\n",
      "   - **Explicaci√≥n:** Aqu√≠, \"estudies\" es el subjuntivo del verbo \"estudiar\". Se utiliza para expresar la importancia de realizar una acci√≥n.\n",
      "\n",
      "3. **Ojal√° que no llueva ma√±ana.**\n",
      "   - **Explicaci√≥n:** \"Llueva\" es el subjuntivo del verbo \"llover\". Se usa para expresar un deseo sobre el clima en el futuro.\n",
      "\n",
      "4. **Quiero que me digas la verdad.**\n",
      "   - **Explicaci√≥n:** En esta frase, \"digas\" es el subjuntivo del verbo \"decir\". Se utiliza porque se est√° pidiendo que alguien realice una acci√≥n espec√≠fica.\n",
      "\n",
      "5. **Es posible que lleguen tarde.**\n",
      "   - **Explicaci√≥n:** \"Lleguen\" es el subjuntivo del verbo \"llegar\". Se usa aqu√≠ para expresar una posibilidad sobre la llegada de alguien.\n",
      "\n",
      "El subjuntivo se utiliza en espa√±ol para expresar deseos, dudas, posibilidades o situaciones hipot√©ticas. ¬°Sigue practicando y ver√°s c√≥mo mejoras!\n",
      "\n",
      "üá™üá∏ Query: Dame 5 frases espa√±olas con el subjuntivo y su explicaci√≥n en espa√±ol.\n",
      "\n",
      "üìù Response:\n",
      "¬°Claro! Aqu√≠ tienes cinco frases en espa√±ol que utilizan el subjuntivo, junto con su explicaci√≥n:\n",
      "\n",
      "1. **Espero que vengas a la fiesta.**\n",
      "   - **Explicaci√≥n:** En esta frase, \"vengas\" es el subjuntivo del verbo \"venir\". Se usa porque expresa un deseo o expectativa sobre la llegada de alguien.\n",
      "\n",
      "2. **Es importante que estudies para el examen.**\n",
      "   - **Explicaci√≥n:** Aqu√≠, \"estudies\" es el subjuntivo del verbo \"estudiar\". Se utiliza para expresar la importancia de realizar una acci√≥n.\n",
      "\n",
      "3. **Ojal√° que no llueva ma√±ana.**\n",
      "   - **Explicaci√≥n:** \"Llueva\" es el subjuntivo del verbo \"llover\". Se usa para expresar un deseo sobre el clima en el futuro.\n",
      "\n",
      "4. **Quiero que me digas la verdad.**\n",
      "   - **Explicaci√≥n:** En esta frase, \"digas\" es el subjuntivo del verbo \"decir\". Se utiliza porque se est√° pidiendo que alguien realice una acci√≥n espec√≠fica.\n",
      "\n",
      "5. **Es posible que lleguen tarde.**\n",
      "   - **Explicaci√≥n:** \"Lleguen\" es el subjuntivo del verbo \"llegar\". Se usa aqu√≠ para expresar una posibilidad sobre la llegada de alguien.\n",
      "\n",
      "El subjuntivo se utiliza en espa√±ol para expresar deseos, dudas, posibilidades o situaciones hipot√©ticas. ¬°Sigue practicando y ver√°s c√≥mo mejoras!\n"
     ]
    }
   ],
   "source": [
    "# Demo 2: Spanish subjunctive query\n",
    "print(\"\\nüéØ Demo 2: Spanish subjunctive query\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "spanish_query = \"Dame 5 frases espa√±olas con el subjuntivo y su explicaci√≥n en espa√±ol.\"\n",
    "spanish_response = ask_multilingual(spanish_query)\n",
    "\n",
    "print(f\"\\nüá™üá∏ Query: {spanish_query}\")\n",
    "print(f\"\\nüìù Response:\")\n",
    "print(spanish_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Demo 3: External content search with Tavily\n",
      "============================================================\n",
      "üîç Testing Tavily search functionality...\n",
      "üîç Searching web for: Korean language present continuous tense -Í≥† ÏûàÏñ¥Ïöî examples grammar\n",
      "\n",
      "üåê Search Results:\n",
      "üìù AI Answer: The Korean present continuous tense is formed with Í≥† ÏûàÏñ¥Ïöî, e.g., Î®πÍ≥† ÏûàÏñ¥Ïöî (meoggo isseoyo) meaning \"I am eating.\" It indicates an ongoing action. This is used in polite speech....\n",
      "\n",
      "üìö Found 3 external sources:\n",
      "\n",
      "1. **Korean Grammar - A Beginner's Guide**\n",
      "   üîó URL: https://www.90daykorean.com/korean-grammar/\n",
      "   üìù Content: Polite Past tense: Ïû§Ïñ¥Ïöî (jasseoyo) ¬∑ Polite Future tense: Ïûò Í±∞ÏòàÏöî (jal geoyeyo); Polite Present progressive: ÏûêÍ≥† ÏûàÏñ¥Ïöî (jago isseoyo). However, the form...\n",
      "   ‚≠ê Score: 0.6675167\n",
      "   üéØ Contains Korean pattern: ‚úÖ\n",
      "\n",
      "2. **Korean Honorific Speech: Show Some Respect! - KoreanClass101**\n",
      "   üîó URL: https://www.koreanclass101.com/lesson/intermediate-lesson-s2-6-korean-honorific-speech-show-some-respect\n",
      "   üìù Content: Easily master this lesson's grammar points with in-depth explanations and examples. ... Yes, both Ï∞æÍ≥† Í≥ÑÏÑ∏Ïöî and Ï∞æÍ≥† ÏûàÏñ¥Ïöî are in the present progressive tense...\n",
      "   ‚≠ê Score: 0.5663309\n",
      "   üéØ Contains Korean pattern: ‚úÖ\n",
      "\n",
      "3. **I Did It, I'm Doing It, And I'm Going To Do It - KoreanClass101**\n",
      "   üîó URL: https://www.koreanclass101.com/lesson/beginner-s3-1-i-did-it-im-doing-it-and-im-going-to-do-it\n",
      "   üìù Content: In this lesson, you'll learn about three main different tenses Visit KoreanClass101 and learn Korean fast with real lessons by real teachers....\n",
      "   ‚≠ê Score: 0.39333898\n",
      "\n",
      "üîç Testing simpler search...\n",
      "\n",
      "üåê Search Results:\n",
      "üìù AI Answer: The Korean present continuous tense is formed with Í≥† ÏûàÏñ¥Ïöî, e.g., Î®πÍ≥† ÏûàÏñ¥Ïöî (meoggo isseoyo) meaning \"I am eating.\" It indicates an ongoing action. This is used in polite speech....\n",
      "\n",
      "üìö Found 3 external sources:\n",
      "\n",
      "1. **Korean Grammar - A Beginner's Guide**\n",
      "   üîó URL: https://www.90daykorean.com/korean-grammar/\n",
      "   üìù Content: Polite Past tense: Ïû§Ïñ¥Ïöî (jasseoyo) ¬∑ Polite Future tense: Ïûò Í±∞ÏòàÏöî (jal geoyeyo); Polite Present progressive: ÏûêÍ≥† ÏûàÏñ¥Ïöî (jago isseoyo). However, the form...\n",
      "   ‚≠ê Score: 0.6675167\n",
      "   üéØ Contains Korean pattern: ‚úÖ\n",
      "\n",
      "2. **Korean Honorific Speech: Show Some Respect! - KoreanClass101**\n",
      "   üîó URL: https://www.koreanclass101.com/lesson/intermediate-lesson-s2-6-korean-honorific-speech-show-some-respect\n",
      "   üìù Content: Easily master this lesson's grammar points with in-depth explanations and examples. ... Yes, both Ï∞æÍ≥† Í≥ÑÏÑ∏Ïöî and Ï∞æÍ≥† ÏûàÏñ¥Ïöî are in the present progressive tense...\n",
      "   ‚≠ê Score: 0.5663309\n",
      "   üéØ Contains Korean pattern: ‚úÖ\n",
      "\n",
      "3. **I Did It, I'm Doing It, And I'm Going To Do It - KoreanClass101**\n",
      "   üîó URL: https://www.koreanclass101.com/lesson/beginner-s3-1-i-did-it-im-doing-it-and-im-going-to-do-it\n",
      "   üìù Content: In this lesson, you'll learn about three main different tenses Visit KoreanClass101 and learn Korean fast with real lessons by real teachers....\n",
      "   ‚≠ê Score: 0.39333898\n",
      "\n",
      "üîç Testing simpler search...\n",
      "‚úÖ Simple search successful: 2 results\n",
      "üìã Sample result: Lesson 18: Present Progressive ~Í≥† ÏûàÎã§; To be Getting ~ÏïÑ/Ïñ¥ÏßÄÎã§...\n",
      "\n",
      "üìä Enhanced Search Status:\n",
      "   ‚Ä¢ API Key: your_tavily_api_key_here\n",
      "   ‚Ä¢ Fresh Client: Created successfully\n",
      "   ‚Ä¢ Search Focus: Korean language learning\n",
      "‚úÖ Simple search successful: 2 results\n",
      "üìã Sample result: Lesson 18: Present Progressive ~Í≥† ÏûàÎã§; To be Getting ~ÏïÑ/Ïñ¥ÏßÄÎã§...\n",
      "\n",
      "üìä Enhanced Search Status:\n",
      "   ‚Ä¢ API Key: your_tavily_api_key_here\n",
      "   ‚Ä¢ Fresh Client: Created successfully\n",
      "   ‚Ä¢ Search Focus: Korean language learning\n"
     ]
    }
   ],
   "source": [
    "# Demo 3: Enhanced Tavily Search with External Content\n",
    "print(\"\\nüéØ Demo 3: External content search with Tavily\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    \n",
    "    # Always create a fresh client with the correct API key\n",
    "    correct_api_key = \"your_tavily_api_key_here\"\n",
    "    fresh_tavily_client = TavilyClient(api_key=correct_api_key)\n",
    "    \n",
    "    print(\"üîç Testing Tavily search functionality...\")\n",
    "    \n",
    "    # Search for Korean language learning content\n",
    "    search_query = \"Korean language present continuous tense -Í≥† ÏûàÏñ¥Ïöî examples grammar\"\n",
    "    print(f\"üîç Searching web for: {search_query}\")\n",
    "    \n",
    "    search_results = fresh_tavily_client.search(\n",
    "        query=search_query,\n",
    "        search_depth=\"basic\",\n",
    "        max_results=3,\n",
    "        include_domains=[\"koreanclass101.com\", \"90daykorean.com\", \"howtostudykorean.com\"],\n",
    "        include_answer=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüåê Search Results:\")\n",
    "    \n",
    "    if search_results.get('answer'):\n",
    "        print(f\"üìù AI Answer: {search_results['answer'][:300]}...\")\n",
    "    \n",
    "    if search_results.get('results'):\n",
    "        print(f\"\\nüìö Found {len(search_results['results'])} external sources:\")\n",
    "        for i, result in enumerate(search_results['results'], 1):\n",
    "            print(f\"\\n{i}. **{result['title']}**\")\n",
    "            print(f\"   üîó URL: {result['url']}\")\n",
    "            print(f\"   üìù Content: {result['content'][:200]}...\")\n",
    "            print(f\"   ‚≠ê Score: {result.get('score', 'N/A')}\")\n",
    "            \n",
    "            # Extract relevant Korean examples if any\n",
    "            content = result['content'].lower()\n",
    "            if 'Í≥† ÏûàÏñ¥Ïöî' in content:\n",
    "                print(f\"   üéØ Contains Korean pattern: ‚úÖ\")\n",
    "    else:\n",
    "        print(\"‚ùå No search results found\")\n",
    "        \n",
    "    # Test with a simpler query\n",
    "    print(f\"\\nüîç Testing simpler search...\")\n",
    "    simple_search = fresh_tavily_client.search(\n",
    "        query=\"Korean grammar present continuous\",\n",
    "        search_depth=\"basic\", \n",
    "        max_results=2\n",
    "    )\n",
    "    \n",
    "    if simple_search.get('results'):\n",
    "        print(f\"‚úÖ Simple search successful: {len(simple_search['results'])} results\")\n",
    "        print(f\"üìã Sample result: {simple_search['results'][0]['title'][:60]}...\")\n",
    "    else:\n",
    "        print(\"‚ùå Simple search failed\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå tavily-python not installed. Run: pip install tavily-python\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Tavily search failed: {str(e)}\")\n",
    "    print(f\"üìã Error details: {type(e).__name__}\")\n",
    "    \n",
    "    # Additional debugging\n",
    "    if \"api key\" in str(e).lower() or \"unauthorized\" in str(e).lower():\n",
    "        print(\"üîë API key issue detected\")\n",
    "        print(\"   ‚Ä¢ Verify API key is correct\")\n",
    "        print(\"   ‚Ä¢ Check account status at https://tavily.com\")\n",
    "    elif \"rate limit\" in str(e).lower():\n",
    "        print(\"‚è∞ Rate limit reached\")\n",
    "    elif \"network\" in str(e).lower():\n",
    "        print(\"üåê Network connectivity issue\")\n",
    "    else:\n",
    "        print(\"üîß Other error - check Tavily service status\")\n",
    "\n",
    "print(f\"\\nüìä Enhanced Search Status:\")\n",
    "print(f\"   ‚Ä¢ API Key: your_tavily_api_key_here\")\n",
    "print(f\"   ‚Ä¢ Fresh Client: Created successfully\")\n",
    "print(f\"   ‚Ä¢ Search Focus: Korean language learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Quick Tavily API Test\n",
      "==============================\n",
      "üîç Testing basic search...\n",
      "‚úÖ Tavily API working correctly!\n",
      "üìä Test result: \"Hello, World!\" program - Wikipedia...\n",
      "\n",
      "üí° If the test fails, try running the enhanced demo above for more detailed troubleshooting.\n",
      "‚úÖ Tavily API working correctly!\n",
      "üìä Test result: \"Hello, World!\" program - Wikipedia...\n",
      "\n",
      "üí° If the test fails, try running the enhanced demo above for more detailed troubleshooting.\n"
     ]
    }
   ],
   "source": [
    "# Quick Tavily API Test\n",
    "print(\"üß™ Quick Tavily API Test\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    \n",
    "    # Initialize with your API key\n",
    "    api_key = \"your_tavily_api_key_here\"\n",
    "    test_client = TavilyClient(api_key=api_key)\n",
    "    \n",
    "    # Simple test search\n",
    "    print(\"üîç Testing basic search...\")\n",
    "    results = test_client.search(\n",
    "        query=\"hello world\",\n",
    "        search_depth=\"basic\",\n",
    "        max_results=1\n",
    "    )\n",
    "    \n",
    "    if results and results.get('results'):\n",
    "        print(\"‚úÖ Tavily API working correctly!\")\n",
    "        print(f\"üìä Test result: {results['results'][0]['title'][:50]}...\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è API responded but no results found\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå tavily-python not installed. Run: pip install tavily-python\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Tavily test failed: {e}\")\n",
    "    \n",
    "    # Detailed error analysis\n",
    "    error_msg = str(e).lower()\n",
    "    if \"unauthorized\" in error_msg or \"api key\" in error_msg:\n",
    "        print(\"üîë Issue: Invalid API key\")\n",
    "        print(\"   ‚Ä¢ Check if the API key is correct\")\n",
    "        print(\"   ‚Ä¢ Verify account status at https://tavily.com\")\n",
    "    elif \"rate limit\" in error_msg:\n",
    "        print(\"‚è∞ Issue: Rate limit exceeded\")\n",
    "    elif \"network\" in error_msg or \"connection\" in error_msg:\n",
    "        print(\"üåê Issue: Network connectivity\")\n",
    "    else:\n",
    "        print(f\"üîß Issue: {e}\")\n",
    "\n",
    "print(\"\\nüí° If the test fails, try running the enhanced demo above for more detailed troubleshooting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tavily Configuration Confirmation\n",
    "print(\"‚úÖ Tavily AI Search is now properly configured!\")\n",
    "print(\"üéØ Your API key has been successfully integrated.\")\n",
    "print(\"üìö Run the Enhanced Tavily Demo above for Korean language learning searches.\")\n",
    "print(\"\\n\udca1 Tavily can now search external websites for:\")\n",
    "print(\"   ‚Ä¢ Korean grammar examples\")\n",
    "print(\"   ‚Ä¢ Language learning resources\") \n",
    "print(\"   ‚Ä¢ Present continuous tense patterns\")\n",
    "print(\"   ‚Ä¢ Educational content from trusted sources\")\n",
    "\n",
    "# Verify the configuration one more time\n",
    "try:\n",
    "    from tavily import TavilyClient\n",
    "    test_client = TavilyClient(api_key=\"your_tavily_api_key_here\")\n",
    "    print(\"\\nüîß Final verification: ‚úÖ Client initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Configuration issue: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spaced Repetition Scheduling\n",
    "\n",
    "Implement Anki-style spaced repetition for vocabulary cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_srs_metadata(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add spaced repetition system metadata to dataframe\"\"\"\n",
    "    df_srs = df.copy()\n",
    "    \n",
    "    # Initialize SRS fields\n",
    "    df_srs['next_review'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    df_srs['ease_factor'] = 2.5  # Default ease factor\n",
    "    df_srs['repetitions'] = 0\n",
    "    df_srs['interval'] = 1  # Days until next review\n",
    "    df_srs['last_review'] = None\n",
    "    df_srs['recall_quality'] = None  # 0-5 scale\n",
    "    \n",
    "    return df_srs\n",
    "\n",
    "def update_srs(df: pd.DataFrame, row_index: int, recall_quality: int) -> pd.DataFrame:\n",
    "    \"\"\"Update SRS scheduling based on recall quality (0-5 scale)\"\"\"\n",
    "    df_updated = df.copy()\n",
    "    \n",
    "    if row_index not in df_updated.index:\n",
    "        print(f\"‚ùå Row index {row_index} not found\")\n",
    "        return df_updated\n",
    "    \n",
    "    # Current values\n",
    "    current_ease = df_updated.loc[row_index, 'ease_factor']\n",
    "    current_reps = df_updated.loc[row_index, 'repetitions']\n",
    "    current_interval = df_updated.loc[row_index, 'interval']\n",
    "    \n",
    "    # Update based on SM-2 algorithm (simplified)\n",
    "    if recall_quality >= 3:  # Correct response\n",
    "        if current_reps == 0:\n",
    "            new_interval = 1\n",
    "        elif current_reps == 1:\n",
    "            new_interval = 6\n",
    "        else:\n",
    "            new_interval = int(current_interval * current_ease)\n",
    "        \n",
    "        new_reps = current_reps + 1\n",
    "        \n",
    "        # Update ease factor\n",
    "        new_ease = current_ease + (0.1 - (5 - recall_quality) * (0.08 + (5 - recall_quality) * 0.02))\n",
    "        new_ease = max(1.3, new_ease)  # Minimum ease factor\n",
    "        \n",
    "    else:  # Incorrect response\n",
    "        new_reps = 0\n",
    "        new_interval = 1\n",
    "        new_ease = current_ease  # Keep ease factor\n",
    "    \n",
    "    # Calculate next review date\n",
    "    next_review_date = datetime.now() + timedelta(days=new_interval)\n",
    "    \n",
    "    # Update dataframe\n",
    "    df_updated.loc[row_index, 'ease_factor'] = new_ease\n",
    "    df_updated.loc[row_index, 'repetitions'] = new_reps\n",
    "    df_updated.loc[row_index, 'interval'] = new_interval\n",
    "    df_updated.loc[row_index, 'next_review'] = next_review_date.strftime('%Y-%m-%d')\n",
    "    df_updated.loc[row_index, 'last_review'] = datetime.now().strftime('%Y-%m-%d')\n",
    "    df_updated.loc[row_index, 'recall_quality'] = recall_quality\n",
    "    \n",
    "    return df_updated\n",
    "\n",
    "def get_due_cards(df: pd.DataFrame, max_cards: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Get cards due for review today\"\"\"\n",
    "    today = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    due_cards = df[df['next_review'] <= today].head(max_cards)\n",
    "    \n",
    "    return due_cards\n",
    "\n",
    "# Initialize SRS metadata for our data\n",
    "if not df_raw.empty:\n",
    "    df_with_srs = initialize_srs_metadata(df_raw)\n",
    "    print(f\"‚úÖ SRS metadata initialized for {len(df_with_srs)} sentences\")\n",
    "    \n",
    "    # Example: simulate reviewing a card\n",
    "    if len(df_with_srs) > 0:\n",
    "        print(\"\\nüé¥ Example SRS update:\")\n",
    "        sample_index = df_with_srs.index[0]\n",
    "        print(f\"Before: Next review = {df_with_srs.loc[sample_index, 'next_review']}\")\n",
    "        \n",
    "        # Simulate good recall (quality = 4)\n",
    "        df_with_srs = update_srs(df_with_srs, sample_index, recall_quality=4)\n",
    "        print(f\"After (quality=4): Next review = {df_with_srs.loc[sample_index, 'next_review']}\")\n",
    "        print(f\"Interval: {df_with_srs.loc[sample_index, 'interval']} days\")\n",
    "        print(f\"Ease factor: {df_with_srs.loc[sample_index, 'ease_factor']:.2f}\")\n",
    "else:\n",
    "    print(\"‚ùå No data available for SRS initialization\")\n",
    "\n",
    "print(\"\\nüß† Spaced Repetition System ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost & Token Usage Dashboard\n",
    "\n",
    "Track and display token usage and associated costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cost_dashboard():\n",
    "    \"\"\"Display comprehensive cost and usage statistics\"\"\"\n",
    "    cost_summary = token_tracker.get_cost_summary()\n",
    "    \n",
    "    print(\"üí∞ COST & TOKEN USAGE DASHBOARD\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = [\n",
    "        [\"üìä Total Tokens\", f\"{cost_summary['total_tokens']:,}\"],\n",
    "        [\"üî§ Embedding Tokens\", f\"{cost_summary['embedding_tokens']:,}\"],\n",
    "        [\"ü§ñ LLM Tokens\", f\"{cost_summary['llm_tokens']:,}\"],\n",
    "        [\"\", \"\"],  # Separator\n",
    "        [\"üíµ Embedding Cost\", f\"${cost_summary['embedding_cost']:.4f}\"],\n",
    "        [\"üíµ LLM Cost\", f\"${cost_summary['llm_cost']:.4f}\"],\n",
    "        [\"üí∞ Total Cost\", f\"${cost_summary['total_cost']:.4f}\"]\n",
    "    ]\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data, columns=['Metric', 'Value'])\n",
    "    \n",
    "    print(tabulate(summary_df, headers=['Metric', 'Value'], tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Additional insights\n",
    "    if cost_summary['total_tokens'] > 0:\n",
    "        avg_cost_per_1k = (cost_summary['total_cost'] / cost_summary['total_tokens']) * 1000\n",
    "        print(f\"\\nüìà Average cost per 1K tokens: ${avg_cost_per_1k:.4f}\")\n",
    "    \n",
    "    # Data insights\n",
    "    if not df_raw.empty:\n",
    "        print(f\"\\nüìö Data Statistics:\")\n",
    "        print(f\"   ‚Ä¢ Total sentences processed: {len(df_raw):,}\")\n",
    "        print(f\"   ‚Ä¢ Unique languages: {df_raw['language'].nunique()}\")\n",
    "        print(f\"   ‚Ä¢ Average tokens per sentence: {cost_summary['total_tokens'] / len(df_raw):.1f}\")\n",
    "    \n",
    "    # Cost projections\n",
    "    print(f\"\\nüîÆ Cost Projections:\")\n",
    "    sentences_per_dollar = (1 / cost_summary['total_cost'] * len(df_raw)) if cost_summary['total_cost'] > 0 else float('inf')\n",
    "    print(f\"   ‚Ä¢ Sentences per $1: {sentences_per_dollar:.0f}\")\n",
    "    print(f\"   ‚Ä¢ Cost for 1,000 more sentences: ${(cost_summary['total_cost'] / len(df_raw) * 1000):.4f}\" if len(df_raw) > 0 else \"   ‚Ä¢ N/A\")\n",
    "    \n",
    "    return cost_summary\n",
    "\n",
    "# Display the dashboard\n",
    "final_costs = display_cost_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save & Export\n",
    "\n",
    "Save processed data and provide export options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary_cards(df: pd.DataFrame, filename: str = \"vocab_cards.csv\"):\n",
    "    \"\"\"Save the augmented dataframe with SRS metadata\"\"\"\n",
    "    try:\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Vocabulary cards saved to: {filename}\")\n",
    "        print(f\"üìä Saved {len(df)} cards with SRS metadata\")\n",
    "        \n",
    "        # Display sample of saved data\n",
    "        if len(df) > 0:\n",
    "            print(\"\\nüîç Sample of saved data:\")\n",
    "            sample_cols = ['text', 'language', 'cefr_level', 'next_review', 'ease_factor']\n",
    "            available_cols = [col for col in sample_cols if col in df.columns]\n",
    "            display(df[available_cols].head(3))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save vocabulary cards: {str(e)}\")\n",
    "\n",
    "def export_to_anki_format(df: pd.DataFrame, output_file: str = \"anki_cards.txt\"):\n",
    "    \"\"\"Export cards in Anki import format (tab-separated)\"\"\"\n",
    "    try:\n",
    "        anki_data = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Front of card (original sentence)\n",
    "            front = row['text']\n",
    "            \n",
    "            # Back of card (translation + metadata)\n",
    "            back_parts = []\n",
    "            \n",
    "            # Try to get translation if available\n",
    "            if 'translation' in row and pd.notna(row['translation']):\n",
    "                back_parts.append(f\"Translation: {row['translation']}\")\n",
    "            \n",
    "            back_parts.extend([\n",
    "                f\"Language: {row['language']}\",\n",
    "                f\"Level: {row['cefr_level']}\",\n",
    "                f\"Source: {Path(row['source_file']).name if 'source_file' in row else 'Unknown'}\"\n",
    "            ])\n",
    "            \n",
    "            if 'emoji' in row and pd.notna(row['emoji']):\n",
    "                back_parts.append(f\"Emoji: {row['emoji']}\")\n",
    "            \n",
    "            back = \"<br>\".join(back_parts)\n",
    "            \n",
    "            # Tags\n",
    "            tags = [row['language'], row['cefr_level']]\n",
    "            if 'source_file' in row:\n",
    "                tags.append(Path(row['source_file']).stem)\n",
    "            \n",
    "            # Anki format: Front \\t Back \\t Tags\n",
    "            anki_line = f\"{front}\\t{back}\\t{' '.join(tags)}\"\n",
    "            anki_data.append(anki_line)\n",
    "        \n",
    "        # Write to file\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"\\n\".join(anki_data))\n",
    "        \n",
    "        print(f\"‚úÖ Anki cards exported to: {output_file}\")\n",
    "        print(f\"üìã Export format: Front \\t Back \\t Tags\")\n",
    "        print(f\"üìä Exported {len(anki_data)} cards\")\n",
    "        \n",
    "        # Instructions\n",
    "        print(\"\\nüìñ To import into Anki:\")\n",
    "        print(\"   1. Open Anki\")\n",
    "        print(\"   2. File ‚Üí Import\")\n",
    "        print(f\"   3. Select {output_file}\")\n",
    "        print(\"   4. Configure field mapping\")\n",
    "        print(\"   5. Import cards\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to export Anki cards: {str(e)}\")\n",
    "\n",
    "# Save vocabulary cards with SRS metadata\n",
    "if 'df_with_srs' in locals() and not df_with_srs.empty:\n",
    "    save_vocabulary_cards(df_with_srs)\n",
    "    export_to_anki_format(df_with_srs)\n",
    "elif not df_raw.empty:\n",
    "    save_vocabulary_cards(df_raw)\n",
    "    export_to_anki_format(df_raw)\n",
    "else:\n",
    "    print(\"‚ùå No data to save\")\n",
    "\n",
    "print(\"\\nüíæ Save & Export operations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stretch Goals & Future Enhancements\n",
    "\n",
    "Optional advanced features that can be implemented:\n",
    "\n",
    "### üîä Audio TTS Integration\n",
    "- Use OpenAI's `audio.speech` API to generate pronunciation audio for each sentence\n",
    "- Store audio files locally and link them to vocabulary cards\n",
    "- Implement playback functionality in notebook interface\n",
    "\n",
    "### üåê FastAPI Web Dashboard  \n",
    "- Create a REST API backend serving the RAG index\n",
    "- Build React frontend for interactive vocabulary practice\n",
    "- Features: real-time queries, SRS scheduling, progress tracking\n",
    "- Deploy as containerized application\n",
    "\n",
    "### üì± Mobile App Integration\n",
    "- Export vocabulary cards to mobile-friendly formats\n",
    "- Sync with popular language learning apps\n",
    "- Offline mode for practicing without internet\n",
    "\n",
    "### üîç Advanced NLP Features\n",
    "- Grammar pattern extraction and explanation\n",
    "- Automatic difficulty assessment using linguistic features\n",
    "- Contextual word sense disambiguation\n",
    "- Automated CEFR level classification\n",
    "\n",
    "### üìä Learning Analytics\n",
    "- Progress tracking and visualization\n",
    "- Weak point identification\n",
    "- Personalized learning recommendations\n",
    "- Export learning statistics\n",
    "\n",
    "### üåç Multi-modal Learning\n",
    "- Image context for vocabulary (visual associations)\n",
    "- Video subtitle integration\n",
    "- Cultural context explanations\n",
    "- Native speaker audio samples\n",
    "\n",
    "To implement any of these features, simply add new cells below and extend the existing codebase. The modular design makes it easy to add new functionality without breaking existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Notebook Complete!\n",
    "\n",
    "### Summary of Achievements:\n",
    "\n",
    "‚úÖ **Multi-format Data Ingestion**: Successfully processed TXT, HTML, SRT, VTT, JSON, and CSV files  \n",
    "‚úÖ **RAG-Powered Queries**: Built vector index with LlamaIndex for intelligent sentence retrieval  \n",
    "‚úÖ **Korean Grammar Demo**: Generated A2-level sentences with '-Í≥† ÏûàÏñ¥Ïöî' pattern  \n",
    "‚úÖ **Multilingual Support**: Language detection and CEFR level estimation  \n",
    "‚úÖ **Spaced Repetition**: Implemented Anki-style SRS scheduling  \n",
    "‚úÖ **Cost Tracking**: Monitored token usage and API costs  \n",
    "‚úÖ **Export Options**: Saved vocabulary cards and Anki import format  \n",
    "\n",
    "### Files Generated:\n",
    "- `vocab_cards.csv` - Complete vocabulary database with SRS metadata\n",
    "- `anki_cards.txt` - Ready-to-import Anki flashcards\n",
    "- `storage/` - Persisted vector index for future sessions\n",
    "\n",
    "### Next Steps:\n",
    "1. Add more multilingual content to your `data/` folder\n",
    "2. Run vocabulary drills for different languages and patterns\n",
    "3. Use the SRS system to track your learning progress\n",
    "4. Import cards into Anki for mobile practice\n",
    "5. Extend with stretch goal features as needed\n",
    "\n",
    "**Happy Learning! üåü**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
